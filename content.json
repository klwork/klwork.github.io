{"meta":{"title":"Klwork","subtitle":"专注于快乐的事情","description":"klwork","author":"wangwei","url":"http://yoursite.com"},"pages":[{"title":"","date":"2019-02-27T14:31:55.144Z","updated":"2019-02-27T14:31:55.143Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"关于我","date":"2013-11-16T16:11:51.000Z","updated":"2019-02-18T04:44:19.886Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"个人简介技术男，湘人，漂于沪，不闲时鼓捣代码，闲时看哲学，管理类书籍。喜欢美的东西。标签: &nbsp;&nbsp;风控模型/信贷系统/决策引擎/机器学习/java/python/Github技术博客：https://klwork.com/GitHub：https://github.com/klwork 联系方式Email: wangwei_fir@126.com"},{"title":"最近项目","date":"2013-11-16T16:20:06.000Z","updated":"2018-02-27T15:51:52.728Z","comments":true,"path":"project/index.html","permalink":"http://yoursite.com/project/index.html","excerpt":"","text":""}],"posts":[{"title":"PySpark的背后原理","slug":"031spark/PySpark 的背后原理","date":"2019-03-10T00:00:00.000Z","updated":"2019-03-11T05:45:35.692Z","comments":true,"path":"category/031spark/PySpark 的背后原理.html","link":"","permalink":"http://yoursite.com/category/031spark/PySpark 的背后原理.html","excerpt":"","text":"本文转自http://sharkdtu.com/posts/pyspark-internal.html Spark主要是由Scala语言开发，为了方便和其他系统集成而不引入scala相关依赖，部分实现使用Java语言开发，例如External Shuffle Service等。总体来说，Spark是由JVM语言实现，会运行在JVM中。然而，Spark除了提供Scala/Java开发接口外，还提供了Python、R等语言的开发接口，为了保证Spark核心实现的独立性，Spark仅在外围做包装，实现对不同语言的开发支持，本文主要介绍Python Spark的实现原理，剖析pyspark应用程序是如何运行起来的。 Spark运行时架构首先我们先回顾下Spark的基本运行时架构，如下图所示，其中橙色部分表示为JVM，Spark应用程序运行时主要分为Driver和Executor，Driver负载总体调度及UI展示，Executor负责Task运行，Spark可以部署在多种资源管理系统中，例如Yarn、Mesos等，同时Spark自身也实现了一种简单的Standalone(独立部署)资源管理系统，可以不用借助其他资源管理系统即可运行。 用户的Spark应用程序运行在Driver上(某种程度上说，用户的程序就是Spark Driver程序)，经过Spark调度封装成一个个Task，再将这些Task信息发给Executor执行，Task信息包括代码逻辑以及数据信息，Executor不直接运行用户的代码。 PySpark运行时架构为了不破坏Spark已有的运行时架构，Spark在外围包装一层Python API，借助Py4j实现Python和Java的交互，进而实现通过Python编写Spark应用程序，其运行时架构如下图所示。 其中白色部分是新增的Python进程，在Driver端，通过Py4j实现在Python中调用Java的方法，即将用户写的PySpark程序”映射”到JVM中，例如，用户在PySpark中实例化一个Python的SparkContext对象，最终会在JVM中实例化Scala的SparkContext对象； 在Executor端，则不需要借助Py4j，因为Executor端运行的Task逻辑是由Driver发过来的，那是序列化后的字节码，虽然里面可能包含有用户定义的Python函数或Lambda表达式，Py4j并不能实现在Java里调用Python的方法，为了能在Executor端运行用户定义的Python函数或Lambda表达式，则需要为每个Task单独启一个Python进程，通过socket通信方式将Python函数或Lambda表达式发给Python进程执行。 语言层面的交互总体流程如下图所示，实线表示方法调用，虚线表示结果返回。 下面分别详细剖析PySpark的Driver是如何运行起来的以及Executor是如何运行Task的。 Driver端运行原理当我们通过spark-submmit提交pyspark程序，首先会上传python脚本及依赖，并申请Driver资源，当申请到Driver资源后，会通过PythonRunner(其中有main方法)拉起JVM，如下图所示。 PythonRunner入口main函数里主要做两件事： 开启Py4j GatewayServer 通过Java Process方式运行用户上传的Python脚本 用户Python脚本起来后，首先会实例化Python版的SparkContext对象，在实例化过程中会做两件事： 实例化Py4j GatewayClient，连接JVM中的Py4j GatewayServer，后续在Python中调用Java的方法都是借助这个Py4j Gateway 通过Py4j Gateway在JVM中实例化SparkContext对象 经过上面两步后，SparkContext对象初始化完毕，Driver已经起来了，开始申请Executor资源，同时开始调度任务。用户Python脚本中定义的一系列处理逻辑最终遇到action方法后会触发Job的提交，提交Job时是直接通过Py4j调用Java的PythonRDD.runJob方法完成，映射到JVM中，会转给sparkContext.runJob方法，Job运行完成后，JVM中会开启一个本地Socket等待Python进程拉取，对应地，Python进程在调用PythonRDD.runJob后就会通过Socket去拉取结果。 把前面运行时架构图中Driver部分单独拉出来，如下图所示，通过PythonRunner入口main函数拉起JVM和Python进程，JVM进程对应下图橙色部分，Python进程对应下图白色部分。Python进程通过Py4j调用Java方法提交Job，Job运行结果通过本地Socket被拉取到Python进程。还有一点是，对于大数据量，例如广播变量等，Python进程和JVM进程是通过本地文件系统来交互，以减少进程间的数据传输。 Executor端运行原理为了方便阐述，以Spark On Yarn为例，当Driver申请到Executor资源时，会通过CoarseGrainedExecutorBackend(其中有main方法)拉起JVM，启动一些必要的服务后等待Driver的Task下发，在还没有Task下发过来时，Executor端是没有Python进程的。当收到Driver下发过来的Task后，Executor的内部运行过程如下图所示。 Executor端收到Task后，会通过launchTask运行Task，最后会调用到PythonRDD的compute方法，来处理一个分区的数据，PythonRDD的compute方法的计算流程大致分三步走： 如果不存在pyspark.deamon后台Python进程，那么通过Java Process的方式启动pyspark.deamon后台进程，注意每个Executor上只会有一个pyspark.deamon后台进程，否则，直接通过Socket连接pyspark.deamon，请求开启一个pyspark.worker进程运行用户定义的Python函数或Lambda表达式。pyspark.deamon是一个典型的多进程服务器，来一个Socket请求，fork一个pyspark.worker进程处理，一个Executor上同时运行多少个Task，就会有多少个对应的pyspark.worker进程。 紧接着会单独开一个线程，给pyspark.worker进程喂数据，pyspark.worker则会调用用户定义的Python函数或Lambda表达式处理计算。 在一边喂数据的过程中，另一边则通过Socket去拉取pyspark.worker的计算结果。 把前面运行时架构图中Executor部分单独拉出来，如下图所示，橙色部分为JVM进程，白色部分为Python进程，每个Executor上有一个公共的pyspark.deamon进程，负责接收Task请求，并fork pyspark.worker进程单独处理每个Task，实际数据处理过程中，pyspark.worker进程和JVM Task会较频繁地进行本地Socket数据通信。 总结总体上来说，PySpark是借助Py4j实现Python调用Java，来驱动Spark应用程序，本质上主要还是JVM runtime，Java到Python的结果返回是通过本地Socket完成。虽然这种架构保证了Spark核心代码的独立性，但是在大数据场景下，JVM和Python进程间频繁的数据通信导致其性能损耗较多，恶劣时还可能会直接卡死，所以建议对于大规模机器学习或者Streaming应用场景还是慎用PySpark，尽量使用原生的Scala/Java编写应用程序，对于中小规模数据量下的简单离线任务，可以使用PySpark快速部署提交。 参考原文地址","categories":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}],"keywords":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}]},{"title":"虚拟环境下spark环境安装配置","slug":"031spark/虚拟环境下spark环境安装配置","date":"2019-03-10T00:00:00.000Z","updated":"2019-03-11T04:26:31.055Z","comments":true,"path":"category/031spark/虚拟环境下spark环境安装配置.html","link":"","permalink":"http://yoursite.com/category/031spark/虚拟环境下spark环境安装配置.html","excerpt":"","text":"机器配置假设已经成功配置了三台虚拟机器，分别为:master,node1,node2。安装的都为centos7。 步骤安装JDK开发插件在master机器中 安装yum install -y java-1.8.0-openjdk-devel 通过rpm -qa | grep openjdk查看OpenJDK版本 查看OpenJDK路径whereis java 编辑 /etc/profile，添加的代码如下 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.201.b09-2.el7_6.x86_64 export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin 使配置生效source /etc/profile 设定 ssh key在master机器中 sudo su - vim ~/.ssh/id_rsa.pub 复制其中的内容 自己的机器也需要进行复制 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys在node1和node2机器中 vim ~/.ssh/authorized_keys 将上面复制的文件，放在其中。 在master机器中：执行ssh node1，如果能够登录，则设置成功。 启动hdfshdfs namenode -format $HADOOP_PREFIX/sbin/start-dfs.sh 成功会启动3个进程 4228 NameNode4327 DataNode4488 SecondaryNameNode 测试是否成功 使用 hdfs dfsadmin -report或者访问浏览器进行访问。http://10.168.1.100:50070 启动yard$HADOOP_PREFIX/sbin/start-yarn.sh 这时会多启动进程4816 NodeManager4713 ResourceManager netstat -nlop 启动spark进入到master机器，执行$SPARK_HOME/sbin/start-all.sh 访问http://10.168.1.100:8080/ 启动进程如下5175 Master5244 Worker 启动hive启动nohup hive –service metastore &amp; pyspark测试pyspark –master spark://10.168.1.100:7077 参考大数据常见错误解决方案","categories":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}],"keywords":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}]},{"title":"centos7配置python3环境","slug":"021python/centos7配置python3环境","date":"2019-03-09T00:00:00.000Z","updated":"2019-03-10T23:54:48.094Z","comments":true,"path":"category/021python/centos7配置python3环境.html","link":"","permalink":"http://yoursite.com/category/021python/centos7配置python3环境.html","excerpt":"","text":"安装python3安装依赖包 yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make libffi-devel 进行安装 wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz tar -xvJf Python-3.7.1.tar.xz cd Python-3.7.1 ./configure --enable-optimizations --prefix=/usr/local/python3 make &amp;&amp; make install 执行这步是后面最好加上 –enable-optimizations 会自动安装pip3及优化配置 如果出现ModuleNotFoundError: No module named ‘_ctypes’的解决办法， yum install libffi-devel -y，如果不能采取下面的方式。 wget http://mirror.centos.org/centos/7/os/x86_64/Packages/libffi-devel-3.0.13-18.el7.x86_64.rpm rpm -ivh libffi-devel-3.0.13-18.el7.x86_64.rpm 以前的安装python进行备份mv /usr/bin/python /usr/bin/python_old2 创建软连接 ln -s /usr/local/python3/bin/python3 /usr/bin/python ln -s /usr/local/python3/bin/python3 /usr/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 加入到环境变量 修改~/.bash_profile,加入 export export PATH=/usr/local/python3/bin/:$PATH 测试 pip3 -V 修改yum配置文件修改yum配置文件sudo vi /usr/bin/yum #将第一行指定的python版本改为python2.7 #!/usr/bin/python 改为 #!/usr/bin/python2.7 urlgrabber文件修改sudo vi /usr/libexec/urlgrabber-ext-down #将第一行的/usr/bin/python改为/usr/bin/python2.7 安装jupyterpip3 install jupyter 如果要访问另一台机器，比如远端服务器上的 notebook, 即默认是不支持 172.104.105.119:8888 这样的访问，需要额外配置 首先执行jupyter notebook --generate-config --allow-root生成配置文件 修改生成的配置文件vim /root/.jupyter/jupyter_notebook_config.py，修改配置项为如下的内容。c.NotebookApp.ip = ‘10.168.1.100’ 然后执行就可以打开了jupyter notebook --allow-root 参考网站centos7 安装 Python3 并配置 pip3使用国内镜像源来加速python pypi包的安装设置 jupyter notebook 可远程访问","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"ubuntu环境下使用vagrant创建虚拟机","slug":"018linux/ubuntu/ubuntu配置vagrant","date":"2019-03-09T00:00:00.000Z","updated":"2019-03-10T09:47:43.434Z","comments":true,"path":"category/018linux/ubuntu/ubuntu配置vagrant.html","link":"","permalink":"http://yoursite.com/category/018linux/ubuntu/ubuntu配置vagrant.html","excerpt":"","text":"说明宿主环境为ubuntu 17,计划在上面安装3台虚拟主机。 10.168.1.100 master10.168.1.101 node110.168.1.102 node2 步骤安装vagrant需要首先安装virtualBox，假设已经安装。 sudo apt-get install vagrant 进入到一个目录，执行vagrant init centos/7，会在目录下建立Vagrantfile文件 定制化Vagrantfile文件修改原有的Vagrantfile文件，如下 Vagrant.configure(&quot;2&quot;) do |config| config.vm.box = &quot;centos/7&quot; config.vm.define :master do |master| master.vm.hostname = &quot;10.168.1.100&quot; master.vm.provider :virtualbox do |v| v.name = &quot;10.168.1.100&quot; v.memory = 1024 v.cpus = 2 end master.vm.network :public_network, ip: &quot;10.168.1.100&quot;,bridge: &quot;enp3s0&quot;,auto_config: false master.vm.provision :shell, path: &quot;bootstrap_master.sh&quot; end (1..2).each do |i| config.vm.define &quot;node#{i}&quot; do |node| node.vm.hostname = &quot;10.168.1.10#{i}&quot; node.vm.provider :virtualbox do |v| v.memory = 512 v.cpus = 1 end node.vm.network :public_network, ip: &quot;10.168.1.10#{i}&quot;,bridge: &quot;enp3s0&quot;,auto_config: false end end config.vm.provision :shell, path: &quot;bootstrap.sh&quot; end 启动vagrant up 启动完成后有个.vagrant目录，其文件目录可能如下 └── machines ├── master │ └── virtualbox │ ├── action_provision │ ├── action_set_name │ ├── creator_uid │ ├── id │ ├── index_uuid │ ├── private_key │ └── synced_folders ├── node1 │ └── virtualbox │ ├── action_provision │ ├── action_set_name │ ├── creator_uid │ ├── id │ ├── index_uuid │ ├── private_key │ └── synced_folders └── node2 └── virtualbox ├── action_provision ├── action_set_name ├── creator_uid ├── id ├── index_uuid ├── private_key └── synced_folders 表示安装了三个机器,master,node1,node2. 进入到安装的系统vagrant ssh mastervagrant ssh node1 如果修改了配置如何生效vagrant reload --provision /vagrant这个目录是自动映射的，被映射到你刚刚建立的文件夹，这样就方便我们以后在开发机中进行开发，在虚拟机中进行运行效果测试了。 常用命令vagrant init # 建立 Vagrant File vagrant up # 建立虚拟机(或开机) vagrant halt # 关闭虚拟机 vagrant reload # 重启虚拟机 vagrant ssh # SSH 至虚拟机 vagrant status # 查看虚拟机运行状态 vagrant destroy # 删除虚拟机 vagrant suspend #暂停虚拟机(类似休眠) vagrant resume #唤醒虚拟机 VBoxManage 命令vagrant依赖virtualBox，virtualBox下的命令行操作如下： 查看当前虚拟机 VBoxManage list vms 查看当前正在运行的虚拟机 VBoxManage list runningvms 关闭虚拟机VBoxManage controlvm 14c2d90b-a0ee-48cd-885a-6b20d50c5c04 poweroff 删除VBoxManage unregistervm 87bf00c7-8468-430e-b68f-446a95e73c93 --delete 错误集解决安装laravel/homestead vagrant环境报”A VirtualBox machine with the name ‘homestead’ already exists.”的错误。使用上面的VBoxManage unregistervm的删除即可。 Vagrant network collides with a non-hostonly networkip地址设置192.168.2.10其他参考https://stackoverflow.com/questions/39049717/vagrant-network-collides-with-a-non-hostonly-network 参考Vagrant 手册之多个虚拟机 multi-machineVBoxManage 命令详解使用vagrant部署开发环境VAGRANT 网络配置https://www.cnblogs.com/wangkongming/p/4301021.htmlhttps://www.jianshu.com/p/a1bc23bc7892 https://blog.csdn.net/kikajack/article/details/80032131ssh免密码登录 https://www.cnblogs.com/chuanqimessi/p/4704850.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"mac下配置pyspark环境","slug":"031spark/mac下spark环境配置","date":"2019-02-11T00:00:00.000Z","updated":"2019-02-20T09:51:32.681Z","comments":true,"path":"category/031spark/mac下spark环境配置.html","link":"","permalink":"http://yoursite.com/category/031spark/mac下spark环境配置.html","excerpt":"","text":"安装过程以前用的spark版本有点老了，来个新的。 安装包下载地址: https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz 解压缩到/soft/spark-2.4.0-bin-hadoop2.7 配置环境变量（Mac是 ~/.bash_profile） export SPARK_HOME=/soft/spark-2.4.0-bin-hadoop2.7 export PATH=${PATH}:${SPARK_HOME}/bin 进入spark安装目录下sbin，执行./spark-shell 访问spark页面：http://localhost:8080/出现画面，表示安装成功。 pyspark 测试启动pyspark 通过http://localhost:4040可以访问。查看后台 Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8 Python 2.7.15 (default, May 1 2018, 16:44:08) 我需要的是python3。 继续修改~/.bash_profile，修改如下 export SPARK_HOME=/soft/spark-2.4.0-bin-hadoop2.7 export PYSPARK_DRIVER_PYTHON=&quot;jupyter&quot; export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot; export PYSPARK_PYTHON=python3 alias snotebook=&apos;$SPARK_HOME/bin/pyspark --master local[2]&apos; 执行source ~/.bash_profile让其生效。 PYSPARK_DRIVER_PYTHON这里指定pyspark的启动形式是jupyter notebook，执行快捷方式snotebook 弹出久违的notebook窗口可以进行开工了。 参考https://www.jianshu.com/p/d0f57e937e8f","categories":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}],"keywords":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}]},{"title":"敏捷数据科学流程","slug":"001机器学习/敏捷数据科学流程","date":"2019-02-10T00:00:00.000Z","updated":"2019-02-18T05:42:20.089Z","comments":true,"path":"category/001机器学习/敏捷数据科学流程.html","link":"","permalink":"http://yoursite.com/category/001机器学习/敏捷数据科学流程.html","excerpt":"","text":"敏捷数据科学流程","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/tags/数据科学/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"机器学习分析流程","slug":"001机器学习/机器学习分析流程","date":"2019-02-10T00:00:00.000Z","updated":"2019-02-18T05:54:53.322Z","comments":true,"path":"category/001机器学习/机器学习分析流程.html","link":"","permalink":"http://yoursite.com/category/001机器学习/机器学习分析流程.html","excerpt":"","text":"机器学习流程步骤 线性回归步骤线性回归的流程和上面的类似,也贴出来,方便查询.","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"数据科学","slug":"数据科学","permalink":"http://yoursite.com/tags/数据科学/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"常用maven操作","slug":"020programming/配置管理/0常用maven操作","date":"2019-02-06T00:00:00.000Z","updated":"2019-02-25T05:36:09.175Z","comments":true,"path":"category/020programming/配置管理/0常用maven操作.html","link":"","permalink":"http://yoursite.com/category/020programming/配置管理/0常用maven操作.html","excerpt":"","text":"常用命令packagemvn package -Dmaven.test.skip=true mvn package -rf :config-server -Dmaven.test.skip=true 如果需要下载源代码 mvn package -Dmaven.test.skip=true -DdownloadSources=true installmvn clean install -Dmaven.test.skip=true 通过D加入参数 mvn clean install -Dmaven.test.skip=true -Dstore-bundle=local deploymvn deploy -Dmaven.test.skip=true assemblymvn assembly:assembly -Dmaven.test.skip=true 其他技巧复制依赖mvn dependency:copy-dependencies -DoutputDirectory=lib 生成项目简单javamvn archetype:generate -DgroupId=com.klwork.learn -DartifactId=learn-java-core -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false web项目mvn archetype:generate -DarchetypeCatalog=internal -DgroupId=com.klwork.myweb -DartifactId=myweb -DarchetypeArtifactId=maven-archetype-webapp 改变版本号mvn versions:set -DnewVersion=1.0.0-SNAPSHOTS 安装外部文件到本地mvn install:install-file -DgroupId=eleSign -DartifactId=bjcaprotobuf -Dversion=1.0.0 -Dpackaging=jar -Dfile=bjcaprotobuf-1.0.0.jar 将jar部署到自己的maven私服中mvn deploy:deploy-file -DgroupId=com.ibm.db2 -DartifactId=db2jcc -Dversion=3.59.81 -Dpackaging=jar -Dfile=db2jcc-3.59.81.jar -Durl=http://112.124.0.156:8081/nexus/content/repositories/releases -DrepositoryId=deployRelease","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"常用","slug":"常用","permalink":"http://yoursite.com/tags/常用/"},{"name":"Maven","slug":"Maven","permalink":"http://yoursite.com/tags/Maven/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"风险模型与计量基础","slug":"003风控/风险模型与计量","date":"2019-01-15T00:00:00.000Z","updated":"2019-02-18T04:35:17.431Z","comments":true,"path":"category/003风控/风险模型与计量.html","link":"","permalink":"http://yoursite.com/category/003风控/风险模型与计量.html","excerpt":"","text":"风险模型与计量基础","categories":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}],"tags":[{"name":"风险管理","slug":"风险管理","permalink":"http://yoursite.com/tags/风险管理/"}],"keywords":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}]},{"title":"风险管理基础","slug":"003风控/风险管理基础","date":"2019-01-15T00:00:00.000Z","updated":"2019-02-18T04:35:22.977Z","comments":true,"path":"category/003风控/风险管理基础.html","link":"","permalink":"http://yoursite.com/category/003风控/风险管理基础.html","excerpt":"","text":"风险管理基础","categories":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}],"tags":[{"name":"风险管理","slug":"风险管理","permalink":"http://yoursite.com/tags/风险管理/"}],"keywords":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}]},{"title":"决策系统设计思考","slug":"006系统设计/决策系统设计思考","date":"2019-01-15T00:00:00.000Z","updated":"2019-02-18T05:34:17.039Z","comments":true,"path":"category/006系统设计/决策系统设计思考.html","link":"","permalink":"http://yoursite.com/category/006系统设计/决策系统设计思考.html","excerpt":"","text":"系统愿景 业务整体 应用架构 分成三个子系统： 模型管理系统 提供web界面进行模型的配置，如规则的定义，规则树的定义等。其依赖“模型运行服务”提供的规则管理服务。 模型运行服务 提供其他系统调用的服务接口（dubbo)。如模型运行服务。依赖规则解释执行服务。 模型解释执行服务 提供dubbo接口，规则的预加载，具体规则的执行逻辑。 功能模块","categories":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}],"tags":[{"name":"规则系统","slug":"规则系统","permalink":"http://yoursite.com/tags/规则系统/"}],"keywords":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}]},{"title":"金融市场与产品知识体系","slug":"003风控/金融市场与产品","date":"2019-01-15T00:00:00.000Z","updated":"2019-02-18T04:35:11.942Z","comments":true,"path":"category/003风控/金融市场与产品.html","link":"","permalink":"http://yoursite.com/category/003风控/金融市场与产品.html","excerpt":"","text":"金融市场与产品知识体系","categories":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}],"tags":[{"name":"风险管理","slug":"风险管理","permalink":"http://yoursite.com/tags/风险管理/"}],"keywords":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}]},{"title":"时间序列知识体系","slug":"003风控/时间序列知识体系","date":"2019-01-15T00:00:00.000Z","updated":"2019-02-18T04:34:54.394Z","comments":true,"path":"category/003风控/时间序列知识体系.html","link":"","permalink":"http://yoursite.com/category/003风控/时间序列知识体系.html","excerpt":"","text":"时间序列知识体系","categories":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}],"tags":[{"name":"风险管理","slug":"风险管理","permalink":"http://yoursite.com/tags/风险管理/"}],"keywords":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}]},{"title":"统计基础","slug":"003风控/统计基础","date":"2019-01-15T00:00:00.000Z","updated":"2019-02-18T04:35:05.979Z","comments":true,"path":"category/003风控/统计基础.html","link":"","permalink":"http://yoursite.com/category/003风控/统计基础.html","excerpt":"","text":"统计基础知识体系 统计学为我们做什么？（意义） 收集数据，知道整体情况 数据分析，目的是作出判断和预测（重要） 统计的目的是为了做出决策。 如何描述数据？原始数据不好观察，需要进行数据的压缩。两个方法： 制作”图”, 求”统计量”。 制作图，使用直方图，频数分布表。缺点：1.抽象的程度无法顺利表达2.表达的需要的空间很大 求统计量，用数字来概括数据的特征。 定量分析 定性分析。定量分析是依据统计数据,建立数学模型,并用数学模型计算出分析对象的各项指标及其数值的一种方法。 定性分析则是主要凭分析者的直觉、经验，凭分析对象过去和现在的延续状况及最新的信息资料，对分析对象的性质、特点、发展变化规律作出判断的一种方法。 二者的关系：现代定性分析方法同样要采用数学工具进行计算，而定量分析则必须建立在定性预测基础上。 描述性统计和推断性统计描述性统计—-对数据的描述，从取得的数据中抽取其特征的技术。（整体到部分） 推断性统计—-用做判断和预测。（部分推断整体） 描述统计学是使用特定的数字或图表来体现数据的集中程度或离散程度。例：每次考试算的平均分，最高分，各个分段的人数分布等，也是属于描述统计学的范围。 推断统计学是根据样本数据来推断总体特征。例：产品质量检查，一般采用抽检，根据所抽样本的质量合格率作为总体的质量合格率的一个估计。 统计和概率的关系？统计是观察所得数据的集合，是对于过去发生的事件的描述。概率：对未来将发生事件的描述。 如何概率理论相结合？ 统计流程收集数据，分析，做出结论 分析工具（基本统计量）集中趋势——&gt;平均值分散性和变异性——&gt;方差机会——&gt;概率预测长期结果——&gt;期望 图 基本概念总体和样本总体参数和样本统计量很多时候总体是不知道的。如何通过部分来推论整体？几种情况？ 分布正态分布为什么正态分布使用频率高？理论价值高，降低计算的复杂性。应用价值高，实践应用中，大量的结果遵循（或近似于）正态分布。 ##中心极限定理 参考网站","categories":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}],"tags":[{"name":"统计学","slug":"统计学","permalink":"http://yoursite.com/tags/统计学/"},{"name":"数学","slug":"数学","permalink":"http://yoursite.com/tags/数学/"}],"keywords":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}]},{"title":"线性回归知识体系","slug":"003风控/线性回归知识体系","date":"2019-01-15T00:00:00.000Z","updated":"2019-02-18T04:35:05.983Z","comments":true,"path":"category/003风控/线性回归知识体系.html","link":"","permalink":"http://yoursite.com/category/003风控/线性回归知识体系.html","excerpt":"","text":"线性回归知识体系","categories":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}],"tags":[{"name":"风险管理","slug":"风险管理","permalink":"http://yoursite.com/tags/风险管理/"}],"keywords":[{"name":"FRM学习之路","slug":"FRM学习之路","permalink":"http://yoursite.com/categories/FRM学习之路/"}]},{"title":"Scikit-Learn入门学习","slug":"001机器学习/Scikit-Learn入门","date":"2019-01-09T00:00:00.000Z","updated":"2019-01-23T13:25:00.551Z","comments":true,"path":"category/001机器学习/Scikit-Learn入门.html","link":"","permalink":"http://yoursite.com/category/001机器学习/Scikit-Learn入门.html","excerpt":"","text":"Scikit-Learn基本学习Scikit-Learn 接口设计Scikit-Learn 设计的 API 设计的非常好。它的主要设计原则是： 一致性：所有对象的接口一致且简单 可检验。所有估计器的超参数都可以通过实例的public变量直接访问（比如，imputer.strategy），并且所有估计器学习到的参数也可以通过在实例变量名后加下划线来访问（比如，imputer.statistics_）。 类不可扩散。只有算法可以用 Python 类表示。数据集都用标准数据类型（NumPy 数组、Pandas DataFrame 、SciPy 稀疏矩阵）表示，参数名称用标准的 Python 字符串。 可组合。尽可能使用现存的模块。例如，用任意的转换器序列加上一个估计器，就可以做成一个流水线，后面会看到例子。 合理的默认值。Scikit-Learn 给大多数参数提供了合理的默认值，很容易就能创建一个系统。 Scikit-Learn中对象 估计器（estimator）。任何可以基于数据集对一些参数进行估计的对象都被称为估计器（比如，imputer就是个估计器）。估计本身是通过fit()方法，只需要一个数据集作为参数（对于监督学习算法，需要两个数据集；第二个数据集包含标签）。 转换器（transformer）。一些估计器（比如imputer）也可以转换数据集，这些估计器被称为转换器。API也是相当简单：转换是通过transform()方法，被转换的数据集作为参数。返回的是经过转换的数据集。所有的转换都有一个便捷的方法fit_transform()，等同于调用fit()再transform()。 预测器（predictor）。最后，一些估计器可以根据给出的数据集做预测，这些估计器称为预测器。例如，LinearRegression模型就是一个预测器：预测器有一个predict()方法，可以用新实例的数据集做出相应的预测。预测器还有一个score()方法。 Scikit-Learn的评估器APIScikit-Learn 评估器 API 的常用步骤如下所示： (1) 通过从 Scikit-Learn 中导入适当的评估器类，选择模型类。 (2) 用合适的数值对模型类进行实例化，配置模型超参数（hyperparameter）。 (3) 整理数据，通过前面介绍的方法获取特征矩阵和目标数组。 (4) 调用模型实例的 fit() 方法对数据进行拟合。 (5) 对新数据应用模型： 在有监督学习模型中，通常使用 predict() 方法预测新数据的标签； 在无监督学习模型中，通常使用 transform() 或 predict() 方法转换或推断数据的性质 例子：简单线性回归123%matplotlib inlineimport matplotlib.pyplot as pltimport numpy as np 12345rng = np.random.RandomState(42)x = 10 * rng.rand(50)y = 2 * x - 1 + rng.randn(50)plt.scatter(x, y);plt.show() (1) 选择模型类 1from sklearn.linear_model import LinearRegression (2) 选择模型超参数 12model = LinearRegression(fit_intercept=True)model LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) (3) 将数据整理成特征矩阵和目标数组 12X = x[:, np.newaxis]X.shape (50, 1) (4) 用模型拟合数据 1model.fit(X, y) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) fit() 命令会在模型内部进行大量运算，运算结果将存储在模型属性中，供用户使用。在 Scikit-Learn 中，所有通过 fit() 方法获得的模型参数都带一条下划线。 1model.coef_ array([1.9776566]) 1model.intercept_ -0.9033107255311146 1上面的输出，分别表示对样本数据拟合直线的斜率和截距。 (5) 预测新数据的标签 模型训练出来之后，有监督机器学习的主要任务就变成了对不属于训练集的新数据进行预测。在 Scikit-Learn 中，我们用 predict() 方法进行预测。 12xfit = np.linspace(-1, 11)xfit array([-1. , -0.75510204, -0.51020408, -0.26530612, -0.02040816, 0.2244898 , 0.46938776, 0.71428571, 0.95918367, 1.20408163, 1.44897959, 1.69387755, 1.93877551, 2.18367347, 2.42857143, 2.67346939, 2.91836735, 3.16326531, 3.40816327, 3.65306122, 3.89795918, 4.14285714, 4.3877551 , 4.63265306, 4.87755102, 5.12244898, 5.36734694, 5.6122449 , 5.85714286, 6.10204082, 6.34693878, 6.59183673, 6.83673469, 7.08163265, 7.32653061, 7.57142857, 7.81632653, 8.06122449, 8.30612245, 8.55102041, 8.79591837, 9.04081633, 9.28571429, 9.53061224, 9.7755102 , 10.02040816, 10.26530612, 10.51020408, 10.75510204, 11. ]) 12Xfit = xfit[:, np.newaxis]Xfit array([[-1. ], [-0.75510204], [-0.51020408], [-0.26530612], [-0.02040816], [ 0.2244898 ], [ 0.46938776], [ 0.71428571], [ 0.95918367], [ 1.20408163], [ 1.44897959], [ 1.69387755], [ 1.93877551], [ 2.18367347], [ 2.42857143], [ 2.67346939], [ 2.91836735], [ 3.16326531], [ 3.40816327], [ 3.65306122], [ 3.89795918], [ 4.14285714], [ 4.3877551 ], [ 4.63265306], [ 4.87755102], [ 5.12244898], [ 5.36734694], [ 5.6122449 ], [ 5.85714286], [ 6.10204082], [ 6.34693878], [ 6.59183673], [ 6.83673469], [ 7.08163265], [ 7.32653061], [ 7.57142857], [ 7.81632653], [ 8.06122449], [ 8.30612245], [ 8.55102041], [ 8.79591837], [ 9.04081633], [ 9.28571429], [ 9.53061224], [ 9.7755102 ], [10.02040816], [10.26530612], [10.51020408], [10.75510204], [11. ]]) 将这些x值转换成[n_samples, n_features]的特征矩阵形式，之后将其输入到模型中 12yfit = model.predict(Xfit)yfit array([-2.88096733, -2.39664326, -1.9123192 , -1.42799513, -0.94367106, -0.459347 , 0.02497707, 0.50930113, 0.9936252 , 1.47794926, 1.96227333, 2.44659739, 2.93092146, 3.41524552, 3.89956959, 4.38389366, 4.86821772, 5.35254179, 5.83686585, 6.32118992, 6.80551398, 7.28983805, 7.77416211, 8.25848618, 8.74281024, 9.22713431, 9.71145837, 10.19578244, 10.68010651, 11.16443057, 11.64875464, 12.1330787 , 12.61740277, 13.10172683, 13.5860509 , 14.07037496, 14.55469903, 15.03902309, 15.52334716, 16.00767122, 16.49199529, 16.97631936, 17.46064342, 17.94496749, 18.42929155, 18.91361562, 19.39793968, 19.88226375, 20.36658781, 20.85091188]) 把原始数据和拟合结果都可视化出来 12plt.scatter(x, y)plt.plot(xfit, yfit) [&lt;matplotlib.lines.Line2D at 0x117ed9c50&gt;] 12 参考网站http://baijiahao.baidu.com/s?id=1600375182495419726&amp;wfr=spider&amp;for=pc","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Scikit-Learn","slug":"Scikit-Learn","permalink":"http://yoursite.com/tags/Scikit-Learn/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"机器学习43条军规","slug":"001机器学习/机器学习43条军规","date":"2019-01-08T00:00:00.000Z","updated":"2019-02-18T04:36:23.309Z","comments":true,"path":"category/001机器学习/机器学习43条军规.html","link":"","permalink":"http://yoursite.com/category/001机器学习/机器学习43条军规.html","excerpt":"","text":"本文转自https://blog.csdn.net/qq_39871625/article/details/78029392原文标题&lt;&lt;机器学习法则：（谷歌）机器学习工程最佳实践（译）&gt;&gt; 概览要打造优质的产品：请把自己看成是一位出色的工程师，而不是一位机器学习专家。 实际上，您将面临的大部分问题都是工程问题。即使在使用出色的机器学习专家掌握的所有资源的情况下，大多数收获也是由合适的特征（而非精确的机器学习算法）带来的。所以，进行机器学习的基本方法是： 确保机器学习流程从头到尾都稳固可靠。 从制定合理的目标开始。 以简单的方式添加常识性特征。 确保机器学习流程始终稳固可靠。 上述方法将在长时间内取得很好的效果。只要您仍然可以通过某种简单的技巧取得进展，就不应该偏离上述方法。增加复杂性会减缓未来版本的发布。 当您充分利用了所有的简单技巧，或许就到了探索机器学习最前沿技术的时候了。 本文档结构如下： 第一部分可帮助您了解构建机器学习系统的时机是否已经成熟。 第二部分介绍了如何部署第一个机器学习流程。 第三部分介绍了在向机器学习流程添加新特征时如何进行发布和迭代、如何评估模型，以及如何应对训练-应用偏差。 最后一部分介绍了当您达到稳定阶段时该怎么做。 机器学习之前Rule #1: Don’t be afraid to launch a product without machine learning. 规则1：不要害怕上线没有机器学习的产品。中心思想一句话概括：If you think that machine learning will give you a 100% boost, then a heuristic will get you 50% of the way there. Rule #2: First, design and implement metrics. 规则2：在动手之前先设计和实现评价指标。在构建具体的机器学习系统之前，首先在当前系统中记录尽量详细的历史信息，留好特征数据。这样不仅能够留好特征数据，还能够帮助我们随时了解系统的状态，以及做各种改动时系统的变化。 Rule #3: Choose machine learning over a complex heuristic. 规则3：不要使用过于复杂的规则系统，使用机器学习系统。简单来讲，复杂的规则系统难以维护，不可扩展，而我们很简单就可以转为ML系统，变得可维护可扩展。 机器学习第一阶段：工作流（Pipeline）ML Phase I: Your First Pipeline构建第一个ML系统时，一定要更多关注系统架构的建设。虽然机器学习的算法令人激动，但是基础架构不给力找不到问题时会令人抓狂。 Rule #4: Keep the first model simple and get the infrastructure right. 规则4：第一个模型要简单，但是架构要正确。第一版模型的核心思想是抓住主要特征、与应用尽量贴合以及快速上线。 Rule #5: Test the infrastructure independently from the machine learning. 规则5：独立于机器学习来测试架构流程。确保架构是可单独测试的，将系统的训练部分进行封装，以确保其他部分都是可测试的。特别来讲： 测试数据是否正确进入训练算法。检查具体的特征值是否符合预期。测试实验环境给出的预测结果与线上预测结果是否一致。Rule #6: Be careful about dropped data when copying pipelines. 规则6：复制pipeline时要注意丢弃的数据。从一个场景复制数据到另一个场景时，要注意两边对数据的要求是否一致，是否有数据丢失的情况。 Rule #7: Turn heuristics into features, or handle them externally. 规则7：将启发规则转化为特征，或者在外部处理它们。机器学习系统解决的问题通常都不是新问题，而是对已有问题的进一步优化。这意味着有很多已有的规则或者启发式规则可供使用。这部分信息应该被充分利用（例如基于规则的推荐排序时用到的排序规则）。下面是几种启发式规则可以被使用的方式： 用启发规则进行预处理。如果启发式规则非常有用，可以这么用。例如在垃圾邮件识别中，如果有发件人已经被拉黑了，那么就不要再去学“拉黑”意味着什么，直接拉黑就好了。制造特征。可以考虑从启发式规则直接制造一个特征。例如，你使用启发式规则来计算query的相关性，那么就可以把这个相关性得分作为特征使用。后面也可以考虑将计算相关性得分的原始数据作为特征，以期获得更多的信息。挖掘启发式规则的原始输入。如果有一个app的规则启发式规则综合了下载数、标题文字长度等信息，可以考虑将这些原始信息单独作为特征使用。修改label。当你觉得启发式规则中包含了样本中没有包含的信息时可以这么用。例如，如果你想最大化下载数，同时还想要追求下载内容的质量。一种可行的方法是将label乘以app的平均star数。在电商领域，也常常用类似的方法，例如在点击率预估的项目中，可考虑对最终下单的商品或者高质量的商品对应的样本增加权重。已有的启发式规则可以帮助机器学习系统更平滑的过渡，但是也要考虑是否有同等效果更简单的实现方式。 机器学习第一阶段：监控Monitoring概括来讲，要保持好的监控习惯，例如使报警是可应对的，以及建设一个Dashboard页面。 Rule #8: Know the freshness requirements of your system. 规则8：了解你系统对新鲜度的要求。如果模型延迟一天更新，你的系统会受到多大的效果影响？如果是一周的延迟呢？或者更久？这个信息可以让我们排布监控的优先级。如果模型一天不更新收入就会下降10%，那么可以考虑让一个工程师全天候监控它。了解系统对新鲜度的要求是决定具体监控方案的第一步。 Rule #9: Detect problems before exporting models. 规则9：在模型上线之前检测问题。模型上线前一定要做完整性、正确性检查，例如AUC、Calibration、NE等指标的计算确认等。如果是模型上线前出了问题，可以邮件通知，如果是用户正在使用的模型出了问题，就需要电话通知了。 Rule #10: Watch for silent failures. 规则10：关注静默失败。这是一个非常重要，而又经常容易被忽略的问题。所谓的静默失败指的是全部流程都正常完成，但是背后依赖数据出了问题，导致模型效果逐步下降的问题。这种问题在其他系统中并不常出现，但是在机器学习系统中出现几率会比较高。例如训练依赖的某张数据表很久没有更新了，或者表中的数据含义发生了变化等，再或者数据的覆盖度忽然变少，都会对效果产生很大的影响。解决方法是是对关键数据的统计信息进行监控，并且周期性对关键数据进行人工检查。 Rule #11: Give feature column owners and documentation. 规则11：给特征组分配负责人，并记录文档。这里的feature column指的是一个特征组，例如用户可能属于的国家这组特征就是一个feature column。 如果系统庞大，数据繁多，那么知道每组数据由谁生成就变得非常重要。虽然数据都有简单描述，但是关于特征的具体计算逻辑，数据来源等都需要更详细的记录。 机器学习第一阶段：优化目标Your Fist Objectiveobjective是模型试图优化的值，而metric指的是任何用来衡量系统的值。 Rule #12: Don’t overthink which objective you choose to directly optimize. 规则12：不要过于纠结该优化哪个目标。机器学习上线的初期，即使你只优化一个目标，很多指标一般都会一起上涨的。所以不用太纠结究竟该优化哪个。 虽然大佬这么说，但是在我自己的实践经验中，只优化一个目标，系统的整体效果却未必会上涨。典型的如推荐系统的CTR模型，上线之后CTR确实会提升，但是对应的CVR很有可能会下降，这时还需要一个CVR模型，两个模型同时使用才能真正提升系统效果。究其原因，是因为每个目标只关注系统整个过程的一个子过程，贪心地去优化这个子过程，不一定能够得到全局的最优解，通常需要把主要的几个子过程都优化之后，才能取得整体效果的提升。 Rule #13: Choose a simple, observable and attributable metric for your first objective. 规则13：为你的第一个objective选择一个简单可观测可归因的metric。objective应该是简单可衡量的，并且是metric的有效代理。最适合被建模的是可直接观测并被归因的行为，例如： 链接是否被点击？软件是否被下载？邮件是否被转发？……尽量不要在第一次就建模非直接效果的行为，例如： 用户第二天是否会访问？用户在网站上停留了多久？日活用户有多少？非直接指标是很好的metric，可以用ABTest来进行观测，但不适合用作优化指标。此外，千万不要试图学习以下目标： 用户对产品是否满意？用户对体验是否满意？……这些指标非常重要，但是非常难以学习。应该使用一些代理指标来学习，通过优化代理指标来优化这些非直接指标。为了公司的发展着想，最好有人工来连接机器学习的学习目标和产品业务。 Rule #14: Starting with an interpretable model makes debugging easier. 规则14：使用可解释性强的模型可降低debug难度。优先选择预测结果有概率含义、预测过程可解释的模型，可以更容易的确认效果，debug问题。例如，如果使用LR做分类，那么预测过程不外乎一些相乘和相加，如果特征都做了离散化，就只有加法了，这样很容易debug一条样本的预测得分是如何被计算出来的。所以出了问题很容易debug。 Rule #15: Separate Spam Filtering and Quality Ranking in a Policy Layer. 规则15：将垃圾过滤和质量排序的工作分离，放到策略层（policy layer）。排序系统工作的环境中数据分布是相对静态的，大家为了得到更好的排序，会遵守系统制定的规则。但是垃圾过滤更多是个对抗性质的工作，数据分布会经常变动。所以不应该让排序系统去处理垃圾信息的过滤，而是应该有单独的一层去处理垃圾信息。这也是一种可以推广的思想，那就是：排序层只做排序层的事情，职责尽量单一，其他工作让架构上更合适的模块去处理。此外，为了提升模型效果，应该把垃圾信息从训练数据中去除。 机器学习第二阶段：特征工程ML Phase II: Feature Engineering前面第一阶段的重点是把数据喂到学习系统中，有了基础的监控指标，有了基础的架构。等这一套系统建立起来后，第二阶段就开始了。 整体来讲，第二阶段的核心工作是将尽量多的有效特征加入到第一版的系统中，一般都可以取得提升。 Rule #16: Plan to launch and iterate. 规则16：做好持续迭代上线的准备。简单来说，就是要深刻认识到，系统优化永远没有终点，所以系统设计方面要对迭代非常友好。例如增加删除特征是否足够简单，正确性验证是否足够简单，模型迭代是否可以并行运行，等等。 这虽然不是一条具体可行动的（actionable）规则，但是这种思想上的准备对整个系统的开发很有帮助。只有真正深刻意识到了系统持续迭代上线的本质，才会在设计在线和离线架构时为持续迭代最好相应的设计，并做好相应的工具，而不是做一锤子系统。 Rule #17: Start with directly observed and reported features as opposed to learned features. 规则17：优先使用直接观测或收集到的特征，而不是学习出来的特征。所谓学习出来的特征，指的是用另外的算法学习出来的特征，而非可以直接观测或收集到的简单特征。学习出来的特征由于存在外部依赖，或者计算逻辑复杂，不一定适用于你当前的模型，所以稳定性和有效性会有风险。而直接可观测的特征由于是相对比较客观的，依赖较少的，所以比较稳定。 Rule #18: Explore with features of content that generalize across contexts. 规则18：探索使用可以跨场景的内容特征。中心思想是在说，要多利用可以在多个场景下使用的特征，例如全局的点击率、浏览量这些特征，可以在多个场景下作为特征使用。这样可以在一些冷启动或者缺乏有效特征的场景下作为特征使用。 Rule #19: Use very specific features when you can. 规则19：尽量使用非常具体的特征。如果数据量足够大，那么相比少数复杂特征，使用海量简单特征是更简单有效的选择。 所谓非常具体，指的是覆盖样本量比较少的特征，例如文档的ID或者query的ID等。这样的特征虽然每个只覆盖很少一部分特征，但是只要这一组特征整体能够覆盖率比较高，例如90%，那就是OK的。而且还可以通过正则化来消除覆盖率过低或者相关性差的特征。这也是大家都偏爱大规模ID特征的一个原因，现在很多大厂的排序模型特征都大量使用了大规模ID特征。 Rule #20: Combine and modify existing features to create new features in human­-understandable ways. 规则20：用人类可理解的方式对已有特征进行组合、修改来得到新特征。离散化和交叉是最常用的两种特征使用方式。其本质都是用特征工程的方式，在不改变使用模型本身的情况下增加模型的非线性。这两种方法本身没什么好说的，值得一致的是，在大规模ID类特征的交叉时，例如一段是query里的关键词，另一端是文档里的关键词，那就会产生很大量级的交叉特征，这时有两种处理方法： 点积。其实计算query和文档共同包含的关键词数量。交集。每一维特征的含义是某个词同时出现在了query和文档中，同时出现则该维特征为1，否则为0。所谓“人类可理解的方式”，我的理解就是离散化和交叉要基于对业务逻辑的理解，不能乱交叉。 Rule #21: The number of feature weights you can learn in a linear model is roughly proportional to the amount of data you have. 规则21：线性模型中可学到的特征权重数量，与训练数据的数量大体成正比。这背后有复杂的统计原理做支撑，但你只需要知道结论就可以了。这个原则给我们的启示，是要根据数据量来选择特征的生成方式，例如： 如果你的系统是一个搜索系统，query和文档中有百万级的词，但是你只有千级别的标注样本。那你就别用ID级关键词特征了，而是要考虑点积类特征，把特征数量控制在几十个这个级别。如果你拥有百万级样本，那么可以将文档和query的关键词进行交叉特征，然后用正则化进行特征选择。这样你会得到百万级特征，但是正则化之后会更少。所以说，千万级样本，十万级特征。如果你有十亿级或者更高级别的样本，那么你可以使用query和文档的ID级特征，然后加上特征选择和正则化。十亿级样本，千万级特征。总结起来就是，根据样本决定特征使用方式，样本不够就对特征进行高层次抽象处理，指导和样本量级相匹配。 Rule #22: Clean up features you are no longer using. 规则22：清理不再使用的特征。如果某个特征已经没有用，并且它与其他特征的交叉也已经没有用，就应该将其清理掉，保持架构的整洁性。 在考虑添加或保留哪些特征时，需要统计一下特征的样本覆盖率，例如一些整体覆盖率很低的个性化feature column，只有很少用户能覆盖到，那么大概率这组特征作用不大。但另一方面，如果某个特征覆盖率很低，例如只有1%，但是其区分度非常大，例如90%取值为1的样本都是正样本，那么 这个特征就值得加入或保留。 机器学习第二阶段：系统的人工分析Human Analysis of the System在更进一步之前，我们需要了解一些机器学习课程上不会教你的内容：如何观察分析模型，并改进它。用作者的话说，这更像是一门艺术 ，但仍然有一些规律可循。 Rule #23: You are not a typical end user. 规则23：你不是一个典型的终端用户。这条规则的中心思想是说，虽然吃自己的狗食是必要的，但也不要总是从工程师的角度来衡量模型的好坏。这不仅可能不值当，而且可能看不出问题。所谓不值当，是因为工程师的时间太贵了，这个大家都懂；而所谓看不出问题，是因为工程师自己看自己开发的模型，容易看不出问题，所谓“不识庐山真面目”。 所以作者认为合理的方法是让真正的终端用户来衡量模型或产品的好坏。要么通过线上ABTest，要么通过众包的方式来做。 Rule #24: Measure the delta between models. 规则24：离线衡量模型之间的差异。原文没有说是离线，但我通过上下文理解他说的应该是离线。这一条规则说的是新模型在上线之前，需要先和老模型做差异对比。所谓差异对比，指的是对于同样的输入，新旧两个模型给出的结果是否差异足够大。例如对于同一个query，两个排序模型给出的差异是否足够大。如果离线计算发现差异很小，那也没必要上线测试了，因为上线后差异肯定也大不了。如果差异比较大，那么还需要看差异是不是好的差异。通过观察差异可以得知新模型究竟对数据产生了什么影响，这种影响是好是坏。 当然，这一切的前提是你需要有一个稳定的对比系统， 起码一个模型和他自己对比的话差异应该非常小，最好是零差异。 Rule #25: When choosing models, utilitarian performance trumps predictive power. 规则25：当选择模型时，实用性指标比预测能力更重要。这是一条很有用的经验。虽然我们训练模型时objective一般都是logloss，也就是说实在追求模型的预测能力。但是我们在上层应用中却可能有多种用途，例如可能会用来排序，那么这时具体的预测能力就不如排序能力重要；如果用来划定阈值然后跟根据阈值判断垃圾邮件，那么准确率就更重要。当然大多数情况下这几个指标是一致的。 除了作者说的这一点，还有一种情况是需要特别注意的，那就是我们在训练时可能会对样本做采样，导致得到的预测值整体偏高或偏低。如果这个预测值是用来直接排序的，那么这个变化关系不大，但如果有其他用处，例如和另外的值相乘，典型的如广告场景下的CTRbid，或者电商推荐排序下的CTRCVR，在这类场景下，预测值本身的准确性也很重要，就需要对其进行校准（calibrate），使其与采样前的样本点击率对齐。 Rule #26: Look for patterns in the measured errors, and create new features. 规则26：在错误中发现模式，并创建新特征。这算是一种用来提升模型效果的通用思路。具体来说，指的是观察训练数据中模型预测错误的样本，看看是否能够通过添加额外特征来使得这条样本被模型预测正确。之所以使用训练集中的数据，是因为这部分数据是模型已经试图优化过的，这里面的错误，是模型知道自己搞错了，目前学不出来的，所以如果你给它足够好的其他特征，它或许就能把这条样本学对了。 一旦发现错误的模式，就可以在当前系统之外寻找新的特征。例如，如果你发现当前系统倾向于错误地把长文章排到后面，那么就可以加入文章长度这一特征，让系统去学习文章长度的相关性和重要性。 Rule #27: Try to quantify observed undesirable behavior. 规则27：尽量将观测到的负面行为量化。如果在系统中观察到了模型没有优化到的问题，典型的例如推荐系统逼格不够这种问题，这时应该努力将这种不满意转化为具体的数字，具体来讲可以通过人工标注等方法标注出不满意的物品，然后进行统计。如果问题可以被量化，后面就可以将其用作特征、objective或者metric。整体原则就是“先量化，再优化”。 多说一句，这里的优化，不一定是用模型来优化，而是指的整体优化。比如推荐系统逼格这种问题，模型可能很难优化，但是只要能量化出来，就可以通过其他方法来尽量减少，例如单独去学习有逼格物品的特征，或者在召回阶段进行一定倾斜。 Rule #28: Be aware that identical short-­term behavior does not imply identical long­-term behavior. 规则28：要注意短期内观察到的类似行为不一定会长期存在。假设你搞了个系统，通过学习每个具体的文档ID和query ID，计算出了每个query下每个文档的点击率。通过离线对比和ABTest，你发现这个系统的行为和当前系统的行为一毛一样，而这个系统又更简单，所以你就把这个系统上线了。后面你会发现这个系统在任何query下都不会给出任何新的文档。奇怪吗？一点都不奇怪，因为你只让它记住了之前的历史数据，它对新数据没有任何信息。 所以唯一能够衡量一个系统是否长期有效的方法，就是让它使用该模型在线上时收集到的真实数据上进行训练。当然这有难度。 往大了说，作者这条规则其实说的是个系统或者模型的泛化能力，如果一个系统或者模型不能对新数据做出很好的预测，那么无论他的离线表现如何，都不能代表它的真正能力。再换个角度来看，一个系统必须具有持续学习适应新数据的能力，才能是一个合格的机器学习系统，否则就只是个学舌的鹦鹉。 机器学习第二阶段：训练偏差Training-Serving Skew训练和服务之间的差异问题时一个大话题，主要原因包括训练时与服务时数据获取方式不同、训练时与服务时数据分布不同以及模型和算法之间的反馈循环等。作者说这种差异在G家的多条产品线上出现过，都产生了负面影响。但要我说这绝对不仅仅是谷歌的问题，谷歌绝对是做的比较好的了，各种中小厂里面这种问题只多不少，所以这部分的经验是非常宝贵的。解决这类问题的核心是对系统和数据的变化进行监控，确保一切差异都在监控之内，不会悄悄进入系统。 Rule #29: The best way to make sure that you train like you serve is to save the set of features used at serving time, and then pipe those features to a log to use them at training time. 规则29：保证服务与训练一致性的最好方法是将服务时的特征保存下来，然后通过日志将特征喂到训练过程中去。这句话基本道出了保证差异最小化的核心套路。这种基于特征日志的方法可以极大提升效果，同时能够减少代码复杂度。谷歌的很多团队也正在往这种做法上迁移。 Rule #30: Importance weight sampled data, don’t arbitrarily drop it! 规则30：对采样样本做重要性赋权，不要随意丢弃！这是作者唯一用了感叹号的一条，可想而知背后的辛酸。当我们有太多训练数据时，我们会只取其中的一部分。但这是错误的。正确的做法是，如果你给某条样本30%的采样权重，那么在训练时就给它10/3的训练权重。通过这样的重要性赋权（importance weight），整个训练结果的校准性（calibration）就还能够保证。 多说一句，这个校准性非常的重要，尤其对于广告系统，或者多个预测值相加或相乘来得到最终结果的系统。如果单个值没有校准，偏低或偏高，那么在相乘或相加之后其含义就会不正确。如果直接使用模型预测值进行排序，校准性就没那么重要，因为校准性不会影响排序，只会影响具体的值。 Rule #31: Beware that if you join data from a table at training and serving time, the data in the table may change. 规则31：如果你在训练时和服务时都在join一张表，那么要注意这张表的数据可能会发生变化。比如说某张表里存着一些文档的特征，你在离线训练之前要去这个表里取这些特征用来训练，但这里就有个风险，那就是这个表里的数据在你离线取的时候和在线服务的时候数据不一样，发生了变化。最好的解决方式就是在服务端将特征记录在日志中，这样能保证数据的一致性。或者如果这张表的变化频率比较低，也可以考虑对其做小时级或天级备份，以此来减少这种差异。但要记住这种方法并不能彻底解决这个问题。 Rule #32: Re­use code between your training pipeline and your serving pipeline whenever possible. 规则32：尽量在训练pipeline和服务pipeline之间复用代码。训练一般是离线批量进行的，而服务则是在线流式进行的，这两者之间虽然在处理数据的方式上存在着较大差异，但仍然有很多代码可以共享。这些代码的共享可以从代码层面介绍训练和服务之间的差异。换句话说，日志记录特征是从数据角度消除差异，那么代码复用就是从代码角度消除差异，双管齐下，效果更好。 Rule #33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after. 规则33：如果训练数据是1月5日之前的，那么测试数据要从1月6日开始。这条规则的主要目的是让测试结果与线上结果更加接近，因为我们在使用模型时就是在用服务当天之前的数据训练，然后来预测当天的数据。这样得到的测试结果虽然可能会偏低，但却更加真实。 Rule #34: In binary classification for filtering (such as spam detection or determining interesting e­mails), make small short-­term sacrifices in performance for very clean data. 规则34：在为过滤服务的二分类问题中（例如垃圾邮件过滤），可以为了干净的数据牺牲一些短期效果。在过滤类的任务中，被标记为负的样本是不会展示给用户的，例如可能会把75%标记为负的样本阻拦住不展现给用户。但如果你只从展示给用户的结果中获取下次训练的样本，显然你的训练样本是有偏的。 更好的做法是使用一定比例的流量（例如1%）专门收集训练数据，在这部分流量中的用户会看到所有的样本。这样显然会影响线上的真实过滤效果，但是会收集到更好的数据，更有利于系统的长远发展。否则系统会越训练越偏，慢慢就不可用了。同时还能保证至少过滤掉74%的负样本，对系统的影响也不是很大。 但是如果你的系统会过滤掉95%或者更多的负样本，这种做法就不那么可行了。即使如此，为了准确衡量模型的效果，你仍然可以通过构造一个更小的数据集（0.1%或者更小）来测试。十万级别的样本足够给出准确的评价指标了。 Rule #35: Beware of the inherent skew in ranking problems. 规则35：注意排序问题中固有的数据偏置。当新的排序算法对线上排序结果产生了重大改变时，你其实是改变了算法将来会看到的数据。这时这种偏置就会出现。这种问题有以下几种方法来解决，核心思想都是更偏重模型已经看到过的数据。 对覆盖更多query（或类似角色，根据业务不同）的特征给予更强的正则化。这样模型会更偏重只覆盖一部分样本的特征，而不是泛化性特征。这样会阻止爆品出现在不相关query的结果中。只允许特征取正的权重值。这样任何好特征都会比“未知”特征要好。不要使用只和文档相关的特征。这是第一条的极端情况，否则会导致类似哈利波特效应的情况出现，也就是一条在任何query下都受欢迎的文档不会到处都出现。去除掉只和文档相关的特征会阻止这种情况发生。Rule #36: Avoid feedback loops with positional features. 规则36：使用位置特征来避免反馈回路。大家都知道排序位置本身就会影响用户是否会对物品产生互动，例如点击。所以如果模型中没有位置特征，本来由于位置导致的影响会被算到其他特征头上去，导致模型不够准。可以用加入位置特征的方法来避免这种问题，具体来讲，在训练时加入位置特征，预测时去掉位置特征，或者给所有样本一样的位置特征。这样会让模型更正确地分配特征的权重。 需要注意的是，位置特征要保持相对独立，不要与其他特征发生关联。可以将位置相关的特征用一个函数表达，然后将其他特征用另外的函数表达，然后组合起来。具体应用中，可以通过位置特征不与任何其他特征交叉来实现这个目的。 Rule #37: Measure Training/Serving Skew. 规则37：衡量训练和服务之间的差异。整体来讲有多种原因会导致这种差异，我们可以将其进行细分为以下几部分： 训练集和测试集之间的差异。这种差异会经常存在，而且不一定是坏事。测试集和“第二天”数据间的差异。这种差异也会一直存在，而这个“第二天”数据上的表现是我们应该努力优化的，例如通过正则化。这两者之间差异如果过大，可能是因为用到了一些时间敏感的特征，导致模型效果变化明显。“第二天”数据和线上数据间的差异。如果同样一条样本，在训练时给出的结果和线上服务时给出的结果不一致，那么这意味着工程实现中出现了bug。 机器学习第三阶段：减慢增速、精细优化和复杂模型ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models一般会有一些明确的信号来标识第二阶段的尾声。首先，每月的提升会逐步降低。你开始在不同指标之间做权衡，有的上升有的下降。嗯，游戏变得有趣了。既然收益不容易获得了，机器学习就得变得更复杂了。 在前两个阶段，大部分团队都可以过得很开心，但到了这个阶段，每个团队都需要找到适合自己的路。 Rule #38: Don’t waste time on new features if unaligned objectives have become the issue. 规则38：如果objective没有达成一致，不要在新特征上浪费时间。当系统整体达到一个稳定期，大家会开始关注机器学习系统优化目标以外的一些问题。这个时候，目标就不如之前那么清晰，那么如果目标没有确定下来的话，先不要在特征上浪费时间。 Rule #39: Launch decisions are a proxy for long­term product goals. 规则39：上线决策是长期产品目标的代理。这句话读起来有点别扭，作者举了几个例子来说明，我觉得核心就是在讲一件事情：系统、产品甚至公司的长远发展需要通过多个指标来综合衡量，而新模型是否上线要综合考虑这些指标。所谓代理，指的就是优化这些综合指标就是在优化产品、公司的长远目标。 决策只有在所有指标都在变好的情况下才会变得简单。但常常事情没那么简单，尤其是当不同指标之间无法换算的时候，例如A系统有一百万日活和四百万日收入，B系统有两百万日活和两百万日收入，你会从A切换到B吗？或者反过来？答案是或许都不会，因为你不知道某个指标的提升是否会cover另外一个指标的下降。 关键是，没有任何一个指标能回答：“五年后我的产品在哪里”？ 而每个个体，尤其是工程师们，显然更喜欢能够直接优化的目标，而这也是机器学习系统常见的场景 。现在也有一些多目标学习系统在试图解决这种问题。但仍然有很多目标无法建模为机器学习问题，比如用户为什么会来访问你的网站等等。作者说这是个AI-complete问题，也常被称为强AI问题，简单来说就是不能用某个单一算法解决的问题。 Rule #40: Keep ensembles simple. 规则40：ensemble策略保持简单。什么叫简单的ensemble？作者认为，只接受其他模型的输出作为输入，不附带其他特征的ensemble，叫做简单的ensemble。换句话说，你的模型要么是单纯的ensemble模型，要么是普通的接收大量特征的基模型。 除了保持简单，ensemble模型最好还能具有一些良好的性质。例如，某个基模型的性能提升不能降低组合模型的性能。以及，基模型最好都是可解释的（例如是校准的)，这样基模型的变化对上层的组合模型来说也是可解释的。同时，一个基模型预测概率值的提升不会降低组合模型的预测概率值。 Rule #41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals. 规则41：当效果进入稳定期，寻找本质上新的信息源，而不是优化已有的信号。你加了一些用户的人口统计学特征，你加了一些文档的文字特征，等等，但是关键指标上的提升还不到1%。现在咋整？ 这时就应该考虑加一些根本上不同的特征，例如用户再过去一天、一周看过的文档历史，或者另外一个数据源的数据。总之，要加入完全不同的维度的特征。此外也可以尝试使用深度学习，但同时也要调整你对ROI的预期，并且要评估增加的复杂度换来的收益是否值得。 Rule #42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are. 规则42：多样性，个性化或者相关性与流行度的相关性关系可能要比你想的弱很多。多样性意味着内容或者来源的多样性；个性化意味着每个用户得到不一样的东西；相关性意味着一个query的返回结果相比其他query与这个query更相关。所以这三个指标的含义都是与普通不一样。 但问题在于普通的东西很难被打败。 如果你的衡量指标是点击、停留时长、观看数、分享数等等，你本质上是在衡量东西的流行度。有的团队有时会希望学到一个多样化的个性化模型。为此，会加入个性化特征和多样化特征，但是最后会发现这些特征并没有得到预期的权重。 这并不能说明多样性、个性化和相关性不重要。像前文指出，可以通过后续的处理来增加多样性或相关性。如果这时看到长期目标提升了，你就可以确定多样性/相关性是有用的。这时你就可以选择继续使用后续处理的方式，或者根据多样性和相关性直接修改要优化的objective。 Rule #43: Your friends tend to be the same across different products. Your interests tend not to be. 规则43：你在不同产品上的好友一般是一样的，但你的兴趣通常会不一样。谷歌经常在不同产品上使用同样的好友关系预测模型，并且取得了很好的效果，这证明不同的产品上好友关系是可以迁移的，毕竟他们是固定的同一批人。但他们尝试将一个产品上的个性化特征使用到另外一个产品上时却常常得不到好结果。可行的做法是使用一个数据源上的原始数据来预测另外数据源上的行为，而不是使用加工后的特征。此外，用户在另一个数据源上的行为历史也会有用。 总结从上面洋洋洒洒43条经验之谈中不难看出，大神作者认为，对于大多数机器学习应用场景来说，我们需要解决的问题大多数都是工程问题，解决这些工程问题需要的并不是复杂的理论，更多是对细节、架构、过程的仔细推敲和精致追求。而这些是我们非大神的普通人可以做到的，如果说大神做的是95分以上的系统，那么我们只要对工程架构、过程和细节做好足够的优化，我们也可以做出至少80分的系统。 参考https://blog.csdn.net/hellozhxy/article/details/80431959","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"Notebook和Markdown互相转换","slug":"021python/Notebook和Markdown互相转换","date":"2019-01-07T00:00:00.000Z","updated":"2019-02-20T07:42:58.667Z","comments":true,"path":"category/021python/Notebook和Markdown互相转换.html","link":"","permalink":"http://yoursite.com/category/021python/Notebook和Markdown互相转换.html","excerpt":"","text":"Jupyter Notebook导出为MarkDown命令行方式实例： jupyter nbconvert --to markdown 评分卡开发.ipynb --output image/评分卡开发 官方参考 Jupyter Notebook参考MarkDown文件需要在Jupyter Notebook中修改其他软件编辑的MarkDown的文件，发现可以使用notedown插件解决。 基本步骤pip3 install https://github.com/mli/notedown/tarball/master 安装完成之后 jupyter notebook --NotebookApp.contents_manager_class=&apos;notedown.NotedownContentsManager&apos; 这时打开相应的.md文件就可以了。 不想每次执行都加参数？操作如下： 执行 jupyter notebook --generate-config 会生成~/.jupyter/jupyter_notebook_config.py文件，在文件中加入配置c.NotebookApp.contents_manager_class = &#39;notedown.NotedownContentsManager&#39;再启动jupyter notebook，和上面的效果一样。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"评分卡模型","slug":"001机器学习/评分卡开发","date":"2019-01-05T00:00:00.000Z","updated":"2019-02-19T11:20:38.973Z","comments":true,"path":"category/001机器学习/评分卡开发.html","link":"","permalink":"http://yoursite.com/category/001机器学习/评分卡开发.html","excerpt":"","text":"1234567891011121314151617import pandas as pdimport datetimeimport collectionsimport numpy as npimport numbersimport randomimport sysimport picklefrom itertools import combinationsfrom sklearn.linear_model import LinearRegressionfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitimport statsmodels.api as smfrom importlib import reloadfrom matplotlib import pyplot as pltfrom sklearn.linear_model import LogisticRegressionCV# -*- coding: utf-8 -*- 初始工作读取数据文件 12345folderOfData = './datasets/score/'data1 = pd.read_csv(folderOfData+'PPD_LogInfo_3_1_Training_Set.csv', header = 0)data2 = pd.read_csv(folderOfData+'PPD_Training_Master_GBK_3_1_Training_Set.csv', header = 0,encoding = 'gbk')data3 = pd.read_csv(folderOfData+'PPD_Userupdate_Info_3_1_Training_Set.csv', header = 0) 1data1.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Idx Listinginfo1 LogInfo1 LogInfo2 LogInfo3 0 10001 2014-03-05 107 6 2014-02-20 1 10001 2014-03-05 107 6 2014-02-23 2 10001 2014-03-05 107 6 2014-02-24 3 10001 2014-03-05 107 6 2014-02-25 4 10001 2014-03-05 107 6 2014-02-27 信贷客户的登彔信息，字段描叙如下： 字段 含义 Idx 用户的唯一标识 LogInfo3 登录日期 LogInfo2 登录事件代码 1data2.head().T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 Idx 10001 10002 10003 10006 10007 UserInfo_1 1 1 1 4 5 UserInfo_2 深圳 温州 宜昌 南平 辽阳 UserInfo_3 4 4 3 1 1 UserInfo_4 深圳 温州 宜昌 南平 辽阳 WeblogInfo_1 NaN NaN NaN NaN NaN WeblogInfo_2 1 0 0 NaN 0 WeblogInfo_3 NaN NaN NaN NaN NaN WeblogInfo_4 1 1 2 NaN 1 WeblogInfo_5 1 1 2 NaN 1 WeblogInfo_6 1 1 2 NaN 1 WeblogInfo_7 14 14 9 2 3 WeblogInfo_8 0 0 3 0 0 WeblogInfo_9 0 0 0 0 0 WeblogInfo_10 0 0 0 0 0 WeblogInfo_11 0 0 0 0 0 WeblogInfo_12 0 0 0 0 0 WeblogInfo_13 0 0 0 0 0 WeblogInfo_14 6 0 0 0 0 WeblogInfo_15 6 0 0 0 0 WeblogInfo_16 0 7 3 0 0 WeblogInfo_17 6 7 4 2 3 WeblogInfo_18 2 0 2 0 0 UserInfo_5 2 2 2 2 2 UserInfo_6 2 2 2 2 2 UserInfo_7 广东 浙江 湖北 福建 辽宁 UserInfo_8 深圳 温州 宜昌 南平 辽阳 UserInfo_9 中国移动 中国移动 中国电信 中国移动 中国移动 UserInfo_10 0 1 0 0 0 UserInfo_11 NaN 0 0 0 NaN … … … … … … ThirdParty_Info_Period7_7 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_8 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_9 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_10 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_11 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_12 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_13 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_14 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_15 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_16 -1 -1 -1 -1 -1 ThirdParty_Info_Period7_17 -1 -1 -1 -1 -1 SocialNetwork_1 0 0 0 0 0 SocialNetwork_2 0 0 0 0 0 SocialNetwork_3 -1 -1 -1 -1 -1 SocialNetwork_4 -1 -1 -1 -1 -1 SocialNetwork_5 -1 -1 -1 -1 -1 SocialNetwork_6 -1 -1 -1 -1 -1 SocialNetwork_7 -1 -1 -1 -1 -1 SocialNetwork_8 126 33 -1 -1 -1 SocialNetwork_9 234 110 -1 -1 -1 SocialNetwork_10 222 1 -1 -1 -1 SocialNetwork_11 -1 -1 -1 -1 -1 SocialNetwork_12 0 0 -1 -1 -1 SocialNetwork_13 0 0 1 0 0 SocialNetwork_14 0 0 0 0 0 SocialNetwork_15 0 0 0 0 0 SocialNetwork_16 0 0 0 0 0 SocialNetwork_17 1 2 0 0 0 target 0 0 0 0 0 ListingInfo 2014/3/5 2014/2/26 2014/2/28 2014/2/25 2014/2/27 228 rows × 5 columns 信贷客户在平台上的申报信息和部分三方数据信息，以及需要预测的目标变量。 字段 含义 Idx 用户的唯一标识 Target 目标变量，以1、0表示违约与非违约 ThirdParty_Info_Period* 第三方数据信息，除-1外，其他都是非负整数。-1可能是特殊值 ListingInfo 贷款发放日期 1data3.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Idx ListingInfo1 UserupdateInfo1 UserupdateInfo2 0 10001 2014/03/05 _EducationId 2014/02/20 1 10001 2014/03/05 _HasBuyCar 2014/02/20 2 10001 2014/03/05 _LastUpdateDate 2014/02/20 3 10001 2014/03/05 _MarriageStatusId 2014/02/20 4 10001 2014/03/05 _MobilePhone 2014/02/20 部分客户的信息修改行为|字段|含义||::|::||Idx|用户的唯一标识||UserupdateInfo1|更改信息的所属字段||UserupdateInfo2|更改日期| 特征构造-数据中衍生特征归属地是否一致，放在city_match字段 123456data2['city_match'] = data2.apply(lambda x: int(x.UserInfo_2 == x.UserInfo_4 == x.UserInfo_8 == x.UserInfo_20),axis = 1)del data2['UserInfo_2']del data2['UserInfo_4']del data2['UserInfo_8']del data2['UserInfo_20'] 1#data2['city_match'] 提取申请日期，计算日期差，放在ListingGap字段中，并查看日期差的分布。 12345678910## 登录日期data1['logInfo'] = data1['LogInfo3'].map(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d')) ## 贷款发放日期data1['Listinginfo'] = data1['Listinginfo1'].map(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d'))data1['ListingGap'] = data1[['logInfo','Listinginfo']].apply(lambda x: (x[1]-x[0]).days,axis = 1)plt.hist(data1['ListingGap'],bins=200)plt.title('Days between login date and listing date')ListingGap2 = data1['ListingGap'].map(lambda x: min(x,365))plt.hist(ListingGap2,bins=200) (array([9.0180e+04, 9.2185e+04, 8.4625e+04, 6.5690e+04, 4.6703e+04, 2.0999e+04, 3.1156e+04, 2.3292e+04, 1.1277e+04, 6.6520e+03, 5.9060e+03, 2.8950e+03, 5.5110e+03, 4.0640e+03, 3.6030e+03, 2.9530e+03, 2.8020e+03, 1.2120e+03, 2.5110e+03, 2.2570e+03, 2.0350e+03, 1.6930e+03, 9.8200e+02, 1.7250e+03, 1.5240e+03, 1.4620e+03, 1.5680e+03, 1.4340e+03, 6.2800e+02, 1.0770e+03, 1.2250e+03, 1.2550e+03, 1.0920e+03, 1.0690e+03, 6.5100e+02, 8.8700e+02, 8.1400e+02, 7.6600e+02, 8.7400e+02, 3.3800e+02, 6.6000e+02, 9.5600e+02, 6.2700e+02, 5.6000e+02, 5.2800e+02, 2.5100e+02, 5.5800e+02, 5.6200e+02, 6.1400e+02, 6.8600e+02, 5.6300e+02, 3.0500e+02, 5.7700e+02, 6.6400e+02, 5.2200e+02, 4.5700e+02, 5.6000e+02, 2.4000e+02, 3.9600e+02, 5.3800e+02, 4.4900e+02, 4.6300e+02, 1.8200e+02, 4.3400e+02, 3.0500e+02, 4.5400e+02, 3.7100e+02, 4.3400e+02, 2.3300e+02, 3.3700e+02, 3.3100e+02, 3.2700e+02, 4.2000e+02, 4.2500e+02, 2.0900e+02, 2.5500e+02, 3.9000e+02, 3.2700e+02, 3.8700e+02, 1.7900e+02, 2.5100e+02, 3.3100e+02, 2.9000e+02, 2.7800e+02, 2.3100e+02, 1.3200e+02, 2.5300e+02, 3.8300e+02, 2.9800e+02, 3.1200e+02, 3.4400e+02, 9.4000e+01, 2.2200e+02, 2.7400e+02, 2.0300e+02, 2.1300e+02, 2.9000e+02, 9.6000e+01, 2.0600e+02, 1.7700e+02, 1.3900e+02, 2.0100e+02, 1.1800e+02, 2.4700e+02, 2.6200e+02, 1.9200e+02, 1.5900e+02, 2.0900e+02, 9.2000e+01, 2.3300e+02, 1.6800e+02, 1.7300e+02, 1.5900e+02, 2.6700e+02, 9.9000e+01, 2.1000e+02, 1.9400e+02, 1.2300e+02, 1.8800e+02, 9.6000e+01, 2.1400e+02, 1.9200e+02, 1.7300e+02, 1.4000e+02, 1.5300e+02, 5.6000e+01, 1.2200e+02, 1.8000e+02, 1.2700e+02, 1.4800e+02, 1.0600e+02, 6.7000e+01, 1.5900e+02, 6.4000e+01, 1.5200e+02, 1.1900e+02, 1.6000e+02, 9.6000e+01, 1.2400e+02, 1.1200e+02, 1.6300e+02, 1.7300e+02, 5.2000e+01, 6.1000e+01, 1.3400e+02, 9.9000e+01, 9.7000e+01, 1.0100e+02, 3.3000e+01, 1.0800e+02, 1.2100e+02, 8.1000e+01, 7.8000e+01, 9.2000e+01, 7.9000e+01, 1.1400e+02, 1.0100e+02, 9.7000e+01, 9.1000e+01, 4.9000e+01, 1.1800e+02, 1.0700e+02, 1.1800e+02, 1.2900e+02, 1.2700e+02, 7.3000e+01, 1.4600e+02, 9.9000e+01, 1.3000e+02, 9.2000e+01, 8.9000e+01, 4.6000e+01, 1.1000e+02, 9.2000e+01, 9.1000e+01, 1.0800e+02, 1.2600e+02, 8.5000e+01, 9.7000e+01, 1.0700e+02, 6.5000e+01, 8.1000e+01, 8.0000e+01, 1.2200e+02, 1.2700e+02, 9.5000e+01, 1.7100e+02, 7.2000e+01, 2.6000e+01, 8.3000e+01, 1.0400e+02, 1.1200e+02, 6.7000e+01, 1.1900e+02, 7.3000e+01, 6.4000e+01, 7.6000e+01, 1.0000e+02, 1.0600e+02, 1.6415e+04]), array([ 0. , 1.825, 3.65 , 5.475, 7.3 , 9.125, 10.95 , 12.775, 14.6 , 16.425, 18.25 , 20.075, 21.9 , 23.725, 25.55 , 27.375, 29.2 , 31.025, 32.85 , 34.675, 36.5 , 38.325, 40.15 , 41.975, 43.8 , 45.625, 47.45 , 49.275, 51.1 , 52.925, 54.75 , 56.575, 58.4 , 60.225, 62.05 , 63.875, 65.7 , 67.525, 69.35 , 71.175, 73. , 74.825, 76.65 , 78.475, 80.3 , 82.125, 83.95 , 85.775, 87.6 , 89.425, 91.25 , 93.075, 94.9 , 96.725, 98.55 , 100.375, 102.2 , 104.025, 105.85 , 107.675, 109.5 , 111.325, 113.15 , 114.975, 116.8 , 118.625, 120.45 , 122.275, 124.1 , 125.925, 127.75 , 129.575, 131.4 , 133.225, 135.05 , 136.875, 138.7 , 140.525, 142.35 , 144.175, 146. , 147.825, 149.65 , 151.475, 153.3 , 155.125, 156.95 , 158.775, 160.6 , 162.425, 164.25 , 166.075, 167.9 , 169.725, 171.55 , 173.375, 175.2 , 177.025, 178.85 , 180.675, 182.5 , 184.325, 186.15 , 187.975, 189.8 , 191.625, 193.45 , 195.275, 197.1 , 198.925, 200.75 , 202.575, 204.4 , 206.225, 208.05 , 209.875, 211.7 , 213.525, 215.35 , 217.175, 219. , 220.825, 222.65 , 224.475, 226.3 , 228.125, 229.95 , 231.775, 233.6 , 235.425, 237.25 , 239.075, 240.9 , 242.725, 244.55 , 246.375, 248.2 , 250.025, 251.85 , 253.675, 255.5 , 257.325, 259.15 , 260.975, 262.8 , 264.625, 266.45 , 268.275, 270.1 , 271.925, 273.75 , 275.575, 277.4 , 279.225, 281.05 , 282.875, 284.7 , 286.525, 288.35 , 290.175, 292. , 293.825, 295.65 , 297.475, 299.3 , 301.125, 302.95 , 304.775, 306.6 , 308.425, 310.25 , 312.075, 313.9 , 315.725, 317.55 , 319.375, 321.2 , 323.025, 324.85 , 326.675, 328.5 , 330.325, 332.15 , 333.975, 335.8 , 337.625, 339.45 , 341.275, 343.1 , 344.925, 346.75 , 348.575, 350.4 , 352.225, 354.05 , 355.875, 357.7 , 359.525, 361.35 , 363.175, 365. ]), &lt;a list of 200 Patch objects&gt;) 计算登录日期与放款日期时间的间隔天数，从上面的图中，可以看到绝大部分的天数在180天以内，使用180天作为最大的时间窗口计算新特征。 所有可以使用的时间窗口可以有7 days, 30 days, 60 days, 90 days, 120 days, 150 days and 180 days.在每个时间窗口内，计算总的登录次数，不同的登录方式，以及每种登录方式的平均次数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def TimeWindowSelection(df, daysCol, time_windows): ''' :param df: the dataset containg variabel of days :param daysCol: the column of days :param time_windows: the list of time window :return: ''' freq_tw = &#123;&#125; for tw in time_windows: freq = sum(df[daysCol].apply(lambda x: int(x&lt;=tw))) freq_tw[tw] = freq return freq_twdef DeivdedByZero(nominator, denominator): ''' 当分母为0时，返回0；否则返回正常值 ''' if denominator == 0: return 0 else: return nominator*1.0/denominator timeWindows = TimeWindowSelection(data1, 'ListingGap', range(30,361,30))time_window = [7, 30, 60, 90, 120, 150, 180]var_list = ['LogInfo1','LogInfo2']data1GroupbyIdx = pd.DataFrame(&#123;'Idx':data1['Idx'].drop_duplicates()&#125;)for tw in time_window: data1['TruncatedLogInfo'] = data1['Listinginfo'].map(lambda x: x + datetime.timedelta(-tw)) temp = data1.loc[data1['logInfo'] &gt;= data1['TruncatedLogInfo']] for var in var_list: #count the frequences of LogInfo1 and LogInfo2 count_stats = temp.groupby(['Idx'])[var].count().to_dict() data1GroupbyIdx[str(var)+'_'+str(tw)+'_count'] = data1GroupbyIdx['Idx'].map(lambda x: count_stats.get(x,0)) # count the distinct value of LogInfo1 and LogInfo2 Idx_UserupdateInfo1 = temp[['Idx', var]].drop_duplicates() uniq_stats = Idx_UserupdateInfo1.groupby(['Idx'])[var].count().to_dict() data1GroupbyIdx[str(var) + '_' + str(tw) + '_unique'] = data1GroupbyIdx['Idx'].map(lambda x: uniq_stats.get(x,0)) # calculate the average count of each value in LogInfo1 and LogInfo2 data1GroupbyIdx[str(var) + '_' + str(tw) + '_avg_count'] = data1GroupbyIdx[[str(var)+'_'+str(tw)+'_count',str(var) + '_' + str(tw) + '_unique']].\\ apply(lambda x: DeivdedByZero(x[0],x[1]), axis=1)data3['ListingInfo'] = data3['ListingInfo1'].map(lambda x: datetime.datetime.strptime(x,'%Y/%m/%d'))data3['UserupdateInfo'] = data3['UserupdateInfo2'].map(lambda x: datetime.datetime.strptime(x,'%Y/%m/%d'))data3['ListingGap'] = data3[['UserupdateInfo','ListingInfo']].apply(lambda x: (x[1]-x[0]).days,axis = 1)collections.Counter(data3['ListingGap'])hist_ListingGap = np.histogram(data3['ListingGap'])hist_ListingGap = pd.DataFrame(&#123;'Freq':hist_ListingGap[0],'gap':hist_ListingGap[1][1:]&#125;)hist_ListingGap['CumFreq'] = hist_ListingGap['Freq'].cumsum()hist_ListingGap['CumPercent'] = hist_ListingGap['CumFreq'].map(lambda x: x*1.0/hist_ListingGap.iloc[-1]['CumFreq']) 对某些统一的字段进行统一 123456789def ChangeContent(x): y = x.upper() if y == '_MOBILEPHONE': y = '_PHONE' return ydata3['UserupdateInfo1'] = data3['UserupdateInfo1'].map(ChangeContent)data3GroupbyIdx = pd.DataFrame(&#123;'Idx':data3['Idx'].drop_duplicates()&#125;) 数据一致性处理数据含义一致性，一般采取手工解决的方式。对 QQ和qQ, Idnumber和idNumber,MOBILEPHONE和PHONE 进行统一。在时间切片内，计算 (1) 更新的频率 (2) 每种更新对象的种类个数 (3) 对重要信息如IDNUMBER,HASBUYCAR, MARRIAGESTATUSID, PHONE的更新 12345678910111213141516171819202122232425262728time_window = [7, 30, 60, 90, 120, 150, 180]for tw in time_window: data3['TruncatedLogInfo'] = data3['ListingInfo'].map(lambda x: x + datetime.timedelta(-tw)) temp = data3.loc[data3['UserupdateInfo'] &gt;= data3['TruncatedLogInfo']] #frequency of updating freq_stats = temp.groupby(['Idx'])['UserupdateInfo1'].count().to_dict() data3GroupbyIdx['UserupdateInfo_'+str(tw)+'_freq'] = data3GroupbyIdx['Idx'].map(lambda x: freq_stats.get(x,0)) # number of updated types Idx_UserupdateInfo1 = temp[['Idx','UserupdateInfo1']].drop_duplicates() uniq_stats = Idx_UserupdateInfo1.groupby(['Idx'])['UserupdateInfo1'].count().to_dict() data3GroupbyIdx['UserupdateInfo_' + str(tw) + '_unique'] = data3GroupbyIdx['Idx'].map(lambda x: uniq_stats.get(x, x)) #average count of each type data3GroupbyIdx['UserupdateInfo_' + str(tw) + '_avg_count'] = data3GroupbyIdx[['UserupdateInfo_'+str(tw)+'_freq', 'UserupdateInfo_' + str(tw) + '_unique']]. \\ apply(lambda x: x[0] * 1.0 / x[1], axis=1) #whether the applicant changed items like IDNUMBER,HASBUYCAR, MARRIAGESTATUSID, PHONE Idx_UserupdateInfo1['UserupdateInfo1'] = Idx_UserupdateInfo1['UserupdateInfo1'].map(lambda x: [x]) Idx_UserupdateInfo1_V2 = Idx_UserupdateInfo1.groupby(['Idx'])['UserupdateInfo1'].sum() for item in ['_IDNUMBER','_HASBUYCAR','_MARRIAGESTATUSID','_PHONE']: item_dict = Idx_UserupdateInfo1_V2.map(lambda x: int(item in x)).to_dict() data3GroupbyIdx['UserupdateInfo_' + str(tw) + str(item)] = data3GroupbyIdx['Idx'].map(lambda x: item_dict.get(x, x))# 进行表的链接allData = pd.concat([data2.set_index('Idx'), data3GroupbyIdx.set_index('Idx'), data1GroupbyIdx.set_index('Idx')],axis= 1)allData.to_csv(folderOfData+'allData_0.csv',encoding = 'gbk') 缺失值处理缺失占比太高，舍弃该字段或者该条记录，缺失占比不高，可以采取补缺或者作为特殊值。 1234allData = pd.read_csv(folderOfData+'allData_0.csv',header = 0,encoding = 'gbk')allFeatures = list(allData.columns)allFeatures [&apos;Idx&apos;, &apos;UserInfo_1&apos;, &apos;UserInfo_3&apos;, &apos;WeblogInfo_1&apos;, &apos;WeblogInfo_2&apos;, &apos;WeblogInfo_3&apos;, &apos;WeblogInfo_4&apos;, &apos;WeblogInfo_5&apos;, &apos;WeblogInfo_6&apos;, &apos;WeblogInfo_7&apos;, &apos;WeblogInfo_8&apos;, &apos;WeblogInfo_9&apos;, &apos;WeblogInfo_10&apos;, &apos;WeblogInfo_11&apos;, &apos;WeblogInfo_12&apos;, &apos;WeblogInfo_13&apos;, &apos;WeblogInfo_14&apos;, &apos;WeblogInfo_15&apos;, &apos;WeblogInfo_16&apos;, &apos;WeblogInfo_17&apos;, &apos;WeblogInfo_18&apos;, &apos;UserInfo_5&apos;, &apos;UserInfo_6&apos;, &apos;UserInfo_7&apos;, &apos;UserInfo_9&apos;, &apos;UserInfo_10&apos;, &apos;UserInfo_11&apos;, &apos;UserInfo_12&apos;, &apos;UserInfo_13&apos;, &apos;UserInfo_14&apos;, &apos;UserInfo_15&apos;, &apos;UserInfo_16&apos;, &apos;UserInfo_17&apos;, &apos;UserInfo_18&apos;, &apos;UserInfo_19&apos;, &apos;UserInfo_21&apos;, &apos;UserInfo_22&apos;, &apos;UserInfo_23&apos;, &apos;UserInfo_24&apos;, &apos;Education_Info1&apos;, &apos;Education_Info2&apos;, &apos;Education_Info3&apos;, &apos;Education_Info4&apos;, &apos;Education_Info5&apos;, &apos;Education_Info6&apos;, &apos;Education_Info7&apos;, &apos;Education_Info8&apos;, &apos;WeblogInfo_19&apos;, &apos;WeblogInfo_20&apos;, &apos;WeblogInfo_21&apos;, &apos;WeblogInfo_23&apos;, &apos;WeblogInfo_24&apos;, &apos;WeblogInfo_25&apos;, &apos;WeblogInfo_26&apos;, &apos;WeblogInfo_27&apos;, &apos;WeblogInfo_28&apos;, &apos;WeblogInfo_29&apos;, &apos;WeblogInfo_30&apos;, &apos;WeblogInfo_31&apos;, &apos;WeblogInfo_32&apos;, &apos;WeblogInfo_33&apos;, &apos;WeblogInfo_34&apos;, &apos;WeblogInfo_35&apos;, &apos;WeblogInfo_36&apos;, &apos;WeblogInfo_37&apos;, &apos;WeblogInfo_38&apos;, &apos;WeblogInfo_39&apos;, &apos;WeblogInfo_40&apos;, &apos;WeblogInfo_41&apos;, &apos;WeblogInfo_42&apos;, &apos;WeblogInfo_43&apos;, &apos;WeblogInfo_44&apos;, &apos;WeblogInfo_45&apos;, &apos;WeblogInfo_46&apos;, &apos;WeblogInfo_47&apos;, &apos;WeblogInfo_48&apos;, &apos;WeblogInfo_49&apos;, &apos;WeblogInfo_50&apos;, &apos;WeblogInfo_51&apos;, &apos;WeblogInfo_52&apos;, &apos;WeblogInfo_53&apos;, &apos;WeblogInfo_54&apos;, &apos;WeblogInfo_55&apos;, &apos;WeblogInfo_56&apos;, &apos;WeblogInfo_57&apos;, &apos;WeblogInfo_58&apos;, &apos;ThirdParty_Info_Period1_1&apos;, &apos;ThirdParty_Info_Period1_2&apos;, &apos;ThirdParty_Info_Period1_3&apos;, &apos;ThirdParty_Info_Period1_4&apos;, &apos;ThirdParty_Info_Period1_5&apos;, &apos;ThirdParty_Info_Period1_6&apos;, &apos;ThirdParty_Info_Period1_7&apos;, &apos;ThirdParty_Info_Period1_8&apos;, &apos;ThirdParty_Info_Period1_9&apos;, &apos;ThirdParty_Info_Period1_10&apos;, &apos;ThirdParty_Info_Period1_11&apos;, &apos;ThirdParty_Info_Period1_12&apos;, &apos;ThirdParty_Info_Period1_13&apos;, &apos;ThirdParty_Info_Period1_14&apos;, &apos;ThirdParty_Info_Period1_15&apos;, &apos;ThirdParty_Info_Period1_16&apos;, &apos;ThirdParty_Info_Period1_17&apos;, &apos;ThirdParty_Info_Period2_1&apos;, &apos;ThirdParty_Info_Period2_2&apos;, &apos;ThirdParty_Info_Period2_3&apos;, &apos;ThirdParty_Info_Period2_4&apos;, &apos;ThirdParty_Info_Period2_5&apos;, &apos;ThirdParty_Info_Period2_6&apos;, &apos;ThirdParty_Info_Period2_7&apos;, &apos;ThirdParty_Info_Period2_8&apos;, &apos;ThirdParty_Info_Period2_9&apos;, &apos;ThirdParty_Info_Period2_10&apos;, &apos;ThirdParty_Info_Period2_11&apos;, &apos;ThirdParty_Info_Period2_12&apos;, &apos;ThirdParty_Info_Period2_13&apos;, &apos;ThirdParty_Info_Period2_14&apos;, &apos;ThirdParty_Info_Period2_15&apos;, &apos;ThirdParty_Info_Period2_16&apos;, &apos;ThirdParty_Info_Period2_17&apos;, &apos;ThirdParty_Info_Period3_1&apos;, &apos;ThirdParty_Info_Period3_2&apos;, &apos;ThirdParty_Info_Period3_3&apos;, &apos;ThirdParty_Info_Period3_4&apos;, &apos;ThirdParty_Info_Period3_5&apos;, &apos;ThirdParty_Info_Period3_6&apos;, &apos;ThirdParty_Info_Period3_7&apos;, &apos;ThirdParty_Info_Period3_8&apos;, &apos;ThirdParty_Info_Period3_9&apos;, &apos;ThirdParty_Info_Period3_10&apos;, &apos;ThirdParty_Info_Period3_11&apos;, &apos;ThirdParty_Info_Period3_12&apos;, &apos;ThirdParty_Info_Period3_13&apos;, &apos;ThirdParty_Info_Period3_14&apos;, &apos;ThirdParty_Info_Period3_15&apos;, &apos;ThirdParty_Info_Period3_16&apos;, &apos;ThirdParty_Info_Period3_17&apos;, &apos;ThirdParty_Info_Period4_1&apos;, &apos;ThirdParty_Info_Period4_2&apos;, &apos;ThirdParty_Info_Period4_3&apos;, &apos;ThirdParty_Info_Period4_4&apos;, &apos;ThirdParty_Info_Period4_5&apos;, &apos;ThirdParty_Info_Period4_6&apos;, &apos;ThirdParty_Info_Period4_7&apos;, &apos;ThirdParty_Info_Period4_8&apos;, &apos;ThirdParty_Info_Period4_9&apos;, &apos;ThirdParty_Info_Period4_10&apos;, &apos;ThirdParty_Info_Period4_11&apos;, &apos;ThirdParty_Info_Period4_12&apos;, &apos;ThirdParty_Info_Period4_13&apos;, &apos;ThirdParty_Info_Period4_14&apos;, &apos;ThirdParty_Info_Period4_15&apos;, &apos;ThirdParty_Info_Period4_16&apos;, &apos;ThirdParty_Info_Period4_17&apos;, &apos;ThirdParty_Info_Period5_1&apos;, &apos;ThirdParty_Info_Period5_2&apos;, &apos;ThirdParty_Info_Period5_3&apos;, &apos;ThirdParty_Info_Period5_4&apos;, &apos;ThirdParty_Info_Period5_5&apos;, &apos;ThirdParty_Info_Period5_6&apos;, &apos;ThirdParty_Info_Period5_7&apos;, &apos;ThirdParty_Info_Period5_8&apos;, &apos;ThirdParty_Info_Period5_9&apos;, &apos;ThirdParty_Info_Period5_10&apos;, &apos;ThirdParty_Info_Period5_11&apos;, &apos;ThirdParty_Info_Period5_12&apos;, &apos;ThirdParty_Info_Period5_13&apos;, &apos;ThirdParty_Info_Period5_14&apos;, &apos;ThirdParty_Info_Period5_15&apos;, &apos;ThirdParty_Info_Period5_16&apos;, &apos;ThirdParty_Info_Period5_17&apos;, &apos;ThirdParty_Info_Period6_1&apos;, &apos;ThirdParty_Info_Period6_2&apos;, &apos;ThirdParty_Info_Period6_3&apos;, &apos;ThirdParty_Info_Period6_4&apos;, &apos;ThirdParty_Info_Period6_5&apos;, &apos;ThirdParty_Info_Period6_6&apos;, &apos;ThirdParty_Info_Period6_7&apos;, &apos;ThirdParty_Info_Period6_8&apos;, &apos;ThirdParty_Info_Period6_9&apos;, &apos;ThirdParty_Info_Period6_10&apos;, &apos;ThirdParty_Info_Period6_11&apos;, &apos;ThirdParty_Info_Period6_12&apos;, &apos;ThirdParty_Info_Period6_13&apos;, &apos;ThirdParty_Info_Period6_14&apos;, &apos;ThirdParty_Info_Period6_15&apos;, &apos;ThirdParty_Info_Period6_16&apos;, &apos;ThirdParty_Info_Period6_17&apos;, &apos;ThirdParty_Info_Period7_1&apos;, &apos;ThirdParty_Info_Period7_2&apos;, &apos;ThirdParty_Info_Period7_3&apos;, &apos;ThirdParty_Info_Period7_4&apos;, &apos;ThirdParty_Info_Period7_5&apos;, &apos;ThirdParty_Info_Period7_6&apos;, &apos;ThirdParty_Info_Period7_7&apos;, &apos;ThirdParty_Info_Period7_8&apos;, &apos;ThirdParty_Info_Period7_9&apos;, &apos;ThirdParty_Info_Period7_10&apos;, &apos;ThirdParty_Info_Period7_11&apos;, &apos;ThirdParty_Info_Period7_12&apos;, &apos;ThirdParty_Info_Period7_13&apos;, &apos;ThirdParty_Info_Period7_14&apos;, &apos;ThirdParty_Info_Period7_15&apos;, &apos;ThirdParty_Info_Period7_16&apos;, &apos;ThirdParty_Info_Period7_17&apos;, &apos;SocialNetwork_1&apos;, &apos;SocialNetwork_2&apos;, &apos;SocialNetwork_3&apos;, &apos;SocialNetwork_4&apos;, &apos;SocialNetwork_5&apos;, &apos;SocialNetwork_6&apos;, &apos;SocialNetwork_7&apos;, &apos;SocialNetwork_8&apos;, &apos;SocialNetwork_9&apos;, &apos;SocialNetwork_10&apos;, &apos;SocialNetwork_11&apos;, &apos;SocialNetwork_12&apos;, &apos;SocialNetwork_13&apos;, &apos;SocialNetwork_14&apos;, &apos;SocialNetwork_15&apos;, &apos;SocialNetwork_16&apos;, &apos;SocialNetwork_17&apos;, &apos;target&apos;, &apos;ListingInfo&apos;, &apos;city_match&apos;, &apos;UserupdateInfo_7_freq&apos;, &apos;UserupdateInfo_7_unique&apos;, &apos;UserupdateInfo_7_avg_count&apos;, &apos;UserupdateInfo_7_IDNUMBER&apos;, &apos;UserupdateInfo_7_HASBUYCAR&apos;, &apos;UserupdateInfo_7_MARRIAGESTATUSID&apos;, &apos;UserupdateInfo_7_PHONE&apos;, &apos;UserupdateInfo_30_freq&apos;, &apos;UserupdateInfo_30_unique&apos;, &apos;UserupdateInfo_30_avg_count&apos;, &apos;UserupdateInfo_30_IDNUMBER&apos;, &apos;UserupdateInfo_30_HASBUYCAR&apos;, &apos;UserupdateInfo_30_MARRIAGESTATUSID&apos;, &apos;UserupdateInfo_30_PHONE&apos;, &apos;UserupdateInfo_60_freq&apos;, &apos;UserupdateInfo_60_unique&apos;, &apos;UserupdateInfo_60_avg_count&apos;, &apos;UserupdateInfo_60_IDNUMBER&apos;, &apos;UserupdateInfo_60_HASBUYCAR&apos;, &apos;UserupdateInfo_60_MARRIAGESTATUSID&apos;, &apos;UserupdateInfo_60_PHONE&apos;, &apos;UserupdateInfo_90_freq&apos;, &apos;UserupdateInfo_90_unique&apos;, &apos;UserupdateInfo_90_avg_count&apos;, &apos;UserupdateInfo_90_IDNUMBER&apos;, &apos;UserupdateInfo_90_HASBUYCAR&apos;, &apos;UserupdateInfo_90_MARRIAGESTATUSID&apos;, &apos;UserupdateInfo_90_PHONE&apos;, &apos;UserupdateInfo_120_freq&apos;, &apos;UserupdateInfo_120_unique&apos;, &apos;UserupdateInfo_120_avg_count&apos;, &apos;UserupdateInfo_120_IDNUMBER&apos;, &apos;UserupdateInfo_120_HASBUYCAR&apos;, &apos;UserupdateInfo_120_MARRIAGESTATUSID&apos;, &apos;UserupdateInfo_120_PHONE&apos;, &apos;UserupdateInfo_150_freq&apos;, &apos;UserupdateInfo_150_unique&apos;, &apos;UserupdateInfo_150_avg_count&apos;, &apos;UserupdateInfo_150_IDNUMBER&apos;, &apos;UserupdateInfo_150_HASBUYCAR&apos;, &apos;UserupdateInfo_150_MARRIAGESTATUSID&apos;, &apos;UserupdateInfo_150_PHONE&apos;, &apos;UserupdateInfo_180_freq&apos;, &apos;UserupdateInfo_180_unique&apos;, &apos;UserupdateInfo_180_avg_count&apos;, &apos;UserupdateInfo_180_IDNUMBER&apos;, &apos;UserupdateInfo_180_HASBUYCAR&apos;, &apos;UserupdateInfo_180_MARRIAGESTATUSID&apos;, &apos;UserupdateInfo_180_PHONE&apos;, &apos;LogInfo1_7_count&apos;, &apos;LogInfo1_7_unique&apos;, &apos;LogInfo1_7_avg_count&apos;, &apos;LogInfo2_7_count&apos;, &apos;LogInfo2_7_unique&apos;, &apos;LogInfo2_7_avg_count&apos;, &apos;LogInfo1_30_count&apos;, &apos;LogInfo1_30_unique&apos;, &apos;LogInfo1_30_avg_count&apos;, &apos;LogInfo2_30_count&apos;, &apos;LogInfo2_30_unique&apos;, &apos;LogInfo2_30_avg_count&apos;, &apos;LogInfo1_60_count&apos;, &apos;LogInfo1_60_unique&apos;, &apos;LogInfo1_60_avg_count&apos;, &apos;LogInfo2_60_count&apos;, &apos;LogInfo2_60_unique&apos;, &apos;LogInfo2_60_avg_count&apos;, &apos;LogInfo1_90_count&apos;, &apos;LogInfo1_90_unique&apos;, &apos;LogInfo1_90_avg_count&apos;, &apos;LogInfo2_90_count&apos;, &apos;LogInfo2_90_unique&apos;, &apos;LogInfo2_90_avg_count&apos;, &apos;LogInfo1_120_count&apos;, &apos;LogInfo1_120_unique&apos;, &apos;LogInfo1_120_avg_count&apos;, &apos;LogInfo2_120_count&apos;, &apos;LogInfo2_120_unique&apos;, &apos;LogInfo2_120_avg_count&apos;, &apos;LogInfo1_150_count&apos;, &apos;LogInfo1_150_unique&apos;, &apos;LogInfo1_150_avg_count&apos;, &apos;LogInfo2_150_count&apos;, &apos;LogInfo2_150_unique&apos;, &apos;LogInfo2_150_avg_count&apos;, &apos;LogInfo1_180_count&apos;, &apos;LogInfo1_180_unique&apos;, &apos;LogInfo1_180_avg_count&apos;, &apos;LogInfo2_180_count&apos;, &apos;LogInfo2_180_unique&apos;, &apos;LogInfo2_180_avg_count&apos;] 12345678910111213141516171819202122allFeatures.remove('target')if 'Idx' in allFeatures: allFeatures.remove('Idx')allFeatures.remove('ListingInfo')#检查是否有常数型变量，并且检查是类别型还是数值型变量numerical_var = []for col in allFeatures: if len(set(allData[col])) == 1: print('delete &#123;&#125; from the dataset because it is a constant'.format(col)) del allData[col] allFeatures.remove(col) else: #uniq_vals = list(set(allData[col])) #if np.nan in uniq_vals: #uniq_vals.remove(np.nan) uniq_valid_vals = [i for i in allData[col] if i == i] uniq_valid_vals = list(set(uniq_valid_vals)) if len(uniq_valid_vals) &gt;= 10 and isinstance(uniq_valid_vals[0], numbers.Real): numerical_var.append(col)categorical_var = [i for i in allFeatures if i not in numerical_var] delete WeblogInfo_10 from the dataset because it is a constant 数据集中度处理在信用风控模型的开发中，数据集中度是常见的问题。 即在变量中，某单一数值的占比就占了全部样本值的 绝大多数。例如，在一批训练样本中，学历为本科的 样本占了全部样本的90%。具有极高的集中度的字段或者变量，需要按照风险程度迚行区分: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#检查变量的最多值的占比情况,以及每个变量中占比最大的值records_count = allData.shape[0]col_most_values,col_large_value = &#123;&#125;,&#123;&#125;for col in allFeatures: value_count = allData[col].groupby(allData[col]).count() col_most_values[col] = max(value_count)/records_count large_value = value_count[value_count== max(value_count)].index[0] col_large_value[col] = large_valuecol_most_values_df = pd.DataFrame.from_dict(col_most_values, orient = 'index')col_most_values_df.columns = ['max percent']col_most_values_df = col_most_values_df.sort_values(by = 'max percent', ascending = False)pcnt = list(col_most_values_df[:500]['max percent'])vars = list(col_most_values_df[:500].index)plt.bar(range(len(pcnt)), height = pcnt)plt.title('Largest Percentage of Single Value in Each Variable')#计算多数值产比超过90%的字段中，少数值的坏样本率是否会显著高于多数值large_percent_cols = list(col_most_values_df[col_most_values_df['max percent']&gt;=0.9].index)bad_rate_diff = &#123;&#125;for col in large_percent_cols: large_value = col_large_value[col] temp = allData[[col,'target']] temp[col] = temp.apply(lambda x: int(x[col]==large_value),axis=1) bad_rate = temp.groupby(col).mean() if bad_rate.iloc[0]['target'] == 0: bad_rate_diff[col] = 0 continue bad_rate_diff[col] = np.log(bad_rate.iloc[0]['target']/bad_rate.iloc[1]['target'])bad_rate_diff_sorted = sorted(bad_rate_diff.items(),key=lambda x: x[1], reverse=True)bad_rate_diff_sorted_values = [x[1] for x in bad_rate_diff_sorted]plt.bar(x = range(len(bad_rate_diff_sorted_values)), height = bad_rate_diff_sorted_values)#由于所有的少数值的坏样本率并没有显著高于多数值，意味着这些变量可以直接剔除for col in large_percent_cols: if col in numerical_var: numerical_var.remove(col) else: categorical_var.remove(col) del allData[col] '''对类别型变量，如果缺失超过80%, 就删除，否则当成特殊的状态'''def MissingCategorial(df,x): missing_vals = df[x].map(lambda x: int(x!=x)) return sum(missing_vals)*1.0/df.shape[0]missing_pcnt_threshould_1 = 0.8for col in categorical_var: missingRate = MissingCategorial(allData,col) print('&#123;0&#125; has missing rate as &#123;1&#125;'.format(col,missingRate)) if missingRate &gt; missing_pcnt_threshould_1: categorical_var.remove(col) del allData[col] if 0 &lt; missingRate &lt; missing_pcnt_threshould_1: # In this way we convert NaN to NAN, which is a string instead of np.nan allData[col] = allData[col].map(lambda x: str(x).upper())allData_bk = allData.copy()'''检查数值型变量'''def MissingContinuous(df,x): missing_vals = df[x].map(lambda x: int(np.isnan(x))) return sum(missing_vals) * 1.0 / df.shape[0]def MakeupRandom(x, sampledList): if x==x: return x else: randIndex = random.randint(0, len(sampledList)-1) return sampledList[randIndex] missing_pcnt_threshould_2 = 0.8deleted_var = []for col in numerical_var: missingRate = MissingContinuous(allData, col) print('&#123;0&#125; has missing rate as &#123;1&#125;'.format(col, missingRate)) if missingRate &gt; missing_pcnt_threshould_2: deleted_var.append(col) print('we delete variable &#123;&#125; because of its high missing rate'.format(col)) else: if missingRate &gt; 0: not_missing = allData.loc[allData[col] == allData[col]][col] #makeuped = allData[col].map(lambda x: MakeupRandom(x, list(not_missing))) missing_position = allData.loc[allData[col] != allData[col]][col].index not_missing_sample = random.sample(list(not_missing), len(missing_position)) allData.loc[missing_position,col] = not_missing_sample #del allData[col] #allData[col] = makeuped missingRate2 = MissingContinuous(allData, col) print('missing rate after making up is:&#123;&#125;'.format(str(missingRate2)))if deleted_var != []: for col in deleted_var: numerical_var.remove(col) del allData[col]allData.to_csv(folderOfData+'allData_1.csv', header=True,encoding='gbk', columns = allData.columns, index=False) /usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy UserInfo_1 has missing rate as 0.0002 UserInfo_3 has missing rate as 0.00023333333333333333 WeblogInfo_2 has missing rate as 0.055266666666666665 UserInfo_5 has missing rate as 0.0 UserInfo_6 has missing rate as 0.0 UserInfo_7 has missing rate as 0.0 UserInfo_9 has missing rate as 0.0 UserInfo_10 has missing rate as 0.0 UserInfo_11 has missing rate as 0.6303 UserInfo_12 has missing rate as 0.6303 UserInfo_13 has missing rate as 0.6303 UserInfo_14 has missing rate as 0.0 UserInfo_15 has missing rate as 0.0 UserInfo_16 has missing rate as 0.0 UserInfo_17 has missing rate as 0.0 UserInfo_19 has missing rate as 0.0 WeblogInfo_19 has missing rate as 0.09876666666666667 WeblogInfo_20 has missing rate as 0.2683333333333333 WeblogInfo_21 has missing rate as 0.10246666666666666 WeblogInfo_30 has missing rate as 0.008433333333333333 SocialNetwork_12 has missing rate as 0.0 SocialNetwork_13 has missing rate as 0.0 SocialNetwork_17 has missing rate as 0.0 WeblogInfo_1 has missing rate as 0.9676666666666667 we delete variable WeblogInfo_1 because of its high missing rate WeblogInfo_3 has missing rate as 0.9676666666666667 we delete variable WeblogInfo_3 because of its high missing rate WeblogInfo_4 has missing rate as 0.05503333333333333 missing rate after making up is:0.0 WeblogInfo_5 has missing rate as 0.05503333333333333 missing rate after making up is:0.0 WeblogInfo_6 has missing rate as 0.05503333333333333 missing rate after making up is:0.0 WeblogInfo_7 has missing rate as 0.0 WeblogInfo_8 has missing rate as 0.0 WeblogInfo_15 has missing rate as 0.0 WeblogInfo_16 has missing rate as 0.0 WeblogInfo_17 has missing rate as 0.0 WeblogInfo_18 has missing rate as 0.0 UserInfo_18 has missing rate as 0.0 WeblogInfo_24 has missing rate as 0.008433333333333333 missing rate after making up is:0.0 WeblogInfo_27 has missing rate as 0.008433333333333333 missing rate after making up is:0.0 WeblogInfo_33 has missing rate as 0.008433333333333333 missing rate after making up is:0.0 WeblogInfo_36 has missing rate as 0.008433333333333333 missing rate after making up is:0.0 ThirdParty_Info_Period1_1 has missing rate as 0.0 ThirdParty_Info_Period1_2 has missing rate as 0.0 ThirdParty_Info_Period1_3 has missing rate as 0.0 ThirdParty_Info_Period1_4 has missing rate as 0.0 ThirdParty_Info_Period1_5 has missing rate as 0.0 ThirdParty_Info_Period1_6 has missing rate as 0.0 ThirdParty_Info_Period1_7 has missing rate as 0.0 ThirdParty_Info_Period1_8 has missing rate as 0.0 ThirdParty_Info_Period1_9 has missing rate as 0.0 ThirdParty_Info_Period1_10 has missing rate as 0.0 ThirdParty_Info_Period1_11 has missing rate as 0.0 ThirdParty_Info_Period1_12 has missing rate as 0.0 ThirdParty_Info_Period1_13 has missing rate as 0.0 ThirdParty_Info_Period1_14 has missing rate as 0.0 ThirdParty_Info_Period1_15 has missing rate as 0.0 ThirdParty_Info_Period1_16 has missing rate as 0.0 ThirdParty_Info_Period1_17 has missing rate as 0.0 ThirdParty_Info_Period2_1 has missing rate as 0.0 ThirdParty_Info_Period2_2 has missing rate as 0.0 ThirdParty_Info_Period2_3 has missing rate as 0.0 ThirdParty_Info_Period2_4 has missing rate as 0.0 ThirdParty_Info_Period2_5 has missing rate as 0.0 ThirdParty_Info_Period2_6 has missing rate as 0.0 ThirdParty_Info_Period2_7 has missing rate as 0.0 ThirdParty_Info_Period2_8 has missing rate as 0.0 ThirdParty_Info_Period2_9 has missing rate as 0.0 ThirdParty_Info_Period2_10 has missing rate as 0.0 ThirdParty_Info_Period2_11 has missing rate as 0.0 ThirdParty_Info_Period2_12 has missing rate as 0.0 ThirdParty_Info_Period2_13 has missing rate as 0.0 ThirdParty_Info_Period2_14 has missing rate as 0.0 ThirdParty_Info_Period2_15 has missing rate as 0.0 ThirdParty_Info_Period2_16 has missing rate as 0.0 ThirdParty_Info_Period2_17 has missing rate as 0.0 ThirdParty_Info_Period3_1 has missing rate as 0.0 ThirdParty_Info_Period3_2 has missing rate as 0.0 ThirdParty_Info_Period3_3 has missing rate as 0.0 ThirdParty_Info_Period3_4 has missing rate as 0.0 ThirdParty_Info_Period3_5 has missing rate as 0.0 ThirdParty_Info_Period3_6 has missing rate as 0.0 ThirdParty_Info_Period3_7 has missing rate as 0.0 ThirdParty_Info_Period3_8 has missing rate as 0.0 ThirdParty_Info_Period3_9 has missing rate as 0.0 ThirdParty_Info_Period3_10 has missing rate as 0.0 ThirdParty_Info_Period3_11 has missing rate as 0.0 ThirdParty_Info_Period3_12 has missing rate as 0.0 ThirdParty_Info_Period3_13 has missing rate as 0.0 ThirdParty_Info_Period3_14 has missing rate as 0.0 ThirdParty_Info_Period3_15 has missing rate as 0.0 ThirdParty_Info_Period3_16 has missing rate as 0.0 ThirdParty_Info_Period3_17 has missing rate as 0.0 ThirdParty_Info_Period4_1 has missing rate as 0.0 ThirdParty_Info_Period4_2 has missing rate as 0.0 ThirdParty_Info_Period4_3 has missing rate as 0.0 ThirdParty_Info_Period4_4 has missing rate as 0.0 ThirdParty_Info_Period4_5 has missing rate as 0.0 ThirdParty_Info_Period4_6 has missing rate as 0.0 ThirdParty_Info_Period4_7 has missing rate as 0.0 ThirdParty_Info_Period4_8 has missing rate as 0.0 ThirdParty_Info_Period4_9 has missing rate as 0.0 ThirdParty_Info_Period4_10 has missing rate as 0.0 ThirdParty_Info_Period4_11 has missing rate as 0.0 ThirdParty_Info_Period4_12 has missing rate as 0.0 ThirdParty_Info_Period4_13 has missing rate as 0.0 ThirdParty_Info_Period4_14 has missing rate as 0.0 ThirdParty_Info_Period4_15 has missing rate as 0.0 ThirdParty_Info_Period4_16 has missing rate as 0.0 ThirdParty_Info_Period4_17 has missing rate as 0.0 ThirdParty_Info_Period5_1 has missing rate as 0.0 ThirdParty_Info_Period5_2 has missing rate as 0.0 ThirdParty_Info_Period5_3 has missing rate as 0.0 ThirdParty_Info_Period5_4 has missing rate as 0.0 ThirdParty_Info_Period5_5 has missing rate as 0.0 ThirdParty_Info_Period5_6 has missing rate as 0.0 ThirdParty_Info_Period5_7 has missing rate as 0.0 ThirdParty_Info_Period5_8 has missing rate as 0.0 ThirdParty_Info_Period5_9 has missing rate as 0.0 ThirdParty_Info_Period5_10 has missing rate as 0.0 ThirdParty_Info_Period5_11 has missing rate as 0.0 ThirdParty_Info_Period5_12 has missing rate as 0.0 ThirdParty_Info_Period5_13 has missing rate as 0.0 ThirdParty_Info_Period5_14 has missing rate as 0.0 ThirdParty_Info_Period5_15 has missing rate as 0.0 ThirdParty_Info_Period5_16 has missing rate as 0.0 ThirdParty_Info_Period5_17 has missing rate as 0.0 ThirdParty_Info_Period6_1 has missing rate as 0.0 ThirdParty_Info_Period6_2 has missing rate as 0.0 ThirdParty_Info_Period6_3 has missing rate as 0.0 ThirdParty_Info_Period6_4 has missing rate as 0.0 ThirdParty_Info_Period6_5 has missing rate as 0.0 ThirdParty_Info_Period6_6 has missing rate as 0.0 ThirdParty_Info_Period6_7 has missing rate as 0.0 ThirdParty_Info_Period6_8 has missing rate as 0.0 ThirdParty_Info_Period6_9 has missing rate as 0.0 ThirdParty_Info_Period6_10 has missing rate as 0.0 ThirdParty_Info_Period6_11 has missing rate as 0.0 ThirdParty_Info_Period6_12 has missing rate as 0.0 ThirdParty_Info_Period6_13 has missing rate as 0.0 ThirdParty_Info_Period6_14 has missing rate as 0.0 ThirdParty_Info_Period6_15 has missing rate as 0.0 ThirdParty_Info_Period6_16 has missing rate as 0.0 ThirdParty_Info_Period6_17 has missing rate as 0.0 SocialNetwork_8 has missing rate as 0.0 SocialNetwork_9 has missing rate as 0.0 SocialNetwork_10 has missing rate as 0.0 UserupdateInfo_7_freq has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_7_unique has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_7_avg_count has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_7_IDNUMBER has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_7_HASBUYCAR has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_7_MARRIAGESTATUSID has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_7_PHONE has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_30_freq has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_30_unique has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_30_avg_count has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_30_IDNUMBER has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_30_HASBUYCAR has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_30_MARRIAGESTATUSID has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_30_PHONE has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_60_freq has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_60_unique has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_60_avg_count has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_60_IDNUMBER has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_60_HASBUYCAR has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_60_MARRIAGESTATUSID has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_60_PHONE has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_90_freq has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_90_unique has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_90_avg_count has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_90_IDNUMBER has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_90_HASBUYCAR has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_90_MARRIAGESTATUSID has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_90_PHONE has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_120_freq has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_120_unique has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_120_avg_count has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_120_IDNUMBER has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_120_HASBUYCAR has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_120_MARRIAGESTATUSID has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_120_PHONE has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_150_freq has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_150_unique has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_150_avg_count has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_150_IDNUMBER has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_150_HASBUYCAR has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_150_MARRIAGESTATUSID has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_150_PHONE has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_180_freq has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_180_unique has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_180_avg_count has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_180_IDNUMBER has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_180_HASBUYCAR has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_180_MARRIAGESTATUSID has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 UserupdateInfo_180_PHONE has missing rate as 0.00016666666666666666 missing rate after making up is:0.0 LogInfo1_7_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_7_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_7_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_7_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_7_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_7_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_30_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_30_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_30_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_30_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_30_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_30_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_60_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_60_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_60_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_60_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_60_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_60_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_90_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_90_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_90_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_90_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_90_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_90_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_120_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_120_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_120_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_120_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_120_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_120_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_150_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_150_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_150_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_150_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_150_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_150_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_180_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_180_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo1_180_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_180_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_180_unique has missing rate as 0.03376666666666667 missing rate after making up is:0.0 LogInfo2_180_avg_count has missing rate as 0.03376666666666667 missing rate after making up is:0.0 12","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"mac下的开发软件安装","slug":"018linux/mac/mac下的开发软件安装","date":"2018-12-07T00:00:00.000Z","updated":"2019-02-21T07:05:30.248Z","comments":true,"path":"category/018linux/mac/mac下的开发软件安装.html","link":"","permalink":"http://yoursite.com/category/018linux/mac/mac下的开发软件安装.html","excerpt":"","text":"zookeeper安装brew install zookeeper 安装后，在/usr/local/etc/zookeeper/ 已经有了缺省配置 查看 ls /usr/local/etc/zookeeper/ 常用命令启动服务zkServerzkServer start 查看zookeeper运行及状态 zkServer status","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"mac","slug":"mac","permalink":"http://yoursite.com/tags/mac/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"凸函数","slug":"001机器学习/凸函数","date":"2018-12-05T00:00:00.000Z","updated":"2019-01-25T07:08:38.724Z","comments":true,"path":"category/001机器学习/凸函数.html","link":"","permalink":"http://yoursite.com/category/001机器学习/凸函数.html","excerpt":"","text":"概念凸函数，一种函数，函数图像以上的区域为凸集。典型凸函数的形状类似于字母 U。 凸集凸集，欧几里得空间的一个子集，其中任意两点之间的连线仍完全落在该子集内。例如，下面的两个图形都是凸集。 很多常见的损失函数（包括下列函数）都是凸函数： L2 损失函数 对数损失函数 L1 正则化 L2 正则化 凸优化 (convex optimization)使用数学方法（例如梯度下降法）寻找凸函数最小值的过程。机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"机器学习概论","slug":"001机器学习/机器学习概论","date":"2018-12-05T00:00:00.000Z","updated":"2019-01-26T07:34:02.888Z","comments":true,"path":"category/001机器学习/机器学习概论.html","link":"","permalink":"http://yoursite.com/category/001机器学习/机器学习概论.html","excerpt":"","text":"机器学习概念几个概念的关系人工智能是计算机科学的一个分支，目的是开发一种拥有智能行为的机器，让机器像人类一样思考。机器学习是人工智能的一种实现方法。 大数据是人工智能的基础，而使大数据转变为知识或生产力，离不开机器学习（Machine Learning），可以说机器学习是人工智能的核心，是使机器具有类似人的智能的根本途径。 而深度学习是机器学习的一个分支。对于传统的机器学习来说，特征提取不是一件简单的事情。在一些复杂问题上，要想通过人工的方式设计有效的特征集合，往往要花费很多的时间和精力。 深度学习解决的核心问题之一就是自动将简单的特征组合成更加复杂的特征，并利用这些组合特征解决问题。它除了可以学习特征和任务之间的关联以外，还能自动从简单特征中提取更加复杂的特征。 特征一般数据可以表示为一个矩阵，根据应用领域的不同，数据矩阵的行可以表示为实体、对象、特征向量、元组等。列可以被称为属性、特征维、度、变量、域等。 特征一般在模型中作为输入变量，即简单线性回归中的 x 变量。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征。一般表示如下： $\\{x_1, x_2, … x_N\\}$$\\sqrt {a_{1},a_{2},a_{i}\\ldots \\ldots a_{4}}$ 标签标签是我们要预测的事物，即简单线性回归中的 y 变量。标签可以是小麦未来的价格、图片中显示的动物品种、音频剪辑的含义或任何事物。 样本样本是指数据的特定实例：x。（我们采用粗体 x 表示它是一个矢量。）我们将样本分为以下两类： 有标签样本 无标签样本 有标签样本同时包含特征和标签。 模型模型定义了特征与标签之间的关系。例如，垃圾邮件检测模型可能会将某些特征与“垃圾邮件”紧密联系起来。我们来重点介绍一下模型生命周期的两个阶段： 训练是指创建或学习模型。也就是说，向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。 推断是指将训练后的模型应用于无标签样本。也就是说，使用经过训练的模型做出有用的预测。 算法分类 监督式学习和无监督式学习是否在人类监督下训练（监督式学习、无监督式学习、半监督式学习和强化学习） 一些最重要的监督式学习的算法： K-近邻算法 线性回归 逻辑回归 支持向量机 决策树和随机森林 神经网络 无监督式学习算法： 聚类算法 k-平均算法（k-Means） 分层聚类分析（Hierarchical Cluster Analysis，HCA） 最大期望算法（Expectation Maximization） 可视化和降维 主成分分析（PCA） 核主成分分析（Kernel PCA） 关联规则学习 Apriori 半监督式学习大多数半监督式学习算法是无监督式和监督式算法的结合。例如深度信念网络（DBN），它基于一种互相堆叠的无监督式组件，这个组件叫作受限玻尔兹曼机（RBM）。受限玻尔兹曼机以无监督的方式进行训练，然后使用监督式学习对整个系统进行微调。 强化学习基于实例与基于模型的学习另一种对机器学习系统进行分类的方法，是简单地将新的数据点和已知的数据点进行匹配，还是像科学家那样，对训练数据进行模式检测，然后建立一个预测模型（基于实例的学习和基于模型的学习）。 基于实例的学习系统先完全记住学习示例（example），然后通过某种相似度度量方式将其泛化到新的实例 基于模型的学习从一组示例集中实现泛化的另一种方法是构建这些示例的模型，然后使用该模型进行预测。这就是基于模型的学习 回归和分类回归这个词的来历? 回归是弗兰西斯·加尔顿提出的统计学术语，当时他正研究一个现象，那就是高个父母的孩子往往比他们要矮一些。由于高个父母的孩子在变矮，他就把这个趋势称为均数回归。后来这个名词就被他用于分析变量之间相关性的方法。 多输出的回归问题多类策略 聚类和降维降维降维的目的是在不丢失太多信息的前提下简化数据。汽车的里程与其使用年限存在很大的相关性，所以降维算法会将它们合并成一个代表汽车磨损的特征。这个过程叫作特征提取。 通常比较好的做法是，先使用降维算法减少训练数据的维度，再将其提供给另一个机器学习算法（例如监督式学习算法）。 机器学习的一般流程学习数据选择模型使用训练数据进行训练，从而使成本函数最小化应用模型对新示例进行预测（称为推断） 机器学习需要面对的问题误差度量训练数据和测试数据是否可学习效果评估参考网站","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"混淆矩阵","slug":"001机器学习/混淆矩阵","date":"2018-12-05T00:00:00.000Z","updated":"2019-01-21T07:33:27.418Z","comments":true,"path":"category/001机器学习/混淆矩阵.html","link":"","permalink":"http://yoursite.com/category/001机器学习/混淆矩阵.html","excerpt":"","text":"概念混淆矩阵是除了ROC曲线和AUC之外的另一个判断分类好坏程度的方法。下面的图中，结果只有2个值（1或者0) 混淆矩阵:打分值反应(预测=1)未反应(预测=0)合计真实结果呈现信号 (真实=1)A(击中) True PositiveB(漏报) False NegativeA+B未呈现信号 (真实=0)C(虚报) False PositiveD(正确否定) True NegativeC +D合计A+CB +DA+ B + C + D 正确率=(A+D)/(A+B+C+D) 灵敏度( Sensitivity ;覆盖率recall)=A/(A+B) 命中率(Precision、PV+)=A/(A+C) 特异度 (Specificity;负例的覆盖率)=D/(C+D) 负命中率( PV-) = D/(D+B) 灵敏度和特异性越 大越好。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}]},{"title":"NumPy学习","slug":"021python/使用NumPy","date":"2018-11-15T00:00:00.000Z","updated":"2019-02-20T07:43:23.316Z","comments":true,"path":"category/021python/使用NumPy.html","link":"","permalink":"http://yoursite.com/category/021python/使用NumPy.html","excerpt":"","text":"简介NumPy 数组与 Python 内置的列表类型非常相似。但是随着数组在维度上变大，NumPy 数组提供了更加高效的存储和数据操作。与Python 列表list相比，固定类型的 NumPy 式数组缺乏这种灵活性，但是能更有效地存储和操作数据。 测试如果没有安装，pip3 install numpy import numpy as np np.__version__ Out[64]: &apos;1.14.3&apos; 操作从Python列表创建数组np.array([1, 4, 2, 5, 3]) np.array([1, 2, 3, 4], dtype=&apos;float32&apos;) grid = np.array([[9, 8, 7], ...: [6, 5, 4]]) NumPy 要求数组必须包含同一类型的数据。如果类型不匹配，NumPy 将会向上转换 不同于 Python 列表，NumPy 数组可以被指定为多维的 np.array([range(i, i + 3) for i in [2, 4, 6]]) 快速创建数组创建一个长度为10的数组，数组的值都是0 np.zeros(10, dtype=int) 创建一个3×5的浮点型数组，数组的值都是1 np.ones((3, 5), dtype=float) 创建一个3×5的浮点型数组，数组的值都是3.14 np.full((3, 5), 3.14) 创建一个3×3的、均值为0、方差为1的 # 正态分布的随机数数组 np.random.normal(0, 1, (3, 3)) 创建一个5个元素的数组，这5个数均匀地分配到0~1，等差数列 np.linspace(0, 1, 5) 创建一个1-20且差为3的等差数列 np.arange(1,20,3) 随机生成整数数据x1 = np.random.randint(10, size=6) # 一维数组 x2 = np.random.randint(10, size=(3, 4)) # 二维数组 x3 = np.random.randint(10, size=(3, 4, 5)) # 三维数组 数据类型参看数据类型arry4.dtype 类型转换arry4 = arry3.astype(np.float64) 数组切片：获取子数组x[start:stop:step] x = np.arange(10) 获取数组的行和列一种常见的需求是获取数组的单行和单列。你可以将索引与切片组合起来实现这个功能，用一个冒号（:）表示空切片： print(x2[:, 0]) # x2的第一列 print(x2[0, :]) # x2的第一行 在获取行时，出于语法的简介考虑，可以省略空的切片：print(x2[0]) 如何复制？关于数组切片有一点很重要也非常有用，那就是数组切片返回的是数组数据的视图，而不是数值数据的副本。这一点也是 NumPy 数组切片和 Python 列表切片的不同之处：在 Python 列表中，切片是值的副本。 如果修改这个子数组，将会看到原始数组也被修改 创建数组的副本x2_sub_copy = x2[:2, :2].copy() NumPy数组的操作NumPy数组的属性每个数组有 nidm（数组的维度）、shape（数组每个维度的大小）和 size（数组的总大小）属性 #随机数 rand 可以生成[0,1)的随机多维数组。npr.rand(3,2) pip3 install matplotlib 参考网站 NumPy中文手册 NumPy教程 NumPy教程","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"Pandas分组技术","slug":"021python/Pandas分组","date":"2018-11-12T00:00:00.000Z","updated":"2019-02-20T07:42:28.221Z","comments":true,"path":"category/021python/Pandas分组.html","link":"","permalink":"http://yoursite.com/category/021python/Pandas分组.html","excerpt":"","text":"分组在数据处理中，需要先将数据进行拆分，然后在其对于的每个分组中进行运算。 df.groupby(‘key’)DataFrame 的 groupby() 方法进行绝大多数常见的分割 - 应用 - 组合操作。返回值不是一个 DataFrame 对象，而是一个 DataFrameGroupBy 对象，可以将它看成是一种特殊形式的 DataFrame，里面隐藏着若干组数据，但是在没有应用累计函数之前不会计算。 sample.groupby(‘class’)[‘math’].max() 拆分列table = pd.DataFrame({‘cust_id’:[10001,10001,10002,10002,10003], ‘type’:[‘Normal’,’Special_offer’,\\ ‘Normal’,’Special_offer’,’Special_offer’], ‘Monetary’:[3608,420,1894,3503,4567]}) cust_id type Monetary 0 10001 Normal 3608 1 10001 Special_offer 420 2 10002 Normal 1894 3 10002 Special_offer 3503 4 10003 Special_offer 4567 pd.pivot_table(table,index=’cust_id’,columns=’type’,values=’Monetary’) 将type拆分为两列，这里使用的是pd.pivot_table函数，第一个参数为待拆分列的表，index表示原数据中的标示列，columns表示该变量中的取值将会成为新变量的变量名，values表示待拆分的列。 type Normal Special_offer cust_id 10001 3608.0 420.0 10002 1894.0 3503.0 10003 NaN 4567.0 堆叠列堆叠列是拆分列的反操作，当存在表示列中有多个数值变量的时候，可以通过堆叠列将多列的数据堆积成一列。 table1 = pd.pivot_table(table,index=&apos;cust_id&apos;, columns=&apos;type&apos;, values=&apos;Monetary&apos;, fill_value=0, aggfunc=np.sum).reset_index() table1 pd.melt(table1, id_vars=&apos;cust_id&apos;, value_vars=[&apos;Normal&apos;,&apos;Special_offer&apos;], value_name=&apos;Monetary&apos;, var_name=&apos;TYPE&apos;) table1代表待堆叠列的列名，id_vars代表标示变量，value_vars代表待堆叠的变量，value_name为堆叠后值变量列的名称，var_name为堆叠后堆叠变量的名称 参考网站","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"Pandas学习","slug":"021python/Pandas学习","date":"2018-11-11T00:00:00.000Z","updated":"2019-02-20T07:43:33.307Z","comments":true,"path":"category/021python/Pandas学习.html","link":"","permalink":"http://yoursite.com/category/021python/Pandas学习.html","excerpt":"","text":"简介Pandas是在NumPy 基础上建立的新程序库，提供了一种高效的DataFrame 数据结构。DataFrame 本质上是一种带行标签和列标签、支持相同类型数据和缺失值的多维数组。pandas的数据结构主要分成三种：Series（一维数组）,DataFrame(二维数组)，和Panel（三维数组） 开始 import numpy as np import pandas as pd pd.version 创建Series普通NumPy 数组通过隐式定义的整数索引获取数值，而Pandas的Series对象用一种显式定义的索引与数值关联。 data = pd.Series([0.25, 0.5, 0.75, 1.0], index=[‘a’, ‘b’, ‘c’, ‘d’]) 产生的数据如下： a 0.25 b 0.50 c 0.75 d 1.00 如果不指定index，会自动加入0-n的整数索引。 根据字典来进行创建population_dict = {‘California’: 38332521, ‘Texas’: 26448193, ‘New York’: 19651127, ‘Florida’: 19552860, ‘Illinois’: 12882135} population = pd.Series(population_dict) 结果： California 38332521 Texas 26448193 New York 19651127 Florida 19552860 Illinois 12882135 dtype: int64 和字典不同，Series 对象还支持数组形式的操作，比如切片。 DataFrame通过Series 对象创建DataFrame可以理解为Series的容器，多个Series共用一个索引。 states=pd.DataFrame(population, columns=[‘population’]) states = pd.DataFrame({‘population’: population,‘area’: area}) 通过数据字典来进行创建d = {‘one’ : pd.Series([1., 2., 3.], index=[‘a’, ‘b’, ‘c’]),’two’ : pd.Series([1., 2., 3., 4.], index=[‘a’, ‘b’, ‘c’, ‘d’])}df = pd.DataFrame(d)df 最外边的字典对应列 one two a 1.0 1.0 b 2.0 2.0 c 3.0 3.0 d NaN 4.0 通过NumPy二维数组创建import numpy as npsample = pd.DataFrame(np.random.randn(4, 5), columns=[‘a’,’b’,’c’,’d’,’e’])sample a b c d e 0 -1.058367 -0.801376 -0.647295 -0.819267 0.024554 1 0.621486 1.598847 -1.604502 0.552664 -0.430284 2 -0.131764 0.234944 -0.390586 -0.653918 0.755551 3 -1.905653 0.484355 0.203149 -1.740032 1.652118 根据已有的列创建新列a=pd.read_csv(&apos;closeprice.csv&apos;,encoding=&apos;gbk&apos;) b={1:&apos;银行&apos;,2:&apos;房地产&apos;,4:&apos;医药生物&apos;,5:&apos;房地产&apos;,6:&apos;采掘&apos;,7:&apos;休闲服务&apos;,8:&apos;机械设备&apos;} a[&apos;ind&apos;]=a.ticker.map(b) a #其他操作 数据读取sample=pd.read_csv(&apos;closeprice.csv&apos;,encoding=&apos;gbk&apos;,dtype={&apos;ticker&apos;: str}) sample 切片和筛选DataFrame有三种切片方法，分别为loc,iloc和ix。 iloc方法只能使用数值作为索引选择行、列，loc方法在选择列时只能使用字符索引，ix方法则可以使用两种索引。 df.loc第一个参数是行标签，第二个参数为列标签。:,表示所有行。 sample.loc[:,[&apos;ticker&apos;,&apos;closePrice&apos;]] ix已经过期，如需要使用 sample.loc[sample.index[[0, 1, 2]], [‘ticker’,’closePrice’]] ix或loc选择时，行索引是前后都包括的，而iloc为前包后不包。 查询a[(a.closePrice&gt;10) &amp; (a.secShortName==’万科A’)] 使用queryPandas数据框提供了方法query，可以完成指定的条件查询 a.query(‘closePrice&gt;20’) 其他a[a[‘closePrice’].between(0,9.12,inclusive=False)] 不希望边界包含在内可以将inclusive参数设定为False 使用isin方法进行查询，进行字符匹配 a[a[&#39;secShortName&#39;].isin([&#39;世纪星源&#39;,&#39;万科A&#39;])] 进行正则表达式匹配进行查询，可使用str.contains a[a[‘secShortName’].str.contains(‘[万]+’)] 排序 sample.sort_values(‘score’,ascending=False,na_position=’last’) ascending=False代表降序排列，设定为True时表示升序排列（默认）；na_position=’last’表示缺失值数据排列在数据的最后位置（默认值），设定为’first’表示缺失值排列在数据的最前面 重复行处理Pandas提供查看、处理重复数据的方法duplicated和drop_duplicates data = pd.DataFrame({&apos;k1&apos;: [&apos;one&apos;] * 3 + [&apos;two&apos;] * 4, &apos;k2&apos;: [3, 2, 1, 3, 3, 4, 4]}) data[data.duplicated()] data.drop_duplicates() data.drop_duplicates(subset=[&apos;k1&apos;],keep=&apos;last&apos;) 删除列a.drop(&apos;Unnamed: 0&apos;, axis=&apos;columns&apos;) 或者 a.drop(‘Unnamed: 0’, axis=1) 重命名列名a.rename(columns={&apos;Unnamed: 0&apos;:&apos;id&apos;}) 替换在一些特定场合下，如错误值处理、异常值处理，可能会对原数据的某些值进行修改，此时会涉及类似SQL的insert或update操作。 单个替换 a.replace(1,np.nan) 遇到一次替换多个值时，使用字典形式， `sample.replace({&apos;score&apos;:{999:np.nan}, &apos;name&apos;:{&apos;Bob&apos;:np.nan}})` score列所有取值为999的值替换为NaN，name列中取值为’Bob’替换为NaN 参考网站https://www.cnblogs.com/prpl/p/5537417.htmlhttps://apachecn.github.io/pandas-doc-zh/io.html#io-read-csv-table NumPy教程 参考手册 NumPy教程","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"Pandas的数据合并","slug":"021python/Pandas的数据合并","date":"2018-11-11T00:00:00.000Z","updated":"2019-02-20T07:43:37.845Z","comments":true,"path":"category/021python/Pandas的数据合并.html","link":"","permalink":"http://yoursite.com/category/021python/Pandas的数据合并.html","excerpt":"","text":"简介在数据处理中，不免会遇到数据之间的合并。如将几个表的数据合成为一个，或者像SQL语句的关联查询。 方法横向连接Pandas数据框提供了merge方法以完成各种表的横向连接操作，这种连接操作与SQL语句的连接操作是类似的，包括内连接、外连接。此外，Pandas也提供了按照行索引进行横向连接的方法。 内连接内连接使用merge函数示例，根据公共字段保留两表共有的信息，how=’inner’参数表示使用内连接，on表示两表连接的公共字段，若公共字段在两表名称不一致时，可以通过left_on和right_on指定 外连接外连接（outer join）包括左连接（left join）、右连接（right join）和全连接（full join）三种连接。 左连接通过公共字段，保留左表的全部信息，右表在左表缺失的信息会以NaN补全df1.merge(df2,how=&#39;left&#39;,on=&#39;id&#39;) 右连接和左连接相对，右连接通过公共字段，保留右表的全部信息，左表在右表缺失的信息会以NaN补全df1.merge(df2,how=’right’,on=’id’) 全连接通过公共字段，保留两表的全部信息，两表互相缺失的信息会以NaN补全 df1.merge(df2,how=’outer’,on=’id’) 行索引连接pd.concat([df1,df2],axis=1) 纵向合并数据的纵向合并指将两张或多张表纵向拼接起来，使得原先两张或多张表的数据整合到一张表上。 Pandas中提供pd.concat方法用于完成横向和纵向合并，当参数axis=0时，类似于SQL中的UNION ALL操作。ignore_index=True表示忽略df1与df2的原先的行索引，合并并重新排列索引。 pd.concat([df1,df2],ignore_index=True,axis=0) 参考网站","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"Pandas缺失值/异常值处理","slug":"021python/Pandas缺失值处理","date":"2018-11-11T00:00:00.000Z","updated":"2019-02-20T07:43:41.902Z","comments":true,"path":"category/021python/Pandas缺失值处理.html","link":"","permalink":"http://yoursite.com/category/021python/Pandas缺失值处理.html","excerpt":"","text":"缺失值处理sample = pd.DataFrame({&apos;name&apos;:[&apos;Bob&apos;,&apos;Lindy&apos;,&apos;Mark&apos;, &apos;Miki&apos;,&apos;Sully&apos;,&apos;Rose&apos;], &apos;score&apos;:[99,78,999,77,77,np.nan], &apos;group&apos;:[1,1,1,2,1,2],}) 查看缺失情况在进行数据分析前，一般需要了解数据的缺失情况，在Python中可以构造一个lambda函数来查看缺失值。 sample.apply(lambda col:sum(col.isnull())/col.size) 以指定值填补Pandas数据框提供了fillna方法完成对缺失值的填补，例如对sample表的列score填补缺失值，填补方法为均值。 sample.score.fillna(sample.score.mean()) 缺失值指示变量每个有缺失值的变量可以生成一个指示哑变量，参与后续的建模。sample.score.isnull().apply(int) 离散化和分箱连续值经常需要离散化，假设你有某项研究中一组人群的数据，你想将他们进行分组，放入离散的年龄框中。我们将这些年龄分为18～25、26～35、36～60以及61及以上等若干组。 ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32] bins = [18, 25, 35, 60, 100] cats = pd.cut(ages, bins) cut返回的对象是一个特殊的Categorical对象。使用cats.codes和cats.categories可以分别查看所属分箱的编号和分箱。 cats.codes array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8) cats.categories IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]] closed=&apos;right&apos;, dtype=&apos;interval[int64]&apos;) 平均进行分箱，pandas将根据数据中的最小值和最大值计算出等长的箱 例如数据被切成四份，代码如下： pd.cut(data, 4, precision=2) 基于样本分位数进行分箱 cats = pd.qcut(data, 4) # 切成四份 pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]) #自定义分位数 异常值处理噪声值是指数据中有一个或几个数值与其他数值相比差异较大的值，又称为异常值、离群值（outlier）。对于大部分的模型而言，噪声值会严重干扰模型的结果，并且使结论不真实或偏颇，需要在数据预处理的时候清除所有噪声值。噪声值的处理方法有很多，对于单变量，常见的方法有盖帽法、分箱法；多变量的处理方法为聚类法。 参考网站","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"matplotlib入门学习","slug":"021python/matplotlib入门","date":"2018-11-11T00:00:00.000Z","updated":"2019-02-20T07:43:08.492Z","comments":true,"path":"category/021python/matplotlib入门.html","link":"","permalink":"http://yoursite.com/category/021python/matplotlib入门.html","excerpt":"","text":"简单绘图matplotlib.pyplot包中包含了简单绘图功能。 import numpy as np import matplotlib.pyplot as plt func = np.poly1d(np.array([1, 2, 3, 4]).astype(float)) x = np.linspace(-10, 10, 30) y = func(x) plt.plot(x, y) plt.xlabel(&apos;x&apos;) plt.ylabel(&apos;y(x)&apos;) plt.show() 绘制子图绘图时可能会遇到图中有太多曲线的情况，而你希望分组绘制它们。这可以使用subplot函数完成。 fig = plt.figure() ax1 = fig.add_subplot(2, 2, 1) ax2 = fig.add_subplot(2, 2, 2) ax3 = fig.add_subplot(2, 2, 3) plt.plot(np.random.randn(50).cumsum(), &apos;k--&apos;) ax1.hist(np.random.randn(100), bins=20, color=&apos;k&apos;, alpha=0.3) matplotlib所绘制的图位于图片（Figure）对象中。使用plt.figure生成一个新的图片。绘图命令plt.plot，matplotlib会在最后一个图片和子图上进行绘制。 使用子图网格创建图片是非常常见的任务，所以matplotlib包含了一个便捷方法plt.subplots，它创建一个新的图片，然后返回包含了已生成子图对象的NumPy数组。 fig, axes = plt.subplots(2, 2) axes axes[1][0].hist(np.random.randn(100), bins=20, color=&apos;g&apos;, alpha=0.3) plot函数参数语法 plot(Y) plot(X1,Y1,...) plot(X1,Y1,LineSpec,...) plot(Y)如果Y是m×n的数组，以1:m为X横坐标，Y中的每一列元素为Y坐标，绘制n条曲线； 参考网站https://blog.csdn.net/fenghuizhidao/article/details/83090043https://www.cnblogs.com/haore147/p/3633017.htmlhttps://blog.csdn.net/qq_30638831/article/details/79938967https://www.matplotlib.org.cnhttps://blog.csdn.net/wizardforcel/article/details/54782693","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"MathJax中Latex基本语法","slug":"059杂七杂八/Latex基本用法","date":"2018-11-09T00:00:00.000Z","updated":"2018-12-10T17:00:58.854Z","comments":true,"path":"category/059杂七杂八/Latex基本用法.html","link":"","permalink":"http://yoursite.com/category/059杂七杂八/Latex基本用法.html","excerpt":"","text":"入门介绍公式标记行内公式$\\alpha+\\beta=\\gamma$ : $\\alpha+\\beta=\\gamma$ 跨行公式$$\\alpha+\\beta=\\gamma$$ : $$\\alpha+\\beta=\\gamma$$ 上标与下标上标和下标分别使用^与_，例如x_i^2 : $x_i^2$ 一些常用函数\\sum用来表示求和符号，其下标表示求和下限，上标表示上限。如\\sum_1^n: $\\sum_1^n$。 \\int用来表示积分符号，同样地，其上下标表示积分的上下限。如\\int_1^\\infty : $\\int_1^\\infty$。 \\lim_{x\\to\\infty}：$\\lim_{x\\to\\infty}$ 其他特殊函数,例如’sin’,’max’,’ln’。 括号 小括号与方括号：使用原始的()，[]即可，如(2+3)[4+4]: $(2+3)[4+4]$ 大括号：由于大括号{}被用来分组，因此需要使用\\lbrace和\\rbrace来表示。如\\lbrace a*b \\rbrace : $\\lbrace a*b \\rbrace$ 尖括号：使用\\langle和\\rangle表示左尖括号和右尖括号。如\\langle x \\rangle : $\\langle x \\rangle$ 上取整：使用\\lceil和\\rceil表示。如\\lceil x \\rceil：$\\lceil x \\rceil$ 下取整：使用\\lfloor和\\rfloor表示。如\\lfloor x \\rfloor：$\\lfloor x \\rfloor$ 不可见括号：使用.表示 分号单个字母可以使用\\frac ab，如果你的分子或分母不是单个字符，请使用分组 \\frac{(n^2+n)(2n+2)}{6}：$ \\frac{(n^2+n)(2n+2)}{6}$ 或者使用\\over，来分隔一个组的前后两部分，如 (n^2+n)(2n+2) \\over 6: $(n^2+n)(2n+2) \\over 6$ 根号根式使用\\sqrt表示，如：\\sqrt[4]{\\frac xy}：$\\sqrt[4]{\\frac xy}$ 希腊字母 名称 Tex 显示 alpha \\alpha $\\alpha$ beta \\beta $\\beta$ delta \\Delta $\\Delta$ theta \\theta $\\theta$ 一个综合示列原始符号并不会随着公式大小缩放，可以使用\\left(...\\right)来自适应地调整括号大小。\\left\\lbrace\\sum_{i=0}^0 i^2 = \\frac{(n^2+n)(2n+2)}{6}\\right\\rbrace\\tag{1.2}$$\\left\\lbrace\\sum_{i=0}^0 i^2 = \\frac{(n^2+n)(2n+2)}{6}\\right\\rbrace\\tag{1.2}$$ 排列和组合表示排列使用\\binom{n+1}{2k}:$\\binom{n+1}{2k}$ 或{n+1 \\choose 2k}:${n+1 \\choose 2k}$ 集合关系与运算\\cup \\cap \\setminus \\subset \\subseteq \\subsetneq \\supset \\in \\notin \\emptyset \\varnothing:$\\cup \\cap \\setminus \\subset \\subseteq \\subsetneq \\supset \\in \\notin \\emptyset \\varnothing$ 其他特殊函数和符号 比较运算符：\\lt \\gt \\le \\ge \\neq:$\\lt \\gt \\le \\ge \\neq$ 箭头：\\to \\rightarrow \\leftarrow \\Rightarrow \\Leftarrow \\mapsto: $\\to \\rightarrow \\leftarrow \\Rightarrow \\Leftarrow \\mapsto$ 省略号：a_1 + a_2 + \\cdots + a_n , a_1, a_2, \\ldots , a_n：$$a_1 + a_2 + \\cdots + a_n , a_1, a_2, \\ldots , a_n$$ 参考资料https://www.cnblogs.com/linxd/p/4955530.html","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}],"keywords":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}]},{"title":"Latex表示矩阵和其他用法","slug":"059杂七杂八/Latex高级用法","date":"2018-11-09T00:00:00.000Z","updated":"2018-12-10T17:00:49.977Z","comments":true,"path":"category/059杂七杂八/Latex高级用法.html","link":"","permalink":"http://yoursite.com/category/059杂七杂八/Latex高级用法.html","excerpt":"","text":"矩阵基本用法使用$$\\begin{matrix}...\\end{matrix}$$来表示矩阵，在\\begin与\\end之间加入矩阵的元素即可。矩阵的行之间用\\分隔，列之间用&amp;分隔。 例如 $$\\begin{matrix} 1 &amp; x &amp; x^2 \\\\\\\\ 1 &amp; y &amp; y^2 \\\\\\\\ 1 &amp; z &amp; z^2 \\end{matrix}$$ 结果： $$\\begin{matrix} 1 &amp; x &amp; x^2 \\\\ 1 &amp; y &amp; y^2 \\\\ 1 &amp; z &amp; z^2 \\end{matrix}$$ 加括号可以使用特殊的matrix，即替换\\begin{matrix}...\\end{matrix}中的matrix为pmatrix , bmatrix , Bmatrix , vmatrix , Vmatrix. pmatrix \\begin{pmatrix} 1 &amp; 2 \\\\\\\\ 3 &amp; 4 \\\\\\\\ \\end{pmatrix}:$$\\begin{pmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ \\end{pmatrix}$$ bmatrix \\begin{bmatrix} 1 &amp; 2 \\\\\\\\ 3 &amp; 4 \\\\\\\\ \\end{bmatrix}:$$\\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ \\end{bmatrix}$$ 省略元素可以使用\\cdots ⋯ \\ddots ⋱ \\vdots ⋮来省略矩阵中的元素，如： \\begin{Vmatrix} 1 &amp; a_1 &amp; a_1^2 &amp; \\cdots &amp; a_1^n \\\\\\\\ 1 &amp; a_2 &amp; a_2^2 &amp; \\cdots &amp; a_2^n \\\\\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\\\\ 1 &amp; a_m &amp; a_m^2 &amp; \\cdots &amp; a_m^n \\end{Vmatrix} $$\\begin{Vmatrix} 1 &amp; a_1 &amp; a_1^2 &amp; \\cdots &amp; a_1^n \\\\ 1 &amp; a_2 &amp; a_2^2 &amp; \\cdots &amp; a_2^n \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; a_m &amp; a_m^2 &amp; \\cdots &amp; a_m^n \\end{Vmatrix}$$ 对齐的公式\\begin{align} \\sqrt{37} &amp; = \\sqrt{\\frac{73^2-1}{12^2}} \\\\ &amp; = \\sqrt{\\frac{73^2}{12^2} \\cdot \\frac{73^2-1}{73^2}} \\\\ &amp; = \\frac{73}{12} \\sqrt{1 - \\frac{1}{73^2}} \\\\ &amp; \\approx \\frac{73}{12} \\left( 1 - \\frac{1}{2 \\cdot 73^2} \\right) \\end{align} 需要使用形如\\begin{align}...\\end{align}的格式，其中需要使用&amp;来指示需要对齐的位置 $$\\begin{align} \\sqrt{37} &amp; = \\sqrt{\\frac{73^2-1}{12^2}}\\\\&amp; = \\sqrt{\\frac{73^2}{12^2} \\cdot \\frac{73^2-1}{73^2}} \\\\\\&amp; = \\frac{73}{12} \\sqrt{1 - \\frac{1}{73^2}} \\\\&amp; \\approx \\frac{73}{12} \\left( 1 - \\frac{1}{2 \\cdot 73^2} \\right) \\end{align}$$ 分类表达式定义函数的时候经常需要分情况给出表达式，可使用\\begin{cases}...\\end{cases}。其中，使用\\来分类，使用&amp;指示需要对齐的位置。如： f(n) = \\begin{cases} n/2, &amp; \\text{if $n$ is even} \\\\\\\\\\ 3n+1, &amp; \\text{if $n$ is odd} \\end{cases} $$f(n) = \\begin{cases} n/2, &amp; \\text{if $n$ is even} \\\\\\ 3n+1, &amp; \\text{if $n$ is odd} \\end{cases}$$ 参考资料https://www.cnblogs.com/linxd/p/4955530.html","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}],"keywords":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}]},{"title":"Python3编程基础","slug":"021python/python基本语法","date":"2018-10-11T00:00:00.000Z","updated":"2019-02-20T07:43:46.090Z","comments":true,"path":"category/021python/python基本语法.html","link":"","permalink":"http://yoursite.com/category/021python/python基本语法.html","excerpt":"","text":"准备环境Mac下安装 sudo pip3 install ipython ipython 基本语法基本运算7/3 7//3 #取整 100/(1+0.1)**2 #**用来计算幂函数 pow((1+0.1),2) 17%4 #计算余数 条件分支与循环控制ifif 表达式 : ... else : ... if 表达式 : ... elif 表达式 : ... else : ... if-else条件分支语句，系统会判断if后边的表达式是否成立，如果值为True则执行冒号下边的代码片，否则判断随后elseif的表达式。 三元运算符result = x if x&lt;5 else 5 whilewhile i&lt;=x : print(i) i += 1 for for循环的作用是可以循环输出集合/数组中的每一个元素，类似于Java中的ForEach语句. arr = [1, 2, 3, 4, 5] for j in arr : print(j) 注意不是arr[j] 使用range循环内置range函数的作用是记录范围，通常和for循环结合使用 result = range(0, 10 ,1) #range函数获得一个0-10的范围集合，以1为步进数（即每次+1） for item in result: #循环输出每一个元素 print(item) 模拟Java的标准for循环 arr = [1, 2, 3, 4, 5] for i in range(0, len(arr), 1) : print(arr[i]) 函数基本语法def hello(): #函数的声明 &apos;这是我的第一个函数&apos; #定义函数文档 print(&quot;hello world&quot;) print(hello.__doc__) #打印函数文档，双下划线 help(hello) #可以通过help函数打印函数文档 有参函数def info(name, age): #定义有参函数 return &quot;这是有参函数：I&apos;m %s %s years old&quot; %(name, age) #返回字符串 def info1(name=&quot;HanMeimei&quot;, age=36): #参数可以设置默认值 print(&quot;这是默认值参数函数：I&apos;m %s %s years old&quot; %(name, age)) 可变参数函数def myFunction1(*params): #定义可变参数的函数 print(&quot;参数的长度是：%s&quot; %len(params)) for item in params: print(item) myFunction1(&quot;hello&quot;, &quot;world&quot;) #函数的调用 匿名函数使用lambda表达式 g = lambda x, y: x + y 等同下面的 def f(a, b): return a + b 字符串常用操作str1 = &quot;hello world&quot; str2 = str1[3:5] #从str1的第4位截取到底6位，不包含底6位 print(str1[6:]) #截取第七个字符到结尾 print(str1[:-3]) #截取从头开始到倒数第三个字符之前 print(str1[2]) #截取第三个字符 result1 = str1.lower(); #将str1转换成小写 result2 = str1.upper(); #将str1转换成大写 str1 = &quot;hello world&quot; str1=str1.capitalize(); #首字母大写 print(str1) Hello world str1 = &quot; HelloWorld &quot; result1 = str1.lstrip(); #去str1的左侧空格 result2 = str1.rstrip(); #去str1的右侧空格 result3 = str1.strip(); #去str1的左右侧空格 str1 = &quot;HelloWorld&quot; len(s1) #求字符串长度 result = str1.count(&apos;o&apos;) #获取次数的函数 str1 = &quot;HelloWorld&quot; result1 = str1.startswith(&apos;ld&apos;) #判断str1是否是以字符串ld开头，返回值是布尔型 result2 = str1.endswith(&apos;ld&apos;) #判断str1是否是以字符串ld结尾，返回值是布尔型 str1 = &quot;HelloWorld&quot; result1 = str1.find(&apos;o&apos;) #从str1左侧查找o，如果找到返回位置索引，找不到返回-1 result1 = s1.find(&apos;o&apos;,7,10) result2 = str1.rfind(&apos;o&apos;) #从str1右侧查找o，如果找到返回位置索引，找不到返回-1 str1 = &quot;aaa,bbb,ccc&quot; result = str1.replace(&quot;,&quot;, &quot;#&quot;) #将str1中所有的逗号替换成井号 print(&quot;替换后的字符串是：%s&quot; %result) str1 = &quot;aaa,bbb,ccc&quot; result = str1.split(&quot;,&quot;) #将字符串按指定字符转换成字符串数组 print(result) list = [&apos;hello&apos;,&apos;world&apos;] str = &apos;-&apos; str.join(list) #list转换为str &apos;hello-world&apos; str1 = &quot;{a} love {b}&quot;.format(a=&quot;LiLei&quot;, b=&quot;HanMeimei&quot;) str2 = &quot;%s love %s&quot; %(&quot;LiLei&quot;, &quot;HanMeimei&quot;) s1 = &apos;hello&apos; s2 = &apos;world&apos; s3 = s1 + s2 #字符串连接 s1 = &apos;hello&apos; max(s1) #求字符串s1中最大字符 s1 = &apos;hello&apos; s1[::-1] #字符串翻转 &apos;olleh&apos; l = list(s1)#字符串转列表 l.reverse() #列表反转（reverse直接对列表进行了修改） result = &quot;&quot;.join(l)#使用列表的reverse方法 s1 = &apos;hello&apos; s1.isdigit()//检测字符串时候只由数字组成 s1.isalpha() ———— 检测字符串是否只由字母组成 str_test = &quot;{&apos;a&apos;: 1, &apos;b&apos;: 2}&quot; dict_test = eval(str_test) #字符串转换为字典 str_test2 = str(dict_test) 常用数据结构元组tuple小括号，元组中的元素的值不可以修改，因此比列表安全。 z = (&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) z? Type: tuple print(z) (&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;) 基本操作读取z中的元素，与list类似print(z[0]) 列表list中括号,有点像java中的list,list中可以放不同类型的元素，在python中数组也是用list进行表达。 y = [“A”, “B”, “C”, “a”, “b”, “c”] intArr = [1, 4, 2, 5, 3, 7, 9, 0] #整型数组 strArr = [“北京”, “上海”, “杭州”, “深圳”] #字符串数组 mixArr = [1, 3.44, “hello”, [1, 2, 3]] #混合数组 emptyArr = [] #空数组 基本操作读取y[-1]y[0]y[0: 3] 查找y.index(“a”) 插入最后面插入新元素 y.append(5)y.append([“d”, “e”]) 在指定位置插入新元素y.insert(3, “new”) 合并x = [“A”, “B”, “C”]y = [“A”, “B”, “D”]print(x+y)[‘A’, ‘B’, ‘C’, ‘A’, ‘B’, ‘D’] 删除y.remove(“B”) 分片截取数组中的一个片段，并生成一个新的数组 x=[0,1,2,3,4,5,6,7,8,9] 从开始位置到结束位置获取数组中的元素集合 x2 = x[1 : 3] print(x2) [1, 2] 是否存在某元素result = &quot;上海&quot; in strArr print(result); True result = &quot;ShangHai&quot; not in strArr print(result); 获取指定元素的个数strArr.append(&quot;上海&quot;) strArr.append(&quot;上海&quot;) result = strArr.count(&quot;上海&quot;); print(result); 3 排序intArr.sort() print(result) intArr.reverse() print(intArr) 获取最大值/最小值result = max(intArr) print(result) result = min(temp) result = sum(temp) 综合示列arr = [4, 2, 5, 3, 45, 232, 98, 1, 0, 6, 89, 43] #创建一个整型数组 for i in range(0, len(arr), 1): #for循环初始值是0，范围是数组的长度 if arr[i] % 2 == 0: #找到偶数的元素 arr[i] += 1 #让元素+1 arr.sort() #元素从小到大排序 cnt = arr.count(1) #获取指定元素的个数 print(&quot;值=1的元素个数：%s&quot; %cnt) print(arr) Set大括号,如果需要去掉重复，这时需要Set y = {&apos;x&apos;, &apos;z&apos;, &apos;y&apos;} y? Type: set 使用列表进行构建 a=[&quot;x&quot;,&quot;x&quot;,&quot;y&quot;,&quot;z&quot;] b=set(a) print(b) 集合操作x1 = set([“A”, “B”, “C”])y1 = set([“A”, “B”, “D”]) 交集print(x1&amp;y1){‘A’, ‘B’} 并集print(x1|y1){‘C’, ‘A’, ‘B’, ‘D’} 差集print(x1-y1){‘C’} 字典dict使用大括号{}表示 x = {&quot;name&quot;: &quot;zhangshang&quot;, &quot;age&quot;: 33, &quot;address&quot;: &quot;xxxx&quot;} print(x[&quot;name&quot;]) 获取print(x[‘name’]) 如果取一个不在的如果使用x[‘g],会出现异常，可以使用下面的get方法，返回Noneprint(x.get(‘g’))print(x.get(‘g’,’没有这个值’)) 方法print(x.keys())dict_keys([‘name’, ‘age’, ‘address’]) print(len(x)) 几个常用的内置函数range range() 函数返回的是一个可迭代对象（类型是对象），而不是列表类型 l = range(6) list() 函数是对象迭代器，可以把range()返回的可迭代对象转为一个列表list(range(10)) 语法range(stop)range(start, stop[, step]) 举例： &gt;&gt;&gt;range(5) range(0, 5) &gt;&gt;&gt; for i in range(5): ... print(i) ... 0 1 2 3 4 &gt;&gt;&gt; list(range(5)) [0, 1, 2, 3, 4] &gt;&gt;&gt; list(range(0)) [] &gt;&gt;&gt; mapmap()是 Python 内置的高阶函数，它接收一个函数 f 和一个 list，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 并返回。 map(lambda x: x + 1, l) 内置的map函数有两个参数，第一个是一个函数；第二个是一个可遍历的对象 def f1(x): return x*x list(map(f1,[1,2,3,4])) filterfilter函数会对指定序列执行过滤操作。接受一个参数，返回布尔值True或False。 list(filter(lambda x: x % 2 == 0, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])) reducereduce函数，reduce函数会对参数序列中元素进行累积。reduce函数的定义：reduce(function, sequence[, initial]) -&gt; value 在Python 3里,reduce()函数已经被从全局名字空间里移除了,它现在被放置在fucntools模块里用的话要先引入 from functools import reduce l = range(6) f = lambda accumValue, newValue: accumValue + newValue reduce(f, l, 0) 第1个参数是相加函数，第2个参数是需要求和的列表“l”，第3个（可选）参数为初始值0。 第一次调用function时，如果提供initial参数，会以sequence中的第一个元素和initial作为参数调用function，否则会以序列sequence中的前两个元素做参数调用function 面向对象和模块化模块模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py。模块可以被别的程序引入，以使用该模块中的函数等功能。 模块让你能够有逻辑地组织你的python代码段；把相关的代码分配到一个模块里能让你的代码更好用，更易懂。模块能定义函数，类和变量，模块里也能包含可执行的代码。 import语法通过import导入整个模块 语法： import module1[, module2[,… moduleN] 例如 import pandas as pd #pd为别名 From…import导入指定模块的部分属性或者模糊导入 语法： from modname import name1 例如 from mini_project.components.Person import sayHello这样就可以直接使用sayHello 导入模块的所有功能 from pandas import * 搜索路径当导入一个模块后，python解析器对模块文件的搜索顺序是：当前目录—-shell变量PYTHONPATH下的每个目录———-python模块路径目录模块的搜索路径存储在system模块的sys.path变量中，包括当前目录，pythonpath和安装过程决定的默认目录。 通过下面可以参看系统搜索路径 import sys print(sys.path) #默认搜索路径 如果程序动态添加可以使用 sys.path.append(&quot;/media&quot;) #添加模块路径到搜索路径中 包包是一个分层次的文件目录结构，它定义了一个由模块及子包，和子包下的子包等组成的 Python 的应用环境。使用包可以很好的组织Python的工程结构，一个包就是一个模块。 简单来说，Python的库其实就是一个包含init.py文件夹，但该文件夹下必须存在 init.py 文件用来指明这是一个包，其用来定义库属性和方法，该文件的内容可以为空。 一个简单工程参考目录如下： ├── __init__.py ├── __pycache__ │ └── __init__.cpython-36.pyc ├── components │ ├── Person.py │ ├── __init__.py │ └── __pycache__ │ ├── Person.cpython-36.pyc │ └── __init__.cpython-36.pyc └── tests ├── __init__.py └── test_person.py 类创建Person.py # -*- coding: UTF-8 -*- #------------------------------------- # file:Person.py class Person: #属性--变量 __sex = &quot;男&quot; #私有变量，只需要在变量前加两个下划线 name = None age = None #方法--函数 def sayHello(self, friendName): print( &quot;Hello %s I&apos;m %s %s years old&quot; % (friendName, self.name, self.age)) #构造函数 def __init__(self, name, age): self.name = name self.age = age 通过class关键字定义类名，要求类名的首字母大写，定义成员方法时，要求传递self参数（相当于Java中的this关键字）；Python中定义构造函数，要求构造函数的名称是init Python 没有严格区分公共 / 外部属性和私有 / 内部属性，但是按照惯例，前面带有下划线表示私有属性或方法。 创建测试文件test_person.py # -*- coding: UTF-8 -*- &quot;&quot;&quot; test_person.py此脚本用于测试person &quot;&quot;&quot; from os import sys, path if __name__ == &quot;__main__&quot;: # 得到mini_project所在的绝对路径 packagePath = path.dirname(path.dirname(path.dirname(path.abspath(__file__)))) # 将mini_project所在路径，加到系统路径里。这样就可以将mini_project作为库使用了 sys.path.append(packagePath) import mini_project.components.Person as p #import 面向对象.类对象.Person as p person = p.Person(&quot;张三&quot;, 28) person.sayHello(&quot;xxxxx&quot;) # print(p.__sex) #无法访问，因为__sex变量是私有的 print(person._Person__sex) #python是伪私有，如果想访问私有变量可以通过 p._Person__sex，即：对象名._类名__变量名 如果一个模块中的name的属性值为main,那么这个模块就是主模块。 直接运行python3 test_person.py ipython使用技巧一些技巧使用通配符 In [10]: *Warning? In [10]: str.*find*? 标准的 Python shell 仅仅包括一个用于获取以前的输出的简单快捷键。 In [9]: print(_) 如果想一次性获取此前所有的输入历史，%history 魔法命令会非常有用 In [16]: %history -n 1-4 ! 符号作为前缀在 IPython 中执行任何命令行命令 In [1]: !ls IPython shell中的快捷键Ctrl + a 将光标移到本行的开始处 Ctrl + e 将光标移到本行的结尾处 Ctrl + b（或左箭头键） 将光标回退一个字符 Ctrl + f（或右箭头键） 将光标前进一个字符 Ctrl + p（或向上箭头） 获取前一个历史命令 Ctrl + n（或向下箭头） 获取后一个历史命令 Ctrl + l 清除终端屏幕的内容 Ctrl + c 中断当前的 Python 命令 Ctrl + d 退出 IPython 会话 IPython增强命令粘贴代码块：%paste和%cpaste执行外部代码：%run计算代码运行时间：%timeit参考网站 Python语法 python3之模块","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}]},{"title":"概率学习","slug":"002数学相关/概率论学习","date":"2018-08-05T00:00:00.000Z","updated":"2019-02-18T05:42:52.149Z","comments":true,"path":"category/002数学相关/概率论学习.html","link":"","permalink":"http://yoursite.com/category/002数学相关/概率论学习.html","excerpt":"","text":"基本概念随机变量自然界和社会实践中产生现象分成确定性现象和随机现象。 随机现象产生的可能结果有多个无，且法事先预知那个结果会发生，概率论研究主要是随机现象。为了方便研究把随机现象产生的结果定义为一个变量，成为随机变量。一般使用大写字母来表示。 结果变量的可能取值称为结果，使用小写字母表示，所有结果组成的集合组成样本空间。 事件和概率取其中的部分结果组成的集合，称为随机事件，简称事件。事件本质为一个集合。 例如，随机变量X表示骰子的点数，事件A={2,4,6},含义为”抛出偶数点数”。 事件A发生的概率记为P(A),p(A)实际上为一个把事件映射到概率的函数。自变量为事件，因变量为事件发生的概率值。 随机变量的取值为有限的个数的为离散随机变量。 P(X=k) = f (其中k=2,4,6), 则函数f称为概率分布。 期望和方差对于取值不定的随机值，其可能的平均取值称为期望值，值和分散程度称为方差。 多个随机变量的关系联合概率/边缘概率条件概率（贝叶斯）事件的独立性随机变量的独立性 变量之间的相关程度一个变量的变化，其他变量将发生多大的变化。协方差 参考网站","categories":[{"name":"数学基础","slug":"数学基础","permalink":"http://yoursite.com/categories/数学基础/"}],"tags":[{"name":"概率论","slug":"概率论","permalink":"http://yoursite.com/tags/概率论/"}],"keywords":[{"name":"数学基础","slug":"数学基础","permalink":"http://yoursite.com/categories/数学基础/"}]},{"title":"CDH学习和安装","slug":"030大数据/CDH学习","date":"2018-05-23T00:00:00.000Z","updated":"2019-02-18T04:05:01.677Z","comments":true,"path":"category/030大数据/CDH学习.html","link":"","permalink":"http://yoursite.com/category/030大数据/CDH学习.html","excerpt":"","text":"测试环境安装docker pull cloudera/quickstart:latest docker rm cloudera -f docker run –name cloudera –privileged=true –hostname=quickstart.cloudera -p 8020:8020 -p 7180:7180 -p 8888:8888 -p 21000:21000 -p 21050:21050 -p 50070:50070 -p 50075:50075 -p 50010:50010 -p 50020:50020 -p 60010:60010 -p 4040:4040 -p 18088:18088 -p 60020:60020 -t -i -d -v /etc/localtime:/etc/localtime:ro cloudera/quickstart /usr/bin/docker-quickstart docker run –name cloudera –privileged=true –hostname=quickstart.cloudera -p 8020:8020 -p 7180:7180 -p 8888:8888 -p 21000:21000 -p 21050:21050 -p 50070:50070 -p 50075:50075 -p 50010:50010 -p 50020:50020 -p 60010:60010 -p 2181:2181 -p 60020:60020 -p 60000:60000 -p 18088:18088 -t -i -d -v /etc/localtime:/etc/localtime:ro cloudera/quickstart /usr/bin/docker-quickstart docker start cloudera 可以通过/usr/bin/docker-quickstart start来启动所有的CDH服务。 如果需要单独启动可单独运行/usr/bin下的命令。 集群测试cloudera-manager为分布式集群设计的，默认没有启动，如果需要进入该容器，并执行/home/cloudera/cloudera-manager --force --express来启动cloudera-manager，有些服务可能被停掉。 http://10.168.1.111:7180cloudera/cloudera进入系统 进入容器docker exec -ti cloudera /bin/bash 常用的管理地址默认用户：cloudera/cloudera 组件 web hue http://10.168.1.111:8888 hbase http://10.168.1.111:60010 或者 http://10.168.1.111:8888/hbase/#Cluster hdfs http://10.168.1.111:50070 数据库密码scm数据库的用户名，密码，可以通过下面获得 more /etc/cloudera-scm-server/db.properties mysql -ucm -pcloudera 常用进程和web端口 组件 节点 默认端口 配置 用途说明 HDFS NameNode 50070 dfs.namenode.http-address http服务的端口 HDFS DataNode 50010 dfs.datanode.address datanode服务端口，用于数据传输 HDFS DataNode 50075 dfs.datanode.http.address http服务的端口 HDFS DataNode 50020 dfs.datanode.ipc.address ipc服务的端口 HDFS SecondaryNameNode HDFS 8020 高可用的HDFS RPC端口 YARN Resourcemanager 8088 yarn.resourcemanager.webapp.address http，Yarn 的WEB UI接口 YARN NodeManager Hbase HMaster 60000 HBase Master管理端口 Hbase HMaster 60010 HBase Master Web UI管理端口 Hbase HRegionServer 60020 HRegionServer向HMaster定期汇报节点的负载状况 Hbase HRegionServer 60030 HBase Region Server Web UI 端口 Hive 9083 metastore服务默认监听端口 Hive 10000 Hive 的JDBC端口 HUE 8888 Hue WebUI 端口 Spark 7077 spark 的master与worker进行通讯的端口 standalone集群提交Application的端口 Spark 8080 master的WEB UI端口 资源调度 Spark 8081 worker的WEB UI 端口 资源调度 Spark 4040 Driver的WEB UI 端口 任务调度 Spark 18088 spark.history.ui.port Spark 19888 mapreduce.jobhistory.webapp.address CDH 7180 Cloudera Manager WebUI端口 CDH 7182 Cloudera Manager Server 与 Agent 通讯端口 ZooKeeper Zookeeper Client 2181 Zookeeper Client ZooKeeper 2888 zookeeper集群内通讯使用，Leader监听此端口 ZooKeeper 3888 zookeeper端口 用于选举leader ZooKeeper QuorumPeerMain Kafka 9092 Kafka集群节点之间通信的RPC端口 Redis 6379 Redis服务端口 例如：通过访问http://10.168.1.111:50070来打开界面进行访问NameNode hadoop集群中主要进程master: NameNode, ResourceManager,slaves: DataNode, NodeManager, RunJar, MRAppMaster,YarnChild 其中 RunJar, MRAppMaster,YarnChild与随着某个job的创建而创建，随着job的完成而终止。它们的作用分别是：RunJar：完成job的初始化，包括获取jobID，将jar包上传至hdfs等。MRAppMaster：每个job一个进程，主要跟踪job的运行情况，向RM申请资源等。YarnChild：运行具体的map/reduce task。 job启动过程：ResourceManage，NodeManager-&gt;RunJar-&gt;MRAppMaster-&gt;YarnChild job退出过程：YarnChild-&gt;MRAppMaster-&gt;RunJar即所有的map/reduce均完成后，MRAppMaster才退出，最后RunJar退出，job完成 参考https://hub.docker.com/r/cloudera/clusterdock/ https://wenku.baidu.com/view/1c2ad6ee43323968001c92be.html http://www.aboutyun.com/thread-7513-1-1.html cds搭建https://blog.csdn.net/u010476994/article/details/79257565https://blog.csdn.net/u010936936/article/details/73650417 https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cdh_ig_spark_configure.html CDH5 快速入门手册v1.0http://blog.itpub.net/31439905/viewspace-2134540/ 错误https://www.2cto.com/net/201609/544957.html 修改密码参考https://blog.csdn.net/yujin2010good/article/details/72482007","categories":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}],"keywords":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}]},{"title":"Hadoop常用组件操作","slug":"030大数据/Hadoop常用组件操作","date":"2018-05-17T00:00:00.000Z","updated":"2019-02-18T04:05:22.627Z","comments":true,"path":"category/030大数据/Hadoop常用组件操作.html","link":"","permalink":"http://yoursite.com/category/030大数据/Hadoop常用组件操作.html","excerpt":"","text":"测试环境安装QuickStart VM是一个包含了分布式数据处理平台的虚拟镜像，方便用户试运行CDH，并了解CDH的新功能、新组件。Docker也可以作为快速启动项来部署启动Apache Hadoop和Cloudera，而且速度要比其他启动方式更快。使用的镜像为cloudera/quickstart:latest 预装软件包(MySQL、Oozie、Hadoop、Hive、Zookeeper、Storm、Kafka以及Spark等等)。 #hdfs# 常用命令hadoop fs -mkdir /user/root/temp hadoop fs -ls /user/root/ hadoop fs -put express-deployment.json /user/root/temp/ hadoop fs -ls /user/root/temp/ 参考 hbase测试启动hbase shell 查看状态status 创建表建立一个表scores，有两个列族grad和courese create &apos;scores&apos;,&apos;grad&apos;,&apos;courese&apos; create &apos;test&apos;,&apos;a&apos; 查看表list 查看表结构describe ‘scores’ 插入数据put ‘表名’,’行键名’,’列名’,’单元格值’,’时间戳’，时间戳可以省略 put ‘scores’,’Tom’,’courese:math’,’100’put ‘scores’,’Mark’,’courese:english’,’120’ put ‘scores’,’Mark’,’grad’,’60’put ‘scores’,’Mark’,’grad’,’61’put ‘scores’,’Mark’,’gradxx’,’6’ 查看数据//查看表所有记录scan ‘scores’ //条件查询get ‘scores’,’Mark’get ‘scores’,’Mark’,’grad’ ###修改表结构(新增列族’ranking’) 增加一列族 alter ‘scores’,NAME=’ranking’ put ‘scores’,’Tom’,’ranking’,1put ‘scores’,’Mark’,’ranking’,2put ‘scores’,’Mark’,’ranking:m1’,7 ###删除表disable ‘hbasename’ drop ‘scores’ web页面进行访问 hive测试启动 hive zookeeper启动 zookeeper-client 进入后，可以执行的命令 显示根目录下文件：ls /创建文件，并设置初始内容： create /zk &quot;test&quot;获取文件内容： get /zk修改文件内容： set /zk &quot;zkbak&quot;删除文件： delete /zk 参考 参考https://hub.docker.com/r/cloudera/clusterdock/ https://wenku.baidu.com/view/1c2ad6ee43323968001c92be.html 从MySQL到Hivehttps://www.cnblogs.com/charlist/p/7122198.htmlhttps://blog.csdn.net/scgaliguodong123_/article/details/46626779","categories":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}],"keywords":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}]},{"title":"Elasticsearch入门学习","slug":"030大数据/elasticsearch学习","date":"2018-05-17T00:00:00.000Z","updated":"2019-02-18T04:05:16.315Z","comments":true,"path":"category/030大数据/elasticsearch学习.html","link":"","permalink":"http://yoursite.com/category/030大数据/elasticsearch学习.html","excerpt":"","text":"docker安装假设宿主机ip为10.168.1.111下载 docker pull elasticsearch:5.6.4 docker pull mobz/elasticsearch-head:5 安装esmaster配置mkdir -p ~/mydocker/esdata1/nodes vi ~/mydocker/es1.yml cluster.name: &quot;dali&quot; node.name: node1 node.master: true node.data: true http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; xpack.security.enabled: false network.host: 0.0.0.0 discovery.zen.minimum_master_nodes: 1 docker run -u 1000 -d –name es1 -p 9200:9200 -p 9300:9300 -v ~/mydocker/es1.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v ~/mydocker/esdata1:/usr/share/elasticsearch/data docker.elastic.co/elasticsearch/elasticsearch:5.6.4 docker run -u 1000 -d –name es1 -p 9200:9200 -p 9300:9300 -v ~/mydocker/es1.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v ~/mydocker/esdata1:/usr/share/elasticsearch/data elasticsearch:5.6.4 如果启动出现docker max virtual memory areas vm.max_map_countsudo sysctl -w vm.max_map_count=655360 node配置mkdir -p ~/mydocker/esdata2/nodes vi ~/mydocker/es2.yml cluster.name: &quot;dali&quot; node.name: node2 node.master: false node.data: true http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; xpack.security.enabled: false network.host: 0.0.0.0 discovery.zen.minimum_master_nodes: 1 discovery.zen.ping.unicast.hosts: es1 docker run -d –name es2 –link es1:es1 -p 9201:9200 -p 9301:9300 -v ~/mydocker/es2.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v ~/mydocker/esdata2:/usr/share/elasticsearch/data docker.elastic.co/elasticsearch/elasticsearch:5.6.4 安装headelasticsearch head是集群管理工具、数据可视化、增删改查工具， Elasticsearch 语句可视化 安装过程如下： docker run -d –name head -p 9100:9100 docker.io/mobz/elasticsearch-head:5 通过访问http://10.168.1.111:9100，打开head界面，修改集群地址http://10.168.1.111:9200/ 安装中文分词进入容器 docker exec -it es1 /bin/bash 执行如下: ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.6.4/elasticsearch-analysis-ik-5.6.4.zip 重新启动docker restart es1 基本概念Node 与 Cluster节点（Node），一个运行的Elasticearch实例，一般是一台机器上的一个进程。一组节点构成一个集群（cluster） 索引（index）/文档（document）索引看成关系型数据库的表。Index里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。一个文档相当于数据库表中的一行记录。 文档的json表示如下: { &quot;user&quot;: &quot;张三&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;数据库管理&quot; } 分片 副本为了支持更大量的数据，索引一般会按某个维度分成多个部分，每个部分就是一个分片，分片被节点(Node)管理。 同一个分片(Shard)的备份数据，一个分片可能会有0个或多个副本，这些副本中的数据保证强一致或最终一致。 基本操作索引操作新建和删除Indexcurl -X PUT &apos;localhost:9200/weather&apos; curl -X DELETE &apos;localhost:9200/weather&apos; 索引库名称必须要全部小写，不能以下划线开头，也不能包含逗号 查询所有的索引curl -X GET &apos;http://localhost:9200/_cat/indices?v&apos; 创建文档格式http://localhost:9200/&lt;index&gt;/&lt;type&gt;/[&lt;id&gt;]分别为：索引名称/类型名称/ID curl -XPUT &apos;http://10.168.1.111:9200/blog/article/1&apos; -d&apos;{&quot;title&quot;: &quot;New version of Elasticsearch released!&quot;,&quot;content&quot;:&quot;Version 1.0 released today!&quot;}&apos; 索引和类型是必需的，而id部分是可选的如果不指定ID，ElasticSearch会为我们生成一个ID。应该使用HTTP的POST而不是PUT请求如果使用put,出现No handler found for uri错误 以下为不指定id的使用示例 curl -XPOST &apos;http://10.168.1.111:9200/blog/article/&apos; -d&apos;{&quot;title&quot;: &quot;New version of Elasticsearch released!&quot;,&quot;content&quot;:&quot;Version 2.0 released today!&quot;}&apos; 更新文档curl -XPOST http://localhost:9200/blog/article/1/_update -d &apos;{&quot;script&quot;: &quot;ctx._source.content = \\&quot;new content\\&quot;&quot;}&apos; 检索文档curl -XGET http://localhost:9200/blog/article/1 ##中文索引设置 凡是需要搜索的中文字段，都要单独设置一下 curl -X PUT ‘localhost:9200/accounts’ -d ‘{ “mappings”: { “person”: { “properties”: { “user”: { “type”: “text”, “analyzer”: “ik_max_word”, “search_analyzer”: “ik_max_word” }, “title”: { “type”: “text”, “analyzer”: “ik_max_word”, “search_analyzer”: “ik_max_word” }, “desc”: { “type”: “text”, “analyzer”: “ik_max_word”, “search_analyzer”: “ik_max_word” } } } }}’ 数据查询返回所有记录curl &apos;localhost:9200/accounts/person/_search&apos; 全文搜索curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos; { &quot;query&quot; : { &quot;match&quot; : { &quot;desc&quot; : &quot;软件&quot; }} }&apos; 如果搜索单个 curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos; { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;query_string&quot;: { &quot;default_field&quot;: &quot;_all&quot;, &quot;query&quot;: &quot;张&quot; } } ], &quot;must_not&quot;: [ ], &quot;should&quot;: [ ] } }, &quot;from&quot;: 0, &quot;size&quot;: 10, &quot;sort&quot;: [ ], &quot;aggs&quot;: { } }&apos; orcurl &apos;localhost:9200/accounts/person/_search&apos; -d &apos; { &quot;query&quot; : { &quot;match&quot; : { &quot;desc&quot; : &quot;软件 系统&quot; }} }&apos; 执行多个关键词的and搜索，必须使用布尔查询 curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos; { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;desc&quot;: &quot;软件&quot; } }, { &quot;match&quot;: { &quot;desc&quot;: &quot;系统&quot; } } ] } } }&apos; 安装kibanadocker pull docker.elastic.co/kibana/kibana:5.6.2 docker run -p 5601:5601 -e &quot;ELASTICSEARCH_URL=http://10.168.1.111:9200&quot; --name my-kibana \\ --network host -d docker.elastic.co/kibana/kibana:5.6.2 参考社区https://elasticsearch.cn Elasticsearch: 权威指南","categories":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}],"keywords":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}]},{"title":"Docker下MongoDB复制集","slug":"030大数据/mongod学习","date":"2018-05-17T00:00:00.000Z","updated":"2019-02-18T04:05:30.929Z","comments":true,"path":"category/030大数据/mongod学习.html","link":"","permalink":"http://yoursite.com/category/030大数据/mongod学习.html","excerpt":"","text":"安装过程安装docker run -p27018:27017--name mongo0 -d mongo:3.6.2-jessie --replSet &quot;rs0&quot; --bind_ip_all docker run --name mongo1 -d mongo:3.6.2-jessie --replSet &quot;rs0&quot; --bind_ip_all docker run --name mongo2 -d mongo:3.6.2-jessie --replSet &quot;rs0&quot; --bind_ip_all 查询ip地址docker inspect mongo0 | grep IPAddress，可知地址如下 mongo0: 172.17.0.2:27017mongo1: 172.17.0.3:27017mongo2: 172.17.0.4:27017 /data/db/usr/bin/mongod 启动和关闭启动mongo 关闭进入use admindb.shutdownServer()； 复制测试在主节点中插入数据进入到172.17.0.2容器mongo mongodb://172.17.0.2:27017,172.17.0.3:27017,172.17.0.4:27017/test?replicaSet=rs0 在rs0:PRIMARY&gt;下执行 db.order.insert({price: 1})WriteResult({ “nInserted” : 1 }) 从库进行读操作mongo --host 172.17.0.3在rs0:SECONDARY&gt; 下执行 db.getMongo().setSlaveOk()允许当前连接对从库进行读操作进行查询db.order.find(){ “_id” : ObjectId(“5a7c5473b74e8cf1bb890979”), “price” : 1 } 查询状态rs.status()查询 &quot;_id&quot; : 2, &quot;name&quot; : &quot;172.17.0.4:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, 重新选举测试关闭172.17.0.4docker stop mongo2 重新查询，这时主节点进行了变更。 常用命令查询库在主库环境下 show dbs; 创建和切换库use mydb; or mongo mydb; 参考https://www.cnblogs.com/jay54520/p/8433515.html#%E5%A4%8D%E5%88%B6%E7%89%B9%E6%80%A7 http://blog.51cto.com/5063935/2074332","categories":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"},{"name":"MongoDB","slug":"MongoDB","permalink":"http://yoursite.com/tags/MongoDB/"}],"keywords":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}]},{"title":"Hbase学习","slug":"030大数据/hbase学习","date":"2018-05-17T00:00:00.000Z","updated":"2019-02-18T04:05:26.793Z","comments":true,"path":"category/030大数据/hbase学习.html","link":"","permalink":"http://yoursite.com/category/030大数据/hbase学习.html","excerpt":"","text":"HBase的特性容量巨大HBase的单表可以有百亿行、百万列 面向列HBase是面向列的存储和权限控制，并支持列独立检索。 vs传统行式数据库 数据是按行存储的 没有索引的查询使用大量I/O 建立索引和物化视图需要花费大量的时间和资源 列式数据库的特性 数据是按行存储的 数据即索引，只访问查询涉及的列，可以大量降低系统I/O 数据类型一致，数据特征相似，可以高效压缩 稀疏性在大多数情况下，采用传统行式存储的数据往往是稀疏的，即存在大量为空（NULL）的列，而这些列都是占用存储空间的，这就造成存储空间的浪费。对于HBase来讲，为空的列并不占用存储空间，因此，表可以设计得非常稀疏。 扩展性HBase底层文件存储依赖HDFS，从“基因”上决定了其具备可扩展性。 高可靠性HBase提供WAL和Replication机制。前者保证了数据写入时不会因集群异常而导致写入数据的丢失；后者保证了在集群出现严重问题时，数据不会发生丢失或者损坏。 操作## create ‘JSpider_cn.made-in-china.com’,’a’ 读取HBase不能支持where条件、Order by 查询，只支持按照Row key来查询，但是可以通过HBase提供的API进行条件过滤 scan ‘blogtable’ ,{COLUMNS =&gt; [‘text:’,’info:title’] } —&gt; 列出 文章的内容和标题scan ‘blogtable’ , {COLUMNS =&gt; ‘info:url’ , STARTROW =&gt; ‘2’} —&gt; 根据范围列出 文章的内容和标题 get ‘blogtable’,’1’, {COLUMN =&gt; ‘info’} —&gt; 列出 文章id 等于1的info 的头(Head)内容get ‘blogtable’,’1’, {COLUMN =&gt; [‘text’,’info:author’]} 参考https://hub.docker.com/r/cloudera/clusterdock/ https://blog.csdn.net/scgaliguodong123_/article/details/46626779http://www.cnblogs.com/cnmenglang/p/6555828.html 值得学习https://www.cnblogs.com/charlist/p/7063991.html接口文章https://cloud.tencent.com/developer/article/1018573 https://blog.csdn.net/dreamsigel/article/details/53835013?fps=1&amp;locationNum=8 Elasticsearch+hbasehttps://blog.csdn.net/cafebar123/article/details/79430782 elasticsearch整合hbase步骤https://blog.csdn.net/high2011/article/details/51596853 mobhttps://yq.aliyun.com/articles/586784 搭建安装https://www.jianshu.com/p/b2d9194ad6f5 HBase中存取图片、文档数据（HBase MOB）https://my.oschina.net/u/234661/blog/1553207","categories":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"},{"name":"Hbase","slug":"Hbase","permalink":"http://yoursite.com/tags/Hbase/"}],"keywords":[{"name":"大数据学习","slug":"大数据学习","permalink":"http://yoursite.com/categories/大数据学习/"}]},{"title":"Linux日常命令","slug":"018linux/0Linux日常命令","date":"2018-05-06T00:00:00.000Z","updated":"2019-03-06T14:42:06.688Z","comments":true,"path":"category/018linux/0Linux日常命令.html","link":"","permalink":"http://yoursite.com/category/018linux/0Linux日常命令.html","excerpt":"","text":"日常操作ssh相关操作登录ssh -p 10622 ww@10.21.10.11 copy文件到服务器scp spark-1.6.1-bin-hadoop2.6.tgz root@10.254.62.6:/root/Public 或者使用rz 下载文件wget -O CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 压缩和解压缩压缩ctar -cvf jpg.tar *.jpg 解压缩xtar zxvf apache-tomcat-7.0.41.tar.gz 解压指定文件夹：tar -xzvf jdk-8u131-linux-x64.tar.gz -C /usr/local/java 其他参数参考-f: 必须参数，放在最后，后面只能接档案名。-z：有gzip属性的-j：有bz2属性的-Z：有compress属性的-v：显示所有过程-O：将文件解开到标准输出 列表显示文件ls -lh 树形参看本目录下的文件tree -L 2 查找目录下的文件sudo find / -name my.cnf 指定查询深度 find . -maxdepth 1 -name &#39;docker&#39; find /data/ -mtime -5 最近访问时间 access time （-atime）、最近更改时间 modify time （-mtime） 和 最近状态改动时间 change time（-ctime），可以使用stat a.txt grep查询文件内容grep &#39;call调用请\\|TransDate&#39; *.loggrep 交易凭证 *.log -C5 查询目录下的指定文件中搜索you文本find ./ -name &quot;*.txt&quot; | xargs grep &quot;you&quot; {}\\; 使用egrep egrep &#39;call调用.*XM2016111100000026&#39; default.log.2016-11-25.0.log -A30 -A：显示匹配到字符那行的后面n行-B：显示匹配到字符那行的前面n行-C：显示匹配到字符那行的前后n行 建立符号链接（目的到源）ln -s /usr/local/bin/abc abc 复制文件cp *.png *.jpeg *.gif /destpath 如果想剪切，就把cp改为mv 文件内容进行替换find -type f ! -path &#39;*/.vagrant/*&#39; | xargs sed -i &#39;s/10.211.55/10.168.1/g 使用需要转义 find -type f ! -path &#39;*/.svn/*&#39; | xargs sed -i &#39;s/abc.xxx.cn/localhost:8090\\//static-inner/g&#39; 用户和权限操作ls -al参看文件的权限 drwxr-xr-x表示的含义：第1个字符：表示文档的类型，d表示是目录，-表示是文件ugo第2~4个字符：文件所有者的权限（u表示，user）第5~7个字符：所有者同组成员的权限（g表示，group）第8~10个字符：其他用户的权限（o表示，other） 八进制的语法表示为r(4)、w(2)、x(1)、-(0),4表示读权限,2表示写权限,1表示执行权限，0表示没有权限。例如：-rw-r–r– (644) 只有拥有者有读写权限；而属组用户和其他用户只有读权限-rwxrwxrwx (777) 所有用户都有读、写、执行权限 新增加一个用户并将其加入到存在的用户组developers中useradd -G developers ww 查看该用户的属性id ww 将一个已有用户增加到一个已存在的用户组developers中usermod -a -G developers ww 将ww的主要用户组改为 developers，则直接使用 -g 选项usermod -g developers ww 修改 tmp 目录所属用户为 root，用户组为 rootchown -R root:root /tmp 修改 tmp 目录为所有权限chmod -R 777 /tmp 系统基本信息linux版本查看系统版本 lsb_release -a cat /proc/version正在运行的内核版本输入”cat /etc/issue”, 显示的是发行版本信息 cpu信息cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 内存cat /proc/meminfo 磁盘sudo fdisk -l 系统维护磁盘使用情况df -h 文件大小当前文件夹下的文件大小du -sh * 进行排序并且显示前5个 du -h /mydata/ --max-depth=2 |sort -n | head -5 查找系统中的大目录sudo du -h / --max-depth=1 | sort -nr | head -5 查找目录下的大文件 sudo find /mydata -type f -size +500M -print0 | xargs -0 ls -lh | sort -nr 进程查询那些进程在使用ps -eo pid,user,cmd | grep ssh 显示某个程序是否在运行ps -aux | grep &quot;memcached&quot; 使用lsof查看占用端口的进程lsof -i :3306 或者使用 sudo netstat -anop | grep 3306 -p 选项查看进程信息 netstat -tnlp 可以查看监听的端口，其中l是listening, p是显示program, n是显示ip而不是name, t只查看tcp的内容 检查包的安装情况rpm -qa |grep jdk 安装rpm -ivh jdk-7-linux-i586.rpm 关机sudo shutdown -h now 参考tarlinux查看cpu、内存、版本信息Linux 中将用户添加到组的指令","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"常用","slug":"常用","permalink":"http://yoursite.com/tags/常用/"},{"name":"更新","slug":"更新","permalink":"http://yoursite.com/tags/更新/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"k8s入门","slug":"011容器技术/k8s安装","date":"2018-02-27T00:00:00.000Z","updated":"2018-12-10T15:20:45.094Z","comments":true,"path":"category/011容器技术/k8s安装.html","link":"","permalink":"http://yoursite.com/category/011容器技术/k8s安装.html","excerpt":"","text":"背景安装步骤安装docker包$ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common添加docker官方GPG秘钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -安装稳定版仓库 sudo add-apt-repository \\ “deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable”再次更新源 sudo apt-get update安装docker-ce 参考apt-cache madison docker-ce sudo apt-get install docker-ce= .默认情况下这个配置文件夹并不存在，我们要创建它$ mkdir -p /etc/systemd/system/docker.service.d 系统初始化echo “127.0.0.1 ww-server-32” &gt;&gt; /etc/hosts 关闭swap如果不关闭，默认配置下kubelet将无法启动swapoff -a 设置系统参数修改内核参数 cat &lt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF 修改后，及时生效sysctl –system 安装相应软件配置apt-get的源cat kube_apt_key.gpg | apt-key add - echo &quot;deb [arch=amd64] https://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main main&quot; &gt;&gt; /etc/apt/sources.list apt-get install -y kubelet kubeadm kubectl kubernetes-cni 安装镜像 如果下载自己的或者dockerhub的镜像。可以利用脚本，批量替换镜像imagenamedocker images | sed ‘s/foxchan/gcr.io\\/google_containers/‘| awk ‘{print “docker tag “$3” “$1”:”$2}’ docker的cgroup driver与 –cgroup-driver要一致。 可以用 docker info |grep Cgroup 查看，有可能是systemd 或者 cgroupfs 配置kubelet配置pod的基础镜像cat &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf &lt;&lt;EOF [Service] Environment=&quot;KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/szss_k8s/pause-amd64:3.0&quot; EOF 启动kubeletsystemctl daemon-reload systemctl restart kubelet 使用kubelet version参看可以看到已经启动 systemctl status kubelet.service 需要具体信息可以使用参看 journalctl -u kubelet -f kubelet version –runtime-cgroups=/systemd/system.slice –kubelet-cgroups=/systemd/system.slice 执行以下命令可以重启kubelet systemctl restart kubelet systemctl daemon-reload ps -ef | grep kubelet /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --cluster-dns=10.96.0.10 --cluster-domain=cluster.local --authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt --cadvisor-port=0 --rotate-certificates=true --cert-dir=/var/lib/kubelet/pki --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 重新配置systemctl stop kubelet;docker rm -f $(docker ps -q);mount | grep “/var/lib/kubelet/*” | awk ‘{print $3}’ | xargs umount 1&gt;/dev/null 2&gt;/dev/null;rm -rf /var/lib/kubelet /etc/kubernetes /var/lib/etcd /etc/cni;systemctl start kubelet;kubeadm init –token= 使用kubeadm初始化集群export KUBE_REPO_PREFIX=&quot;registry.cn-hangzhou.aliyuncs.com/szss_k8s&quot; export KUBE_ETCD_IMAGE=&quot;registry.cn-hangzhou.aliyuncs.com/szss_k8s/etcd-amd64:3.0.17&quot; kubeadm init --kubernetes-version=v1.9.0 --skip-preflight-checks kubeadm init –kubernetes-version=v1.9.2 –skip-preflight-checks kubeadm init –pod-network-cidr=172.30.0.0/20 –kubernetes-version=v1.9.0 –skip-preflight-checks 如果kubeadm出错，修改完成之后需要 kubeadm reset在重启初始化 让master节点参与调度默认情况下，为了保证master的安全，master是不会被调度到app的。你可以取消这个限制通过输入。 kubectl taint nodes --all node-role.kubernetes.io/master- 检查master安装kubectl get componentstatuses 里面有其他的文件/etc/kubernetes/manifests 配置kubectl的kubeconfig最新的kubectl链接api server不是通过8080端口连接的，而是通过6443端口，可以在admin.conf查看kubectl get nodesThe connection to the server localhost:8080 was refused - did you specify the right host or port? 或者出现Unable to connect to the server: x509: mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/configkubectl get nodes NAME STATUS ROLES AGE VERSION ww-server-32 NotReady master 3m v1.9.0 kube-dns镜像使用 kubectl get nodes参看NotReady 没有发现kube-dns镜像 要配置pod的网络，这里使用了flannel网络 kubectl create -f kube-flannel-rbac.yml # Create the clusterrole and clusterrolebinding: # $ kubectl create -f kube-flannel-rbac.yml # Create the pod using the same namespace used by the flannel serviceaccount: # $ kubectl create --namespace kube-system -f kube-flannel.yml --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: - &quot;&quot; resources: - pods verbs: - get - apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 安装其他主机kubeadm init –apiserver-advertise-address=172.16.120.200 –pod-network-cidr=192.168.0.0/16 –ignore-preflight-errors ‘Swap’主要目的是下载k8s相关组件d 使用flanne，需要设置–pod-network-cidr参数，10.244.0.0/16是kube-flannel.yml文件配置的默认网段，如果需要修改，–pod-network-cidr和kube-flannel.yml文件需要保持一致。kubeadm init –kubernetes-version=v1.9.0 –pod-network-cidr=10.244.0.0/16 –skip-preflight-checks 查看集群状态机器的端口如下： 参数 解释 Kubernetes API server 6443* etcd server client API 2379-2380 Kubelet API 10250 kube-scheduler 10251 kube-controller-manager 10252 Read-only Kubelet API 10255 默认的端口范围:https://kubernetes.io/docs/concepts/services-networking/service/ kubectl查看node和pod的状态kubectl get nodes node节点安装和加入集群export KUBE_REPO_PREFIX=”registry.cn-hangzhou.aliyuncs.com/szss_k8s”export KUBE_ETCD_IMAGE=”registry.cn-hangzhou.aliyuncs.com/szss_k8s/etcd-amd64:3.0.17”kubeadm join –token 242b80.86d585ebd6358b08 172.16.120.151:6443 –skip-preflight-checks kubeadm join –token 0cb8d6.bc8e73f8d69d3110 10.168.1.111:6443 –discovery-token-ca-cert-hash sha256:96015707712d4b6a27c1d63ebfae0e0a22015c38f7c7b1c4983a93fae43d4060 node节点安装验证kubectl get nodes 部署Dashboard学习http://www.dockerinfo.net/4468.html在开启TLS的Kubernetes1.6集群上安装Dashboardhttps://www.kubernetes.org.cn/3238.html wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml kubectl create -f kubernetes-dashboard.yml kubectl get secret –all-namespaces | grep kubernetes-dashboard-adminkubectl describe secret kubernetes-dashboard-admin-token-xxnml -n kube-system https://10.168.1.111:32270/#!/overview?namespace=defaulthttps://10.168.1.111:30865/#!/overview?namespace=default 10.168.1.111:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard kubectl proxy –address=’10.168.1.111’ –port=8086 –accept-hosts=’^*$’ http://10.168.1.111:8086/ui可以访问 参考https://blog.csdn.net/chenleiking/article/details/79316171https://www.cnblogs.com/zhenyuyaodidiao/p/6500897.htmlhttps://www.cnblogs.com/xinhaige/p/8595259.html kubectl apply -f kubernetes-dashboard.ymlkubectl apply -f kubernetes-dashboard-rbac-admin.yml kubectl delete deployment kubernetes-dashboard –namespace=kube-system kubectl delete svc kubernetes-dashboard –namespace=kube-system kubectl get svc –all-namespaces http://10.101.36.111:8055 错误Kubernetes failed to get imageFs info: unable to find data for container 一个简单的流程 kubectl create -f mysql-rc.yaml kubectl get rc kubectl get pods 查询原因kubectl describe pods mysql 参考每天5分支http://www.cnblogs.com/CloudMan6/tag/Docker/default.html?page=3 https://www.jianshu.com/p/33b3fa304797 https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/ https://github.com/EagleChen/kubernetes_init/blob/master/install.sh编译安装https://www.kubernetes.org.cn/3336.html https://www.kubernetes.org.cn/3336.html Kubernetes创建可外部访问的mysql容器https://www.58jb.com/html/133.html https://www.jianshu.com/p/21a39ee86311https://blog.csdn.net/zhuchuangang/article/details/76572157","categories":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"}],"keywords":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}]},{"title":"ubuntu中java环境的安装","slug":"018linux/ubuntu/ubuntu中java环境的安装","date":"2018-02-27T00:00:00.000Z","updated":"2019-02-18T01:50:00.902Z","comments":true,"path":"category/018linux/ubuntu/ubuntu中java环境的安装.html","link":"","permalink":"http://yoursite.com/category/018linux/ubuntu/ubuntu中java环境的安装.html","excerpt":"","text":"常用安装maven和Nexus安装maven安装 cd /mydate/temp wget https://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar zxvf apache-maven-3.3.9-bin.tar.gz mv apache-maven-3.3.9 /opt/ 修改环境变量 vi /etc/profile 将下面的内容追加到下面 export M2_HOME=/opt/apache-maven-3.3.9 export PATH=${M2_HOME}/bin:$PATH 编译测试 source /etc/profile mvn -v 安装Nexuswget http://download.sonatype.com/nexus/oss/nexus-2.12.0-01-bundle.tar.gztar zxvf nexus-2.12.0-01-bundle.tar.gz -C /opt/ cd /opt/nexus-2.12.0-01/binvi nexus修改RUN_AS_USER=root 修改nexus的工作目录/opt/nexus-2.12.0-01/conf修改nexus-work=/mydata/data/sonatype-work/nexus，修改目录权限sudo chmod -R 777 sonatype-work/nexus 启动./nexus start 测试访问地址：http://10.168.1.111:8081/nexus 默认登录用户名密码：admin/admin123 配置数据存储目录 http://blog.csdn.net/siyongkai/article/details/53783944 Gitlab安装获取curl -LJO https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/ubuntu/pool/xenial/main/g/gitlab-ce/gitlab-ce_10.0.6-ce.0_amd64.deb 安装sudo dpkg -i gitlab-ce_10.0.6-ce.0_amd64.deb 修改配置文件sudo vi /etc/gitlab/gitlab.rb 修改gitlab配置文件指定服务器ip和自定义端口 启动和重启GitLabsudo gitlab-ctl reconfiguresudo gitlab-ctl restart 汉化步骤git clone https://gitlab.com/xhang/gitlab.git cd gitlab git diff v10.0.6 v10.0.6-zh &gt; ../10.0.6.diff cd ../ gitlab-ctl stop patch -d /opt/gitlab/embedded/service/gitlab-rails -p1 &lt; 10.0.6.diff gitlab-ctl start gitlab-ctl reconfigure 参考https://www.cnblogs.com/gabin/p/6385908.html配置和汉化参考http://blog.csdn.net/love8753/article/details/75308652 Jenkins安装步骤添加Jenkins源 wget -q -O - http://pkg.jenkins-ci.org/debian-stable/jenkins-ci.org.key | sudo apt-key add - 创建源列表 sh -c &#39;echo deb http://pkg.jenkins-ci.org/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list&#39; 安装 sudo apt-get updatesudo apt-get install jenkins 启动和安装 service jenkins start 配置war /usr/share/jenkins/jenkins.war日志文件， /var/log/jenkins/jenkins.log, 遇到问题可以在这里查看问题所在配置参数，/etc/default/jenkins 为相关的各种配置参数，可以修改端口号，默认为8080 进行访问http://10.168.1.111:8090 第一次使用，需要解锁，/var/lib/jenkins/secrets/initialAdminPassword 查看 Jenkins 的运行状态sudo service jenkins status 服务管理 sudo service jenkins start sudo service jenkins restart sudo service jenkins stop 参考http://blog.csdn.net/dfhuang09/article/details/54730119","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"kubernetes安装","slug":"011容器技术/kubernetes安装","date":"2018-02-27T00:00:00.000Z","updated":"2018-12-10T15:21:23.904Z","comments":true,"path":"category/011容器技术/kubernetes安装.html","link":"","permalink":"http://yoursite.com/category/011容器技术/kubernetes安装.html","excerpt":"","text":"安装基本部署步骤1）minion节点安装docker 2）minion节点配置跨主机容器通信 3）master节点部署并启动etcd、kube-apiserver、kube-controller-manager和kube-scheduler组件 4）minion节点部署并启动kubelet、kube-proxy组件 minion节点配置跨主机容器通信由于docker自身还未支持跨主机容器通信，需要借助docker网络开源解决方案实现。这里利用OpenVSwich即开放式虚拟交换机实现容器跨主机通信。 什么是OpenVSwich？ OpenVSwich是一种开源软件，通过软件的方式实现二层交换机功能，专门管理多租赁云计算网络环境，提供虚拟网络中的访问策略、网络隔离、流量监控等。既然是虚拟交换机，自然与传统的物理交换机有着相同的特性，操作中可以按照理解物理交换机的方式去操作。 安装步骤安装openvswitch sudo apt-get install openvswitch-switch bridge-utils 添加网桥obr0 sudo ovs-vsctl add-br obr0 理解为添加了一个交换机 将gre0接口加入到网桥obr0， 远程IP写对端IP（创建一个GRE隧道并添加到网桥中） sudo ovs-vsctl add-port obr0 gre0 -- set Interface gre0 type=gre options:remote_ip=10.168.1.184 查看ovs信息 $ sudo ovs-vsctl show ———-分界线 5、添加docker网桥 $ sudo brctl addbr kbr0 将obr0网桥加入kbr0网桥，并启动 $ sudo brctl addif kbr0 obr0 //sudo ip link dev kbr0 up $ sudo ip link set dev kbr0 up 查看网桥信息 $ sudo brctl show 添加docker网桥配置信息 配置系统路由参数,防止kubeadm报路由警告echo &quot; net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 &quot; &gt;&gt; /etc/sysctl.conf sysctl -p ##加载镜像 for image in ls -l . |awk &#39;{print $9}&#39;;do echo “$image is loading”&amp;&amp;docker load &lt; ${image};done 导入镜像docker load &lt; /root/k8s_images/docker_images/etcd-amd64_v3.1.10.tardocker load &lt;/root/k8s_images/docker_images/flannel\\:v0.9.1-amd64.tardocker load &lt;/root/k8s_images/docker_images/k8s-dns-dnsmasq-nanny-amd64_v1.14.7.tardocker load &lt;/root/k8s_images/docker_images/k8s-dns-kube-dns-amd64_1.14.7.tardocker load &lt;/root/k8s_images/docker_images/k8s-dns-sidecar-amd64_1.14.7.tardocker load &lt;/root/k8s_images/docker_images/kube-apiserver-amd64_v1.9.0.tardocker load &lt;/root/k8s_images/docker_images/kube-controller-manager-amd64_v1.9.0.tardocker load &lt;/root/k8s_images/docker_images/kube-scheduler-amd64_v1.9.0.tardocker load &lt; /root/k8s_images/docker_images/kube-proxy-amd64_v1.9.0.tardocker load &lt;/root/k8s_images/docker_images/pause-amd64_3.0.tardocker load &lt; /root/k8s_images/docker_images/kubernetes-dashboard_v1.8.1.tar apt-get install -y socat ebtables cat kube_apt_key.gpg | apt-key add - echo “deb [arch=amd64] https://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main” &gt;&gt; /etc/apt/sources.list apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl 安装etcd apt install etcd systemctl start etcd 检测 etcdctl cluster-health http://localhost:2379 配置k8s集群重启kubelet并设置开机启动(Master Node都需要配置) systemctl enable kubelet &amp;&amp; sudo systemctl start kubeletsystemctl status kubelet #参考网站 # https://segmentfault.com/a/1190000013903445 参考ubuntu 搭建Kubenetes1.9.2 集群https://www.jianshu.com/p/21a39ee86311 https://my.oschina.net/andylo25/blog/1618342?from=timeline&amp;isappinstalled=0 kubernetes_init.githttps://www.jianshu.com/p/0e54aa7a20cf DockerCloud利用GitHub构建镜像http://blog.csdn.net/zgkpy/article/details/79181326 最有用http://www.mamicode.com/info-detail-1186209.html http://fishcried.com/2016-02-09/openvswitch-ops-guide/","categories":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"}],"keywords":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}]},{"title":"使用容器构建dubbo服务","slug":"011容器技术/使用容器构建dubbo服务","date":"2018-02-27T00:00:00.000Z","updated":"2018-12-10T16:08:19.456Z","comments":true,"path":"category/011容器技术/使用容器构建dubbo服务.html","link":"","permalink":"http://yoursite.com/category/011容器技术/使用容器构建dubbo服务.html","excerpt":"","text":"背景安装步骤假设运行docker的机器为10.168.1.111 创建Tomcat容器docker pull chaimm/tomcat:1.1docker run --name gaoxi-user-1 -p 8082:8080 -v /mydata/temp/tomcat/log:/opt/tomcat/gaoxi-log chaimm/tomcat:1.1 –name：指定容器的名字 -p：指定容器的端口映射 -p 8082:8080 表示将容器的8080端口映射到宿主机的8082端口上 -v：指定容器数据卷的映射 xxx:yyy 表示将容器yyy目录映射到宿主机的xxx目录上，从而访问宿主机的xxx目录就相当于访问容器的yyy目录。 chaimm/tomcat:1.1：表示容器所对应的镜像。 测试通过http://10.168.1.111:8082 访问到gaoxi-user-1容器的tomcat","categories":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"}],"keywords":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}]},{"title":"Docker常用操作","slug":"011容器技术/0docker常用操作","date":"2018-02-27T00:00:00.000Z","updated":"2019-02-20T10:21:57.413Z","comments":true,"path":"category/011容器技术/0docker常用操作.html","link":"","permalink":"http://yoursite.com/category/011容器技术/0docker常用操作.html","excerpt":"","text":"docker概念三个基本概念（镜像／容器／仓库） 服务启用停用sudo service docker startsudo service docker stop Docker初步使用 docker run -i -t --name mytest ubuntu:latest /bin/bash i表示使用交互模式t表示分配一个伪终端docker run = docker create + docker start其他参数-d参数，容器启动后进入后台。 镜像搜索docker search maven下载到本地 docker pull stephenreed/jenkins-java8-maven-git 容器重新进入如果容器以后台形式运行，可以使用下面命令进入，这样就可以看到容器的信息。docker exec -ti 243c32535da7 /bin/bash 查看容器的基本信息docker inspect 130a4565c27d 可以用 grep 或 –format 参数来过滤感兴趣的信息 终止容器docker stopdocker stop $(docker ps -a -q) 删除容器docker rm 命名删除处于终止状态的容器 删除所有停止的容器docker rm $(docker ps -a -q) ##数据卷共享 docker run -it –volumes-from dbdatab –name db1 ubuntu docker run –volumes-from dbdatab -ti -v $(pwd):/backup –name worker2 ubuntu 备份dbdata数据卷容器内的数据卷将本机的当前目录 挂载到 容器的根目录／backup docker run -d -P –name web -v /src/webapp:/opt/webapp training/webapp python app.py上面的命令加载主机的/src/webapp目录到容器的/opt/webapp目录： 使用文件构建docker build -t u-cents6 . 目录下需要干净，只有dock #参考网站 # docker1.12安装配置及使用笔记http://www.cnblogs.com/ilinuxer/p/6367004.html dock安装","categories":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"},{"name":"常用","slug":"常用","permalink":"http://yoursite.com/tags/常用/"}],"keywords":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}]},{"title":"软件架构漫谈","slug":"006系统设计/4+1视图","date":"2017-10-06T00:00:00.000Z","updated":"2018-12-10T17:01:41.964Z","comments":true,"path":"category/006系统设计/4+1视图.html","link":"","permalink":"http://yoursite.com/category/006系统设计/4+1视图.html","excerpt":"","text":"架构分类业务架构,功能架构,（系统架构／技术架构），应用架构 业务（逻辑）架构使用一套方法论对产品（项目）所涉及到的需求的业务进行业务边界划分，简单的讲就是根据一套逻辑思路进行业务的拆分，总体原则是对业务进行业务边界的划分，比如做一个企业订购服务网站，你需要把商品类目、商品、订单、订单服务、支付、退款很清晰的划分出来，而业务架构不需要考虑诸如我用什么技术开发、我的并发大怎么办、我选择什么样的硬件等等。 应用架构应用是介于业务语言与技术语言之间，是对整个系统实现的总体上的架构，他需要指出系统的层次、系统开发的原则、系统各个层次的应用服务，例如，上述系统中可以分为、数据层（资源层）、数据服务层、中间构建服务层、业务逻辑层、表现层，并写明每个层次应用服务。 数据架构对存储数据（资源）的架构方法论，其架构原则同应用架构大同小异，即考虑到各个系统应用场景、不同时间段的应用场景对数据进行诸如数据异构、读写分离、数据库或NOSQL的策略、缓存的使用、分布式数据（数据库）策略等等。 技术架构对上述架构中提出的功能（或服务）进行技术方案的实现。包括软件系统实现、操作系统选择、运行时设计。 架构与架构视图的关系软件架构视图什么是软件架构视图呢？Philippe Kruchten在其著作《Rational统一过程引论》中写道：一个架构视图是对于从某一视角或某一点上看到的系统所做的简化描述，描述中涵盖了系统的某一特定方面，而省略了于此方面无关的实体。 架构需要描叙2个方面的内容，业务架构和技术架构。 对应需求，可以分成，功能需求和非功能需求。功能需求分为非功能需求包含质量属性，约束。没有完美的架构，因为任何项目都有约束。其中质量属性又包含运行期质量属性和开发期质量属性。运行期质量属性，常见如高性能，易用性等。开发期质量属性，常见如易理解性，模块间松耦合，易测试性等。非功能需求偏技术一些。现代的架构还需要考虑未来的扩展性的问题，这是一种艺术。 4+1视图“4+1”视图最早由Philippe Kruchten提出，他在1995年的《IEEE Software》上发表了题为《The 4+1 View Model of Architecture》的论文，引起了业界的极大关注，并最终被RUP采纳，现在已成为架构设计的结构标准。 如下图所示。 场景视图－用户需求也叫用例视图，描述用户的业务场景，从用户的角度识别出业务需求，它是架构设计的起点和终点。 具体可以描叙的参考内容：业务流程图，用例图，每个用例的活动图。 逻辑视图－设计满足功能需求的架构逻辑视图关注功能，不仅包括用户可见的功能，还包括为实现用户功能而必须提供的”辅助功能模块”。 如果设计采用面向对象的方法，逻辑视图就是对象模型。逻辑视图重点在于功能，功能包括可见的业务功能，也包括不可见的系统功能（如日志、权限、事务等）。同时更重要的是确立逻辑分层、模块划分和模块之间的依赖关系。 具体可以描叙的参考内容：模块划分与依赖关系（逻辑结构图），业务实体，领域模型（类图）。 开发视图－设计满足开发期质量属性的架构侧重描述开发期质量属性，软件在开发环境下的静态组织。开发视图关注程序包，应用的统一框架，引用的类库、SDK和中间件，以及工程和包的划分规则等，规范、约束开发环境的结构。 具体可以描叙的参考内容：开发环境，技术框架，分层策略，目录结构 处理视图－设计满足运行期质量属性的架构描述系统的并发和同步方面的设计，侧重运行期质量属性。处理视图关注进程、线程、对象等运行时概念，以及相关的并发、同步、通信等问题。如果系统不需要考虑这些方面，本视图可以省略。 VS开发视图：开发视图一般偏重程序包在编译时期的静态依赖关系，而这些程序运行起来之后会表现为对象、线程、进程，处理视图比较关注的正是这些运行时单元的交互问题。 物理视图－部署相关的架构决策也叫部署视图，描述软件如何映射到硬件，反映系统在分布/部署上的设计。 软件最终要驻留、安装或部署到硬件才能运行，物理视图关注”目标程序及其依赖的运行库和系统软件”最终如何安装或部署到物理机器，以及如何部署机器和网络来配合软件系统的可靠性、可伸缩性等要求。 问题类／库／服务／应用领域模型在实现时可大可小，在业务的早期，在系统比较小的情况下，它有可能是一个类。当系统做大了以后，它可能是个 DLL 库。再做更大一点的时候，它可能是一个服务，给不同的应用去调用。每一个方法都有成为服务的潜质，特别是在系统中后期。领域模型是业务逻辑代码的施工图纸，它不仅有利于对现在系统业务逻辑的了解，同时也指导未来的架构改造。另外一个话题，架构演化。 为什么需要进行架构验证？将风险提前，为了有效的识别风险。可以将代价降到最低。只有系统做一次，才不需要进行验证。风险应该贯彻到架构的每个阶段。 为什么需要架构演化约束在进行变化，产生的结果也进行变化。业务的发展推生架构需要进行相应的调整。 http://www.cnitpm.com/pm/21856.html 参考http://www.cnblogs.com/JCSU/articles/2803598.html","categories":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}],"tags":[],"keywords":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}]},{"title":"Drools初步学习","slug":"010基础技术/规则引擎/drools初步学习","date":"2017-08-30T00:00:00.000Z","updated":"2018-12-10T14:33:32.222Z","comments":true,"path":"category/010基础技术/规则引擎/drools初步学习.html","link":"","permalink":"http://yoursite.com/category/010基础技术/规则引擎/drools初步学习.html","excerpt":"","text":"简介何为规则引擎规则引擎（rule engine）是指将复杂的业务逻辑抽象成规则，然后使用特定的算法（比如Rete）对规则进行求值等操作。简单点说，规则引擎就是实现复杂业务逻辑的框架。 规则引擎由推理引擎发展而来，是一种嵌入在应用程序中的组件，实现了将业务决策从应用程序代码中分离出来，并使用预定义的语义模块编写业务决策。接受数据输入，解释业务规则，并根据业务规则做出业务决策,从而给编程带来了极大的方便。 引入业务规则技术的目的对系统的使用人员 把业务策略（规则）的创建、修改和维护的权利交给业务经理 提高业务灵活性 加强业务处理的透明度，业务规则可以被管理 减少对IT人员的依赖程度 避免将来升级的风险 对IT开发人员 简化系统架构，优化应用 提高系统的可维护性和维护成本 方便系统的整合 减少编写“硬代码”业务规则的成本和风险 Drools简介Drools是一款基于Java的开源规则引擎，以将复杂多变的规则从硬编码中解放出来，以规则脚本的形式存放在文件中，使得规则的变更不需要修正代码重启机器就可以立即在线上环境生效。 Drools推出了一套新的基于KIE（Knowledge Is Everything 知识就是一切）概念的API，其目的是将之前版本中对规则引擎繁琐的调用和加载过程加以简化。 Drools的基本工作过程Drools执行时传递进去数据，用于规则的检查，调用外部接口，同时还可能需要获取到规则执行完毕后得到的结果。在drools中，这个传递数据进去的对象，术语叫Fact对象。Fact对象是一个普通的java bean，规则中可以对当前对象进行任何的读写操作，调用该对象提供的方法。 规则：LHS（Left Hand Side）部分为条件分支逻辑。 在 LHS 当中，可以包含 0~n 个条件，如果 LHS 部分没空的话，那么引擎会自动添加一个eval(true)的条件。 RHS（Right Hand Side）为执行逻辑。在RHS当中可以使用LHS 部分当中定义的绑定变量名、设置的全局变量、或者是直接编写Java代码。 结果对象：规则处理完毕后的结果。需要支持自定义类型或者简单类型（Integer、Long、Float、Double、Short、String、Boolean等） ##第一个程序 ###建立一个maven工程，pom如下 &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.fir.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-moudles&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;drools-moudles&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- drools 规则引擎 版本 --&gt; &lt;drools.version&gt;6.4.0.Final&lt;/drools.version&gt; &lt;spring.version&gt;4.2.6.RELEASE&lt;/spring.version&gt; &lt;log4j2.version&gt;2.5&lt;/log4j2.version&gt; &lt;/properties&gt; &lt;!-- 依赖项定义 --&gt; &lt;dependencies&gt; &lt;!-- start drools --&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-core&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-compiler&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-decisiontables&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-workbench-models-guided-template&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-simulator&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jbpm&lt;/groupId&gt; &lt;artifactId&gt;jbpm-flow-builder&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-spring&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-ci&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-internal&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie&lt;/groupId&gt; &lt;artifactId&gt;kie-api&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-workbench-models-guided-dtable&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.drools&lt;/groupId&gt; &lt;artifactId&gt;drools-templates&lt;/artifactId&gt; &lt;version&gt;${drools.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- end drools --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;directory&gt; ${project.basedir}/src/main/resources &lt;/directory&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 编辑Kmodule.xml新建kmodule.xml文件放到src/main/resources/META-INF/文件夹下 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;kmodule xmlns=&quot;http://www.drools.org/xsd/kmodule&quot;&gt; &lt;kbase name=&quot;kbase1&quot; packages=&quot;rules.testwrod&quot;&gt; &lt;ksession name=&quot;session&quot;/&gt; &lt;/kbase&gt; &lt;/kmodule&gt; 编辑一个规则文件在src/main/resources/rules/testwrod/下新建一个Person.drl文件如下： package rules.testwrod import com.drools.test.Person rule test001 when $p:Person(name==&quot;张三&quot;,age==30); then $p.setName(&quot;李四&quot;); System.out.println(&quot;改完后的名字&quot;+$p.getName()); end 创建一个java文件package com.drools.test; /** * Created by ww on 17/8/29. */ import org.kie.api.KieServices; import org.kie.api.runtime.KieContainer; import org.kie.api.runtime.KieSession; import org.kie.api.runtime.rule.FactHandle; public class TestWrod{ public static void main(String[] args) { KieServices kss = KieServices.Factory.get(); //从classpath中读取kmodule，创建KieContainder容器 KieContainer kc = kss.getKieClasspathContainer(); //kieContainer根据kmodule.xml定义的ksession的名称找到KieSession的定义，然后创建一个KieSession的实例 KieSession ks =kc.newKieSession(&quot;session&quot;); //Fact对象 Person person=new Person(&quot;张三&quot;,30); FactHandle insert = ks.insert(person); //KieSession就是一个到规则引擎的链接，通过它就可以跟规则引擎通讯，并且发起执行规则的操作 int count = ks.fireAllRules(); //最后将kiesession连接关闭 System.out.println(&quot;总执行了&quot;+count+&quot;条规则&quot;); ks.dispose(); } } 运行java程序，查看结果改完后的名字李四 总执行了1条规则 表示执行成功。 Drools语法学习复杂一点的语法rule &quot;rule1&quot; when $customer:Customer(age&gt;20,gender==&apos;male&apos;) Order(customer==$customer,price&gt;1000) then .... end 第一个：pattern(模式) 有三个约束1、 对象 类型必须是 Cutomer；2、Cutomer 的 age 要大于 203、Cutomer 的 gender 要是 male第二个:pattern(模式) 有三个约束1、 对象类型必须是 Order;2、 Order 对应的 Cutomer 必须是前面的那个 Customer3、 当前这个 Order 的 price 要大于 1000 这两个 pattern 没有符号连接，在 Drools 当中在 pattern 中没有连接符号，那么就用 and 来作为默认连接，所以在该规则的 LHS 部分 中两个pattern(模式)只有都满足了才会返回 true。对于对象内部的多个约束可以采用&amp;&amp;（and）、||(or)和,(and)来连接。&amp;&amp;和,不能混用 规则的属性Salience优先级作用是用来设置规则执行的优先级， salience 属性的值是一个数字，数字越大执行优先级越高，同时它的值可以是一个负数。默认情况下，规则的 salience 默认值为 0 package rules.testwrod rule test001 salience 2 when then System.out.println(&quot;执行test001&quot;); end rule test002 salience 1 when then System.out.println(&quot;执行test002&quot;); end no-loop防止死循环date-effective日期比较小于等于该属性是用来控制规则只有在到达后才会触发 date-expires日期比较大于date-expires 的作用是用来设置规则的有效期 Enabled是否可用lock-on-active 规则只执行一次activation-group分组具有相同 activation-group 属性的规则中只要有一个会被执行，其它的规则都将不再执行。 agenda-group议程分组引擎在调用这些设置了 agenda-group 属性的规则的时候需要显示的指定某个 Agenda Group 得到 Focus（焦点），这样位于该 Agenda Group 当中的规则会才触发执行，否则将不执行。 auto-focus焦点分组实际应用当中 agenda-group 可以和 auto-focus属性一起使用，这样就不会在代码当中显示的为某个 Agenda Group 设置 setFocus 了。一旦将某个规则的 auto-focus 属性设置为 true，那么即使该规则设置了 agenda-group 属性，我们也不需要在代码当中显示的设置 Agenda Group 的setFocus了。 ruleflow-group规则流 在使用规则流的时候要用到 ruleflow-group属性，在规则流当中通过使用 ruleflow-group 属性的值，从而使用对应的规则，该属性会通过流程的走向确定要执行哪一条规则。 ##Drools函数的使用 函数是在你的规则源文件中放置语义代码的一种方法，相对于在普通的 java 类中的方法。在一条规则的推论（then）部分中调用动作，函数是最有用的，特别是如果反复使用的特殊动作。 package rule.testword function String hello(String name) { return &quot;Hello &quot;+name+&quot;!&quot;; } rule &quot;using a static function&quot; when eval( true ) then System.out.println( hello( &quot;Bob&quot; ) ); end Drool支持函数导入的应用，import function my.package.Foo.hello 函数不能直接用在when条件里，正确的应该eval(hello(&quot;张三functionTest&quot;)); ##元数据的定义 ##Drools宏函数 Drools当中，在RHS里面，提供了一些对当前 Working Memory 实现快速操作的宏函数或对象，比如insert/insertLogical、update/modify 和 retract 就可以实现对当前 Working Memory 中的 Fact 对象进行新增、修改或者是删除；如果您觉得还要使用 Drools 当中提供的其它方法，那么您还可以使用另一外宏对象 drools，通过该对象可以使用更多的操作当前 Working Memory 的方法；同时Drools还提供了一个名为 kcontext 的宏对象，使我们可以通过该对象直接访问当前 Working Memory 的 KnowledgeRuntime。 参考文章 jboss规则引擎KIE Drools 6.3.0 Final 教程(1) http://blog.csdn.net/column/details/16183.html?&amp;page=2","categories":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}],"tags":[{"name":"Drools","slug":"Drools","permalink":"http://yoursite.com/tags/Drools/"},{"name":"规则引擎","slug":"规则引擎","permalink":"http://yoursite.com/tags/规则引擎/"}],"keywords":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}]},{"title":"Ubuntu软件安装和配置","slug":"018linux/ubuntu/Ubuntu软件安装和配置","date":"2017-06-26T00:00:00.000Z","updated":"2019-03-06T03:04:45.790Z","comments":true,"path":"category/018linux/ubuntu/Ubuntu软件安装和配置.html","link":"","permalink":"http://yoursite.com/category/018linux/ubuntu/Ubuntu软件安装和配置.html","excerpt":"","text":"常用安装版本为Ubuntu16.04 设置SSH服务apt-get install openssh-server 确认安装SSH服务成功与否 ps -ef | grep sshd 看到sshd表明ssh-server启动;否则 /etc/init.d/ssh start 手动进行启动 ##SSH无密码登录设置 在本地机器上使用ssh-keygen产生公钥私钥对确保本机有SSH公钥（一般是文件~/.ssh/id_rsa.pub），如果没有的话，使用ssh-keygen命令生成一个 ssh-keygen -t rsa 用ssh-copy-id将公钥复制到远程机器中将本机的公钥拷贝到服务器的authorized_keys文件ssh-copy-id -i ~/.ssh/id_rsa.pub test@192.168.200.1 如果对方机器ssh端口不是22，可以使用如下命令ssh-copy-id -i ~/.ssh/id_rsa.pub &quot;-p 20022 test@192.168.200.1&quot; 如果没有ssh-copy,可以使用cat ~/.ssh/id_rsa.pub | ssh test@192.168.200.1 &quot;cat - &gt;&gt; ~/.ssh/authorized_keys&quot; 登录ssh test@192.168.200.1 安装unrarsudo apt install unrar 安装sz/rz进入到su环境 cd /tmp wget http://www.ohse.de/uwe/releases/lrzsz-0.12.20.tar.gz tar zxvf lrzsz-0.12.20.tar.gz &amp;&amp; cd lrzsz-0.12.20 ./configure &amp;&amp; make &amp;&amp; make install 默认把lsz和lrz安装到了/usr/local/bin/目录下，现在我们并不能直接使用，下面创建软链接，并命名为rz/sz： cd /usr/bin ln -s /usr/local/bin/lrz rz ln -s /usr/local/bin/lsz sz rz命令本地上传文件到服务器sz命令发送文件到本地 打开SecureCRT软件 -&gt; Options -&gt; session options -&gt; X/Y/Zmodem 下可以设置上传和下载的目录。 安装git和svnsudo apt-get install gitsudo apt-get install subversion 安装flashpluginsudo apt-get install flashplugin-installer 安装Chrome到 https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb 下载最新的安装文件。 然后 代码如下: sudo apt-get install libappindicator1 libindicator7 sudo dpkg -i google-chrome-stable_current_amd64.deb sudo apt-get -f install 安装中文man手册sudo apt-get install manpages-zh 查看man 手册安装到哪里dpkg -L manpages-zh | less 常用配置用户配置添加用户组addgroup admin 添加一个新用户useradd -d /home/test -s /bin/bash -m test 参数m表示如果该目录不存在，则创建该目录 设置密码passwd test 将新用户test添加到用户组adminusermod -a -G admin test 为新用户设定sudo权限vi /etc/sudoers找到root ALL=(ALL:ALL) ALL，下面在加入一行test ALL=(ALL) NOPASSWD: ALL NOPASSWD表示，切换sudo的时候，不需要输入密码 sudo不用输入密码的方法确保用户属于sudo组sudo vi /etc/sudoers确保下面的没有被“＃”注释%sudo ALL=(ALL:ALL) ALL添加下面新命令到文件最下面%sudo ALL=NOPASSWD:ALLwq! 保存修改 允许su到rootUbuntu 安装后，root用户默认是被锁定了的，不允许登录，也不允许“su”到 root。允许 su 到 root非常简单，设置密码就可以了 `sudo passwd` 更换国内软件源sudo vi /etc/apt/sources.list 在文件开头添加下面的网易的软件源 deb http://mirrors.163.com/ubuntu/ precise-updates main restricted deb-src http://mirrors.163.com/ubuntu/ precise-updates main restricted deb http://mirrors.163.com/ubuntu/ precise universe deb-src http://mirrors.163.com/ubuntu/ precise universe deb http://mirrors.163.com/ubuntu/ precise-updates universe deb-src http://mirrors.163.com/ubuntu/ precise-updates universe deb http://mirrors.163.com/ubuntu/ precise multiverse deb-src http://mirrors.163.com/ubuntu/ precise multiverse deb http://mirrors.163.com/ubuntu/ precise-updates multiverse deb-src http://mirrors.163.com/ubuntu/ precise-updates multiverse deb http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse deb-src http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse 或者 # deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial universe deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties deb http://archive.canonical.com/ubuntu xenial partner deb-src http://archive.canonical.com/ubuntu xenial partner deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 更新软件源：sudo apt-get update 如果出现错误E: Problem executing scripts APT::Update::Post-Invoke-Success，可以先执行如下命令 sudo apt-get remove libappstream3","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"rsyslog学习","slug":"018linux/ubuntu/rsyslog使用","date":"2017-03-26T00:00:00.000Z","updated":"2018-05-27T09:06:00.058Z","comments":true,"path":"category/018linux/ubuntu/rsyslog使用.html","link":"","permalink":"http://yoursite.com/category/018linux/ubuntu/rsyslog使用.html","excerpt":"","text":"rsyslog学习简介最新的操作系统大都已经采用了rsyslog取代syslog，作为新的系统日志程序，rsyslog支持将日志存储到远端的日志服务器。 1.支持多线程2.支持TCP,SSL,TLS,RELP等协议3.支持将日志写入MySQL, PGSQL, Oracle等多种关系型数据中4.拥有强大的过滤器，可实现过滤系统信息中的任意部分5.可以自定义日志输出格式6.适用于企业级的日志记录需求 rsyslog日志服务的配置文件Rsyslog的配置文件为/etc/rsyslog.conf,其中共分为三块内容: 定义模块相关信息$MmodLoad imuxsock #用于加载模块 $ModLoad imuxsock 加载socket的日志$ModLoad imklog 加载log的日志$ModLoad imudp 加载远程日志服务器的UDP协议….. 定义全局的配置信息$IncludeConfig /etc/rsyslog.d/*.conf#用于加载其他的配置文件,实现分段管理 定义日志记录规则*.info;mail.none;authpriv.none;cron.none/var/log/messages #定义各类型日志存放位置 rsyslog规则定义格式facility.priority Targetauth #pam产生的日志，认证日志 facility(设施)auth –pam产生的日志authpriv –ssh,ftp等登录信息的验证信息cron –时间任务相关kern –内核lpr –打印mail –邮件mark(syslog)–rsyslog服务内部的信息,时间标识news –新闻组user –用户程序产生的相关信息uucp –unix to unix copy, unix主机之间相关的通讯local 1~7 –自定义的日志设备 可以使用以下通配符:*:所有设施f1,f2,f3…..:列表!:取反 priority(级别)debug –有调式信息的，日志信息最多info –一般信息的日志，最常用notice –最具有重要性的普通条件的信息warning –警告级别err –错误级别，阻止某个功能或者模块不能正常工作的信息crit –严重级别，阻止整个系统或者整个软件不能正常工作的信息alert –需要立刻修改的信息emerg –内核崩溃等严重信息none –什么都不记录从上到下，级别从低到高，记录的信息越来越少详细的可以查看手册: man 3 syslog Target(目标)/path/to/file#路径*#打印到已登录的用户界面@ServerIP#发送到日志服务器|COMMAND#发送到指定目录进行处理 ##配置应用例子 :: *.info;mail.none;authpriv.none;cron.none /var/log/messages authpriv.* /var/log/secure mail.* /var/log/maillog cron.* /var/log/cron *.emerg * uucp,news.crit /var/log/spooler local7.* /var/log/boot.log 例子记录到普通文件或设备文件::. /var/log/file.log # 绝对路径. /dev/pts/0测试: logger -p local3.info ‘KadeFor is testing the rsyslog and logger ‘ logger 命令用于产生日志 转发到远程::. @192.168.0.1 # 使用UDP协议转发到192.168.0.1的514(默认)端口. @@192.168.0.1:10514 # 使用TCP协议转发到192.168.0.1的10514(默认)端口 发送给用户(需要在线才能收到)::. root. root,kadefor,up01 # 使用,号分隔多个用户. # 号表示所有在线用户 忽略,丢弃::local3.* ~ # 忽略所有local3类型的所有级别的日志 执行脚本::local3.* ^/tmp/a.sh # ^号后跟可执行脚本或程序的绝对路径 安装/etc/rsyslog.conf rsyslog日志的主配置文件/etc/rsyslog.d/*.conf rsyslog日志的辅助配置文件/var/log/ 日志文件的目录 参考http://www.cnblogs.com/zengkefu/p/5606875.htmlhttp://dddbk.blog.51cto.com/6837943/1542807","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"开发方法论总结","slug":"006系统设计/我的开发世界观","date":"2017-02-07T00:00:00.000Z","updated":"2018-12-10T16:58:12.486Z","comments":true,"path":"category/006系统设计/我的开发世界观.html","link":"","permalink":"http://yoursite.com/category/006系统设计/我的开发世界观.html","excerpt":"","text":"引言当编程开发的年限到了羞于向外人道的时候，这期间出现了很多编程思维和方法论。将自己选择和应该遵循的记录下来。 职业观－成为专家我们并不是因为专业技术成为专家，而是因为“向上帝发誓，以此为职业”——把顾客作为出发点而成为专家。语出 大前研一《专业精神》 产品观-工匠精神把自己的手艺当成是与这个世界呼吸吐纳的入口，不论这手艺是什么。从60%提高到99%，和从99%提高到99.99%是一个概念。－－工匠精神 架构设计架构师，首要的是理清楚需求。游离与技术和业务之间。业务架构需要考虑的内容：主要两点业务流程与 业务规则。弄清楚业务流程，要搞清楚它的未来，过去，以及现在? 哪些是人工完成，哪些是系统完成？如果是人工的，可能就是本次项目需要完成的，即项目范围。确定项目范围后，就需要进一步了解相关的业务规则。 除了功能性需求，还需要考虑非功能性需求（性能／扩展型）根据需求，进行技术设计，采取基于风险的架构设计。参考《恰如其分的软件架构》 如果是自己公司的项目，考虑到成本和未来的扩展性。可以采用也可扩展性作为架构设计的核心。如果有足够的话语权，成为大架构师，将人员的扩展和公司的业务的扩张作为架构设计的元素。 项目管理项目就是在已经确定好的时间内必须解决的问题。可以采取基于风险的驱动的按时交付或者基于成本的按时交付。 在任何情况下都应该明白，组织是人的集合，过程是人在处理。如果人的因素出现问题，那么过程就可能出现问题。 项目管理的三重约束——质量、时间和成本，换个说法，就是好、快、省。通常的说法是：好、快、省中只能选两个。就是说，三者之中你只能决定两个，第三个是个变量。还有一个约束是范围与规模。前面三个决定范围的大小。如果需求范围能够确定下来，那就可以调节其他2个变量（一般降低性能质量是无法接受）。如果成本也确定了，那就只能要么缩减规模，要么延长时间。","categories":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}],"tags":[{"name":"世界观","slug":"世界观","permalink":"http://yoursite.com/tags/世界观/"}],"keywords":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}]},{"title":"Redis入门","slug":"010基础技术/redis/redis入门","date":"2017-02-04T00:00:00.000Z","updated":"2018-12-10T14:23:54.355Z","comments":true,"path":"category/010基础技术/redis/redis入门.html","link":"","permalink":"http://yoursite.com/category/010基础技术/redis/redis入门.html","excerpt":"","text":"Redis学习简介Redis是一种基于键值对（key-value）的NoSQL数据库，与很多键值对数据库不同的是，Redis中的值可以是由string（字符串）、hash（哈希）、list（列表）、set（集合）、zset（有序集合）、Bitmaps（位图）、HyperLogLog、GEO（地理信息定位）等多种数据结构和算法组成。 常用命令键总数dbsize 检查键是否存在exists not_exist_key如果键存在则返回1，不存在则返回0 删除键del a b c 键过期expire hello 10为键hello设置了10秒过期时间 查看键的剩余过期ttl命令会返回键的剩余过期时间，它有3种返回值：·大于等于0的整数：键剩余的过期时间。·-1：键没设置过期时间。·-2：键不存在 ttl hello 查询内部编码object encoding mylist 查看键的数据结构类型type mylist 设置值set key value [ex seconds] [px milliseconds] [nx|xx]例如SET abc abcvalue EX 60 NX ex seconds：为键设置秒级过期时间。px milliseconds：为键设置毫秒级过期时间。nx：键必须不存在，才可以设置成功，用于添加。xx：与nx相反，键必须存在，才可以设置成功，用于更新 setnxsetnx hello redis如果有多个客户端同时执行setnx key value，根据setnx的特性只有一个客户端能设置成功，setnx可以作为分布式锁的一种实现方案 获取值get hello 批量设置值和获取值mset key value mset a 1 b 2 c 3 d 4 mget a b c d 计数incr key incr命令用于对值做自增操作，返回结果分为三种情况：·值不是整数，返回错误。·值是整数，返回自增后的结果。·键不存在，按照值为0自增，返回结果为1。 除了incr命令，Redis提供了decr（自减）、incrby（自增指定数字）、decrby（自减指定数字）、incrbyfloat（自增浮点数） 数据结构哈希设置值hset user:1 name abc 获取值hget user:1 name 删除fieldhdel user:1 name 批量设置和获取hmset user:1 name mike age 12 city tianjin hmget user:1 name age city 获取所有fieldhkeys user:1 列表在Redis中，可以对列表两端插入（push）和弹出（pop），还可以获取指定范围的元素列表、获取指定索引下标的元素等。 列表是一种比较灵活的数据结构，它可以充当栈和队列的角色，在实际开发上有很多应用场景。 获取列表指定索引下标的元素lindex listkey -1 索引下标有两个特点：第一，索引下标从左到右分别是0到N-1，但是从右到左分别是-1到-N 获取指定范围内的元素列表从左到右获取列表元素lrange listkey 1 3 从右向左插入元素rpush listkey c b a 从列表左边开始删除4个为a的元素lrem listkey 4 a 典型使用场景缓存计数许多应用都会使用Redis作为计数的基础工具，它可以实现快速计数、查询缓存的功能，同时数据可以异步落地到其他数据源。 排行榜系统Redis提供了列表和有序集合数据结构，合理地使用这些数据结构可以很方便地构建各种排行榜系统。 ###分布式锁 ###一分钟不能超过5次 SET key value EX 60 NX #参考 # 一个微服务的编排器","categories":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}]},{"title":"异步理论学习","slug":"022java/thread/异步学习","date":"2017-02-01T00:00:00.000Z","updated":"2019-02-20T07:44:16.876Z","comments":true,"path":"category/022java/thread/异步学习.html","link":"","permalink":"http://yoursite.com/category/022java/thread/异步学习.html","excerpt":"","text":"概念介绍阻塞/非阻塞阻塞调用是指调用结果返回之前，当前线程会被挂起。vs同步，对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回而已。 非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，不会阻塞当前线程，而会立刻返回。 同步/异步同步调用：一种阻塞式调用，调用方要等待对方执行完毕才返回。异步调用：一种类似消息或事件的机制，不过它的调用方向刚好相反，接口的服务在收到某种讯息或发生某种事件时，会主动通知调用方。 异步执行完成如何通知调用方？回调/future/注册一个监听器如果结果可以保存在一个公用的地方，可以采取，调用方主动轮询。 异步IO高性能的标志:事件驱动、异步非阻塞。 例如：Nginx。 异步IO：我们都知道I/O和cpu计算是可以并行， IO是昂贵的。但我们的语言开发却是同步的IO。异步在操作系统层面是支持的。多线程通过信号量进行协助，消息等方式。事件模型是异步的保证。 UNIX提供的几种IO/模型1.阻塞I/O模型2.非阻塞I/O模型（read轮询)3.I/O复用模型(文件描述符select/poll—事件通知epoll)4.信号驱动I/O模型(内核信号驱动开始I/O)5.异步I/O模型(内核通知已经完成)-AIO 任何技术都不是完美的，非阻塞是需要轮询的,AIO只有linux有，不能利用系统缓存NodeJs利用线程池实现AIO, Libuv自行实现线程池完成异步io 同步和异步的转换利用java中future接口异步转为同步import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future; public class TestFuture { public static void main(String args[]) { System.out.println(&quot;start&quot;); ExecutorService es = Executors.newCachedThreadPool(); Future&lt;String&gt; fu = es.submit(new Callable() { @Override public String call() throws Exception { mockExecute(); return &quot;hello world&quot;; } }); System.out.println(&quot;hello world1&quot;); String result = &quot;&quot;; try { System.out.println(&quot;hello world2&quot;); result = fu.get(); System.out.println(&quot;hello world3&quot;); System.out.println(&quot;打印结果:&quot; + result); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } es.shutdown(); // shutdown the executor } private static void mockExecute() throws InterruptedException { System.out.println(&quot;hello 开始执行&quot;); Thread.sleep(10000); System.out.println(&quot;hello 执行完成&quot;); } } 异步io的异步转为同步其他参考网站http://www.cnblogs.com/caolei1108/p/6210614.html eventloop &amp; actor模式 &amp; Java线程模型演进 &amp; Netty线程模型 总结","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"线程池的学习","slug":"022java/thread/线程池的学习","date":"2017-02-01T00:00:00.000Z","updated":"2019-02-20T07:44:25.298Z","comments":true,"path":"category/022java/thread/线程池的学习.html","link":"","permalink":"http://yoursite.com/category/022java/thread/线程池的学习.html","excerpt":"","text":"线程池的学习Java提供了4钟线程池：newCachedThreadPoolnewFixedThreadPoolnewSingleThreadExecutornewScheduledThreadPool这四种线程池都直接或者间接获取的ThreadPoolExecutor实例，其构造函数如下 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize： 核心池的大小。 当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中maximumPoolSize： 线程池最大线程数，它表示在线程池中最多能创建多少个线程；keepAliveTime： 表示线程没有任务执行时最多保持多久时间会终止。unit： 参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性。workQueue： 一个阻塞队列，用来存储等待执行的任务。可以选择LinkedBlockingQueue，SynchronousQueue。threadFactory： 线程工厂，主要用来创建线程；handler： 表示当拒绝处理任务时的策略，有以下四种取值： ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：只要线程池不关闭，该策略直接在调用者线程中，运行当前被丢弃的任务 如果这4中策略不符合要求，最好自己定义拒绝策略，实现RejectedExecutionHandler接口 执行过程队列有界的情况1.初始的poolSize &lt; corePoolSize，提交的runnable任务，会直接做为new一个Thread的参数，立马执行 。2.当提交的任务数超过了corePoolSize，会将当前的runable提交到一个block queue中,。3.队列满了之后，如果poolSize &lt; maximumPoolsize时，会尝试new 一个Thread的进行救急处理，立马执行对应的runnable任务。4.如果3中也无法处理了，就会走到第四步执行reject操作。 其他参考网站 有界、无界队列对ThreadPoolExcutor执行的影响","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"阻塞队列","slug":"022java/thread/阻塞队列","date":"2017-02-01T00:00:00.000Z","updated":"2019-02-20T07:44:45.302Z","comments":true,"path":"category/022java/thread/阻塞队列.html","link":"","permalink":"http://yoursite.com/category/022java/thread/阻塞队列.html","excerpt":"","text":"概念介绍阻塞队列与普通队列的区别试图从空的阻塞队列中获取元素的线程将会被阻塞，直到其他的线程往空的队列插入新的元素。同样，试图往已满的阻塞队列中添加新元素的线程同样也会被阻塞，直到其他的线程使队列重新变得空闲起来。 阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 队列类型JDK7提供了7个阻塞队列。分别是 ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 LinkedBlockingQueue和ArrayBlockingQueue是FIFO队列。PriorityBlockingQueue是一个按优先级顺序排序的队列，当你不希望按照FIFO的属性处理元素时，这个PriorityBolckingQueue是非常有用的。SynchronousQueue，不会为队列元素维护任何存储空间。不过，它维护一个排队的线程清单，这些线程等待把元素加入（enqueue）队列或者移出（dequeue）队列。因为SynchronousQueue没有存储能力，所以除非另一个线程已经准备好参与移交工作，否则put和take会一直阻止。SynchronousQueue这类队列只有在消费者充足的时候比较合适，它们总能为下一个任务作好准备。 生产者-消费者阻塞队列支持生产者-消费者设计模式，只要符合生产者-消费者模型的都可以使用阻塞队列。 阻塞队列（Blocking queue）提供了可阻塞的put和take方法。阻塞队列简化了消费者的编码，因为take会保持阻塞直到可用数据出现。如果生产者不能足够快地产生工作，让消费者忙碌起来，那么消费者只能一直等待，直到有工作可做。同时，put方法的阻塞特性也大大地简化了生产者的编码；如果使用一个有界队列，那么当队列充满的时候，生产者就会阻塞，暂不能生成更多的工作，从而给消费者时间来赶进进度。 有界队列是强大的资源管理工具，用来建立可靠的应用程序：它们遏制那些可以产生过多工作量、具有威胁的活动，从而让你的程序在面对超负荷工作时更加健壮。 非阻塞算法简介一个线程的失败或挂起不应该影响其他线程的失败或挂起，这样的算法成为非阻塞（nonblocking）算法；基于锁的算法，如果线程在持有锁的时候因为阻塞I/O，页面错误，或其他原因发生延迟，很可能所有的线程都不能前进了。非阻塞算法没有使用锁，而是使用低层次的并发原语，比如比较交换，取代锁。 其他参考网站 阻塞队列与非阻塞队列 java自带线程池和队列详细讲解 有界、无界队列对ThreadPoolExcutor执行的影响","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"业务流程引论","slug":"006系统设计/业务流程引论","date":"2016-12-29T00:00:00.000Z","updated":"2018-12-10T17:02:41.565Z","comments":true,"path":"category/006系统设计/业务流程引论.html","link":"","permalink":"http://yoursite.com/category/006系统设计/业务流程引论.html","excerpt":"","text":"什么是业务流程? 业务流程简单理解即企业做事的过程，它具有准确的定义：有组织的活动，相互联系，为客户创造价值。 可以说任何企业的活动都是以业务为主线，以流程为线索串联起来的。企业的规章制度、业务操作手册等都与业务流程有着契合点。 流程的学术定义：迈克尔·哈默：业务流程是把一个或多个输入转化为对顾客有价值的输出的活动。 T·H·达文波特：业务流程是一系列结构化的可测量的活动集合，并为特定的市场或特定的顾客产生特定的输出。 ISO9000：业务流程是一组将输入转化为输出的相互关联或相互作用的活动。 业务流程对于企业的意义一家企业成功的基础，就是通过业务流程协调各种资源来达成企业目标。无论是向顾客交付产品，与合作伙伴协同，还是引导员工的努力，业务流程能够将企业的产品、品牌和价值有机地编织到一起，例如下面这些业务活动都是业务流程：根据生产所需，安排原材料的检验、入库和供应；回答客户的咨询；从供应商那里采购；向市场投放新产品。事实上，业务流程集成了企业内各种业务的特征，业务流程也因此成为企业运作特性的核心。 业务流程对于企业的意义不仅仅在于对企业关键业务的一种描述，更在于对企业的业务运营有着指导意义，这种意义体现在对资源的优化、对企业组织机构的优化以及对管理制度的一系列改变。这种优化的目的实际也是企业所追求的目标： 降低企业的运营成本，提高对市场需求的响应速度，争取企业利润的最大化。 战略和流程的关系我们究竟应该如何理解战略和流程的关系呢？让我们先来看看战略和战略管理的定义。 企业战略是是对企业整体性、长期性、基本性问题的计谋，其中包括竞争战略、营销战略、发展战略、品牌战略、融资战略、技术开发战略、人才开发战略、资源开发战略等等。 而战略管理定义为是企业确定其使命，根据组织外部环境和内部条件设定企业的战略目标，为保证目标的正确落实和实现进行谋划，并依靠企业内部能力将这种谋划和决策付诸实施，以及在实施过程中进行控制的一个动态管理过程。 因此，我们可以将战略管理分解成以下三个过程。 战略制定，确定企业任务，认定企业的外部机会与威胁，认定企业内部优势与弱点，建立长期目标，制定供选择战略，以及选择特定的实施战略。 战略实施，树立年度目标、制定政策、激励员工和配置资源，以便使制定的战略得以贯彻执行。 战略评价，重新审视外部与内部因素；度量业绩；采取纠偏措施。 导致战略执行不力，战略落地不实的原因是流程执行过程出现纰漏，尤其是主营业务流程运行不到位。因此，构建优秀的符合战略选择的业务流程是对战略最好的落地和执行！ 让战略落地的流程管理。 更有形象的比喻说，如果企业是一艘船，那企业的战略就是船头，业务流程是船身，战略引导企业到达企业目标，而业务流程则承载着企业的部门、岗位、绩效等，推进企业不断前进。 业务流程的定义要素“流程”的定义包括了这样六个要素：输入资源、活动、活动的相互作用（即结构）、输出结果、顾客、价值。 在流程的六要素中，有两个关键要素–客户和价值 “流程的客户是谁” “流程给客户创造的价值是什么” 业务流程管理（Business Process Management，BPM）定义业务流程管理是将生产流程、业务流程、各类行政申请流程、财务审批流程、人事处理流程、质量控制及客服流程等70%以上需要两人以上协作实施的任务全部或部分由计算机处理，并使其简单化、自动化的业务过程。 业务流程管理（Business Process Management, BPM）不是一个新概念，甚至不是一个新名词。它是从相关的业务流程变革领域，如业务流程改进（BPI）、业务流程重组（BPR）、业务流程革新中发展起来的。 Gartner Inc．给出的BPM的定义是：BPM是一个描述一组服务和工具的一般名词，这些服务和工具为显式的流程管理(如流程的分析、定义、执行、监视和管理)提供支持。 业务流程管理的需求与产生的背景20世纪90年代，Michael Hammer和James Champy的成名之作《公司再造》（Reengineering the Corporation）一书在全美公司领域引发了一股有关业务流程改进的汹涌浪潮。这两位管理学宗师在书中展示了这样一个观点——重新设计公司的流程、结构和文化能够带来绩效上的显著提高。但是由于缺少对变革管理以及员工变革主动性的关注，在很多致力于把他们的理论付诸实践的公司身上产生了反作用的结果。曾经的有关业务流程再造的金科玉律黯然失色，并且变得落时。今天，业务流程改造有了新名字——业务流程管理（BPM），而且再次进入了流行时段。受到全球竞争压力、消费品化以及政府监管的刺激，美国公司正在重新审视他们的业务流程，寻找到更高效的方法，通过自动化甚至外包的手段去实施它们。公司再次把业务流程管理——这种通过分析、建模和监控持续优化业务流程的实践，当作一种解决业务难题和帮助公司实现自己财务目标的系统方法。 业务分析实际这也是业务流程管理最重要的部分。它需要对企业业务有着强大的分析能力，因为业务分析对企业的运营有着重大的指导意义，只有具备了这样的业务分析能力，才能把握住企业运转的真实流程，而且这种分析能力往往带有对整个行业的深刻理解和前瞻性。业务分析在于人，与IT无关。 业务流程的持续改进不仅仅是流程管理人员（管理决策层）根据运行效益的分析和商业环境的分析对流程进行重整。还包括每个员工对其参与的流程的持续反馈和持续改进。 IT系统与业务流程的关系业务流程管理是一个很大的命题，IT系统通过信息化对它的子集进行支撑，办公自动化系统、事务处理系统和决策支持系统等都是常见的企业信息系统，但这些系统并没有加入流程的因素，只是用来帮助员工更好地完成某些特定的任务。工作流系统的出现使得整个流程的自动流转或自动执行成为可能，但是工作流一般只解决生产流程层的问题，与企业的计划和战略决策还存在一定的脱节。另外，随着企业业务流程向企业外部(供应商和客户)延伸，传统的工作流系统无力解决跨企业的流程集成问题。 业务流程管理的核心在于业务流程的分析解构和重整，这点是所有软件都不可企及的，关键在于人的参与。 业务流程重组简介企业再造 业务流程重组概念1990年，哈默在《哈佛商业评论》上发表了一篇名为《再造：不是自动化，而是重新开始》的文章，率先提出企业再造的思想.所谓“企业再造”，简单地说就是以工作流程为中心，重新设计企业的经营、管理及运作方式，在新的企业运行空间条件下，改造原来的工作流程，以使企业更适应未来的生存发展空间。它以一种再生的思想重新审视企业，并对传统的管理学赖以存在的基础——分工理论提出了质疑，是管理学发展史中的一次巨大变革。后来哈默教授与CSC Index的首席执行官JamesChampy于1993年发表了《公司重组：企业革命的宣言》。 此后，业务流程重组（Business Process Reengineering，简称BPR）作为一种新的管理思想，象一股风潮席卷了整个美国和其他工业化国家，并大有风靡世界之势。BPR被称作是“恢复美国竞争力的唯一途径”，并将“取代工业革命，使之进入重组革命的时代”。 根据Hammer与Champy的定义，BPR就是“对企业的业务流程Process）进行根本性（Fundamental）再思考和彻底性（Radical）再设计，从而获得在成本、质量、服务和速度等方面业绩的戏剧性的（Dramatic）改善”。 企业流程优化-ESIA流程是由一系列的活动组成的，而活动有增值活动与非增值活动之分。对于“增值”的判断，“重组”的提出者哈默博士也曾经提出过一个实用的原则—-客户愿意付费的就是增值的。 所有企业的最终目的都应该是为了提升顾客在价值链上的价值分配。重新设计新的流程以替代原有流程的根本目的，就是为了以一种新的结构方式为顾客提供这种价 值的增加，及其价值增加的程度。反映到具体的流程设计上，就是尽一切可能减少流程中非增值活动调整流程中的核心增值活动。其基本原则就是ESIA。ESIA法是减少流程中非增值活动以及调整流程的核心增值活动的实用原则。 清除——Eliminate删除无附加价值的步骤。 企业围绕价值链所进行的所有活动，有大概20%左右是增值活动，80%左右的是非增值活动中,而在非增值活动中的10%到20%是无效活动。 无效活动首先要予以清除。(协调与控制) 等待时间，由于上一个环节总是不到位，出现虚耗，这就是一种浪费，必须清除。 故障\\缺陷和失误，由于失误，在工作中产生了残次品，而每生产一个残次品就是对成本的提高，就是对资源的浪费，必须清除。 重复性劳动，单位中有时会存在因人设事或重复性的劳动，这些都是要坚决清除的。 ###简化——Simply###简化所有过于复杂的环节。 表格、程序、沟通渠道、问题区域 某些表格很繁琐，为了拿一只灯泡，要经过好几个部门签字，而这些部门还理直气壮地问，你怎么连表都不会填，像这些就需要简化。问题区域也需要简化。某个环节经常出现问题，称之为问题区域。问题区域往往是因工作水平问题或用人不当造成的，这是应该关注的一个重点。 ###整合——Integrate###集成功能 ，理顺流程过程。 活动。赋权一个人完成一系列简单活动，将活动进行整合，从而可以减少活动转交的发错率和缩短工作处理时间，实现流程与流程之间的“单点接触”。 团队。合并专家组成团队，形成“个案团队”或“责任团队”。这样使得物料、信息和文件旅行距离最短，改善同一流程上工作的人与人之间的沟通。 顾客（流程的下游）。面向客户，和客户建立完全的合作关系，整合客户组织和自身的关系，将自己的服务交送于顾客组织的流程。 供应商（流程的上游）。消除企业和供应商之间的一些不必要的官僚手续，建立信任和伙伴关系，整合双方的流程。###自动化——Automate###运用先进的信息技术加速流程转 ，提高流程运行质量。 将脏活、累活与乏味的工作，进行自动化。 数据的采集与传输。减少反复的数据采集，并降低单次采集的时间。 数据的分析。通过分析软件，对数据进行收集，整理与分析。加强对信息的利用率。 参考文章 http://wiki.mbalib.com/wiki/%E6%88%98%E7%95%A5%E6%B5%81%E7%A8%8Bhttp://www.chinaacc.com/new/234_237_201104/15hu327116737.shtmlhttp://baike.baidu.com/view/686289.htmhttp://blog.sina.com.cn/s/blog_81c734650101a9pk.htmlhttp://wiki.mbalib.com/wiki/%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%AE%A1%E7%90%86http://wiki.mbalib.com/wiki/%E5%8F%98%E9%9D%A9%E7%AE%A1%E7%90%86http://wiki.mbalib.com/wiki/ESIA","categories":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}],"tags":[{"name":"工作流","slug":"工作流","permalink":"http://yoursite.com/tags/工作流/"}],"keywords":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}]},{"title":"关于需求","slug":"050它山之玉/需求","date":"2016-12-29T00:00:00.000Z","updated":"2018-12-10T15:26:12.671Z","comments":true,"path":"category/050它山之玉/需求.html","link":"","permalink":"http://yoursite.com/category/050它山之玉/需求.html","excerpt":"","text":"背景本文摘抄于《需求 : 缔造伟大商业传奇的根本力量》 书中观点每个独一无二的需求故事都有着同样的起点：一个人、一个问题、一个点子。 真正的需求创造大师，把所有的时间和精力都投入到对“人”的了解上。他们创造出的产品令我们无法抗拒，更令竞争对手无法复制。 想要成为成功的需求创造者，就要把思维方式从劝说人们购说人们购买产品，升华到人与人之间的深入理解，升华到从客户的双眼和情感角度看世界。 因为麻烦，所以需求。 服务与我们实际购买的东西之间，总是存在着一道巨大的鸿沟。而这道鸿沟，就代表着创造新的机会。 需求，以解决顾客问题为中心，而不是满足需求为中心。 打开成功大门的钥匙，是“以用户问题为中心”的创新方案，而不是将目光锁定在设备性能本身。 明天的需求从哪里来？如果将同样的问题摆在需求创造者面前，他们不会将希望寄托于政府、世界500强。他们，只会望向镜中的自己…… 创造需求的6大关键 为产品赋予魔力 贏家并不是先行者，而是第一个能创造出情感共鸣并能把握住市场方向的人。 化解生活中的麻烦 构建完善的背景因素 寻找激发力 创造需求的最大障碍是人们的惰性、多疑、习惯和冷漠。大多数听说过某产品的人都会持观望态度，并没有购买意向，正是这种心态限制了需求的增长。只撬动杠杆，才能激发人们采取行动。就算是最优秀的公司，也常常要花上很多年才能找准激发力的命脉。但需求创造者们总会一刻不停地搜寻，持续不断地实验，以期尽快找到观望者转变为客户的关键力量。 去平均化 举例Zipcar的诞生可谓恰逢其时。在20世纪90 年代末，由于有了技术创新（包括基于互联网的通信、无线电话和智能卡）、经济发展（例如燃料成本的大幅震荡），以及社会趋势（如美国年轻人不断增长的环境意识），汽车共享这样一套简单便捷的系统才能应运而生。 将公司业务立足于客户需求的巨大鸿沟之上。这道鸿沟的一边是客户购买的汽车及其带来的费用和麻烦，另一边是客户真心想要的那种说走就走的自由感。","categories":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"},{"name":"互联网","slug":"互联网","permalink":"http://yoursite.com/tags/互联网/"}],"keywords":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}]},{"title":"看见未来读书摘要","slug":"050它山之玉/看见未来读书笔记","date":"2016-12-29T00:00:00.000Z","updated":"2018-12-10T15:26:05.263Z","comments":true,"path":"category/050它山之玉/看见未来读书笔记.html","link":"","permalink":"http://yoursite.com/category/050它山之玉/看见未来读书笔记.html","excerpt":"","text":"观点经济学的角度，生产要素 在凯文·凯利看来，信息乃是万物的根本凯文·凯利观点， 互联网的发展，更加契合生命进化的特点，呈现出去中心化的特征，没有中央控制，创新总在边缘发生。想起了一本书，《技术的本质》，里面也谈到了技术的进化。 沟通的重要性， 沟通是人类生活的核心 曼纽尔·卡斯特尔认为，沟通是人类最重要的活动，是个人生活、商业、教育、娱乐和一切的基础，也是人类生活的核心，正是沟通成就了人类。 沟通能够进行进行升华，当谈到某种深度时，发生思想的碰撞，就会产生新的思想，进而产生创新的发生。 互联网是一种好的沟通手段，让人链接的更为紧密。而且互联网能够和物的链接，产生进一步的想象。随着互联网互联沟通，像细胞的进化一样，产生新的生态。 社交，金融，只是这一过程中产生的某种生态。平衡然后在打破。 个体的贡献不仅限于其所能提供的价值，还包括人与人之间互动所产生的创造力。这种创造力就是生产力的源泉，而生产力则是财富的源泉。（软财富，包含人的创意等） 生产关系是什么？ 获得知识的途径？ 交流是一种获得知识的途径，而读书也是。都是通过互联其他的东西的形式，产生进一步的升华。读万卷书，行万里路，阅人无数。从计算机的角度来看，人与人的同步交流，而读书时是人与人的异步交流。思考是自己和自己的知识库，进行链接学习。武侠常有闭关之说。主动学习，深度学习？？ 实践应用观念要么表达出来，目的是有用。 ##分享的必要性？产生进一步的知识，沟通升华。可以让别人搜索，进行启迪。联网的目的，不是产生垃圾，而是形成智慧网络。 互联网为何能增加人们的幸福感 相互协作、相互分享，促进个人的知识、创造的价值能在这个网络中实现碰撞，实现多样化组合，进而大幅度提升知识的丰富程度，激发创新，创造更大、更新的价值。这才是网络精髓所在。 怎样使用互联网？服务网的路由器，还是数据价值产生器？人需要学会找知识。 新奇的观点：产品经理，是不是需要进行排列组合，然后产生新的价值？ 链接的例子：马克思主义，老子的哲学，中国纠结于是反对外来的东西，谈西方的管理和东方的管理。其实都是互相学习的结果。何为批判的继承？ 互联网的驱动力提高人机交互体验其实是互联网发展进程中非常关键的一条主线","categories":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"},{"name":"互联网","slug":"互联网","permalink":"http://yoursite.com/tags/互联网/"}],"keywords":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}]},{"title":"关于破坏试创新","slug":"050它山之玉/创新的窘境","date":"2016-12-29T00:00:00.000Z","updated":"2018-12-10T15:26:22.251Z","comments":true,"path":"category/050它山之玉/创新的窘境.html","link":"","permalink":"http://yoursite.com/category/050它山之玉/创新的窘境.html","excerpt":"","text":"背景本文摘抄于《创新的窘境》 书中观点一般来说，破坏性创新并不涉及特别复杂的技术变革，其主要表现形式就是将成品元件组装在一起，但相比之前的产品，产品结构通常会变得更加简单。 破坏性创新并不能为主流市场的客户提供更好的产品，因此这种创新首先发生在主流市场的可能性很小。相反，破坏性创新提供的是一种完全不同的产品组合，只有远离主流市场或对主流市场没有太大意义的新兴市场，客户才会重视这些产品组合的属性。 成熟企业一般善于改善业已成熟的技术，而新兴企业似乎更善于利用突破性新技术，原因通常是它们将已经研发和采用过的技术从一个行业引入另一个行业。 克拉克认为，企业一般是凭借经验或等级来构建某种产品（例如汽车）的技术能力。对于应该解决和应该避免的技术问题，企业的历史选择决定了它所积累的技能和知识的类型。当应对产品或流程执行问题的最佳解决方案需要企业具备与其积累的经验大相径庭的知识时，这家企业很有可能会遭遇挫折。 “密切关注你的客户”这句流行口号似乎并不总是一个经得起推敲的建议。相反，人们可能会认为，是客户引导了其供应商的延续性创新进程，但这句话在破坏性技术变革中并没有发挥引领作用——或者甚至是明显误导了供应商； 倘若成熟企业决定忽略无法解决当前客户需求的技术+，当两条本来井水不犯河水的轨线最终交汇时，这个决定将给它们造成致命的打击。 “成熟企业面临的最大障碍就是它们缺乏这么做的意愿。” 好的企业内许多受过良好培训的决策者，如何发挥他们在资源分配方面的作用——他们将把资金和人力资源分配到那些他们认为将给企业带来最大增长空间和利润率的项目上。即便一家企业首席执行官已下定决心，要让企业及早把握下一轮破坏性技术浪潮，并引导企业成功地设计出经济型新产品；但这家企业的员工却不认为一个规模仅为8000万美元的低端市场，能够解决一家收入达数十亿美元企业的增长和利润问题——特别是企业的竞争对手将会采取一切可能的措施，来抢占为他们带来这数十亿美元收入的客户资源。 而领先企业在这些市场上接到的第一笔订单也都是小订单，培育了这些新兴市场的企业所建立的成本结构，也必须能使企业以很小的规模实现贏利。 破坏性创新如何转化为商业用途 破坏性技术首先由成熟企业研制成功 市场销售随后收集公司主要客户的反馈 成熟企业加快对延续性技术的开发步伐 新企业已经出现，破坏性技术市场在反复尝试中逐渐成形 新兴企业向高端市场转移 成熟企业在维护客户基础方面棋慢一招 延伸一针顶破天的理论源创新和流创新的区别","categories":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"},{"name":"互联网","slug":"互联网","permalink":"http://yoursite.com/tags/互联网/"}],"keywords":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}]},{"title":"快速检查Linux服务器性能","slug":"018linux/0快速检查Linux服务器性能","date":"2016-12-15T00:00:00.000Z","updated":"2019-02-20T10:12:42.912Z","comments":true,"path":"category/018linux/0快速检查Linux服务器性能.html","link":"","permalink":"http://yoursite.com/category/018linux/0快速检查Linux服务器性能.html","excerpt":"","text":"通过执行以下命令，可以在1分钟内对系统资源使用情况有个大致的了解。 uptime 参考dmesg | tail显示了最新的10个系统信息 Linux dmesg命令用于显示开机信息。kernel会将开机信息存储在ring buffer中。您若是开机时来不及查看信息，可利用dmesg来查看。开机信息亦保存在/var/log目录中，名称为dmesg的文件里。 vmstat 1 参考 r：等待CPU的进程数。该指标能更好地判定CPU是否饱和，因为它不包括I/O。简单地说，r值高于CPU数时就意味着饱和。free：空闲的内存千字节数。如果你数不清有多少位，就说明系统内存是充足的。接下来要讲到的第7个命令，free -m，能够更清楚地说明空闲内存的状态。si，so：Swap-ins和Swap-outs。如果它们不为零，意味着内存已经不足，开始动用交换空间。us，sy，id，wa，st：它们是所有CPU的使用百分比。它们分别表示user time，system time（处于内核态的时间），idle，wait I/O和steal time（被其它租户，或者是租户自己的Xen隔离设备驱动域（isolated driver domain），所占用的时间）。相加us和sy的百分比，你可以确定CPU是否处于忙碌状态。一个持续不变的wait I/O意味着瓶颈在硬盘上，这种情况往往伴随着CPU的空闲。 mpstat -P ALL 1参考 检查CPU是否存在负载不均衡 pidstat 1参考 %CPU列是在各个CPU上的使用量的总和； iostat -xz 1参考free -m参考sar -n DEV 1参考sar -n TCP,ETCP 1参考top参考 Ctrl-s暂停，Ctrl-q继续 参考http://www.techug.com/linux-performance-analysis-first-60-second","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"常用","slug":"常用","permalink":"http://yoursite.com/tags/常用/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"strace使用指南","slug":"018linux/strace使用指南","date":"2016-12-11T00:00:00.000Z","updated":"2018-05-27T09:06:00.057Z","comments":true,"path":"category/018linux/strace使用指南.html","link":"","permalink":"http://yoursite.com/category/018linux/strace使用指南.html","excerpt":"","text":"基本使用strace -c lsstrace cat /dev/null 参数详解 参数 描叙 -c 统计每一系统调用的所执行的时间,次数和出错的次数等. -d 输出strace关于标准错误的调试信息. -f 跟踪由fork调用所产生的子进程. -ff 如果提供-o -F 尝试跟踪vfork调用.在-f时,vfork不被跟踪. -h 输出简要的帮助信息. -i 输出系统调用的入口指针. -q 禁止输出关于脱离的消息. -r 打印出相对时间关于,每一个系统调用. -t 在输出中的每一行前加上时间信息. -tt 在输出中的每一行前加上时间信息,微秒级. -ttt 微秒级输出,以秒了表示时间. -T 显示每一调用所耗的时间. -v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出. -V 输出strace的版本信息. -x 以十六进制形式输出非标准字符串 -xx 所有字符串以十六进制形式输出. -a column 设置返回值的输出位置.默认为40 -e expr 指定一个表达式,用来控制如何跟踪 -o filename 将strace的输出写入文件filename -p pid跟踪指定的进程pid. -s strsize指定输出的字符串的最大长度.默认为32.文件名一直全部输出. 举例strace -o output.txt -c -e trace=all -p 988输出如下1234567% time seconds usecs/call calls errors syscall------ ----------- ----------- --------- --------- ----------------100.00 0.004000 4000 1 restart_syscall 0.00 0.000000 0 1 accept 0.00 0.000000 0 2 futex------ ----------- ----------- --------- --------- ----------------100.00 0.004000 4 total time:显示系统cpu花在哪里的百分比usecs/call: 每次调用的次数平均系统cpu时间（毫秒）calls:系统调用次数 跟踪进程的所有系统调用（-e trace=all），-c 用于系统调用活动的统计总结。 e用来控制展示特定的系统调用,可以使用一下参数-e trace=open,close,rean,write表示只跟踪这四个系统调用-e trace=file只跟踪有关文件操作的系统调用.-e trace=process只跟踪有关进程控制的系统调用.-e trace=network跟踪与网络有关的所有系统调用.-e strace=signal跟踪所有与系统信号有关的 系统调用-e trace=ipc跟踪所有与进程通讯有关的系统调用-e trace=all 所有系统调用 关于系统调用的名称可以参考常用系统调用 strace -o output.txt -T -tt -e trace=all -p 988跟踪进程的所有系统调用，并统计系统调用的花费时间，以及开始时间（并以可视化的时分秒格式显示），最后将记录结果存在output.txt文件里面","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"perf使用指南","slug":"018linux/perf使用指南","date":"2016-12-11T00:00:00.000Z","updated":"2018-05-27T09:06:00.056Z","comments":true,"path":"category/018linux/perf使用指南.html","link":"","permalink":"http://yoursite.com/category/018linux/perf使用指南.html","excerpt":"","text":"基本使用安装 apt install linux-tools-commonapt install linux-tools-4.4.0-47-generic 测试 perf list显示可采样的事件。如果能够显示，表示安装成功。 perf list miss 进行模糊查询 perf的运行原理性能调优工具如 perf，Oprofile 等的基本原理都是对被监测对象进行采样，最简单的情形是根据 tick 中断进行采样，即在 tick 中断内触发采样点，在采样点里判断程序当时的上下文。假如一个程序 90% 的时间都花费在函数 foo() 上，那么 90% 的采样点都应该落在函数 foo() 的上下文中。 改变采样的触发条件使得我们可以获得不同的统计数据： 以时间点 ( 如 tick) 作为事件触发采样便可以获知程序运行时间的分布。 以 cache miss 事件触发采样便可以知道 cache miss 的分布，即 cache 失效经常发生在哪些程序代码中。如此等等。 perf 事件使用 perf list 命令可以列出所有能够触发 perf 采样点的事件。 事件分为以下三种：Hardware Event 是由 PMU 硬件产生的事件，比如 cache 命中，当您需要了解程序对硬件特性的使用情况时，便需要对这些事件进行采样；Software Event 是内核软件产生的事件，比如进程切换，tick 数等 ;Tracepoint event 是内核中的静态 tracepoint 所触发的事件，这些 tracepoint 用来判断程序运行期间内核的行为细节，比如 slab 分配器的分配次数等。 用法Perf statperf stat ls 显示如下： 1234567891011121314Performance counter stats for &apos;ls&apos;: 0.776039 task-clock (msec) # 0.697 CPUs utilized 0 context-switches # 0.000 K/sec 0 cpu-migrations # 0.000 K/sec 92 page-faults # 0.119 M/sec &lt;not supported&gt; cycles &lt;not supported&gt; stalled-cycles-frontend &lt;not supported&gt; stalled-cycles-backend &lt;not supported&gt; instructions &lt;not supported&gt; branches &lt;not supported&gt; branch-misses 0.001113814 seconds time elapsed 几个统计值的含义分别如下： task-clock (msec)：CPU利用率，该值越高，说明花费越多时间在CPU计算上而非其它（比如I/O）；执行时间为0.776039毫秒，而通过最后一行知道程序整个执行时间为0.001113814秒，所以百分比为0.776039/1.113814=0.697。 context-switches：记录程序在运行过程中发生的进程切换次数，应该避免频繁的进程切换；这里为0次。 CPU-migrations：记录程序在运行过程中发生的CPU迁移次数，即被调度器从一个CPU转移到另外一个 CPU上运行；这里为0次。 page-faults：记录程序在运行过程中发生的缺页中断次数。 cycles：记录执行程序所发费的处理器周期数。instructions：记录执行程序所发费的指令数。branches：记录程序在运行过程中的分支指令数。branch-misses：记录程序在运行过程中的分支预测失败次数。cache-references：记录程序在运行过程中的cache命中次数。cache-misses：记录程序在运行过程中的cache失效次数。 统计该应用程序运行期间究竟发生了多少次系统调用。在哪里发生的？perf stat -e raw_syscalls:sys_enter ls 对整个系统直接进行采样使用 perf stat 的时候，往往您已经有一个调优的目标。如果只是发现系统性能无端下降，并不清楚究竟哪个进程可以使用perf top 实时显示当前系统的性能统计信息。 通过-e指定关注的事件，比如如下查看造成context-switches最多的函数排行perf top -e context-switches 精确制导——定位程序瓶颈，使用 perf record查找时间上的热点函数 perf record -e raw_syscalls:sys_enter ls perf report perf record -e writeback:* ls perf report g按照调用关系进行显示的统计信息 e指定关注的事件。 perf record的其他参数： -f：强制覆盖产生的.data数据 -c：事件每发生count次采样一次 -p：指定进程 -t：指定线程 参考https://www.ibm.com/developerworks/cn/linux/l-cn-perf1/http://www.blogjava.net/qileilove/archive/2013/09/04/403646.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"linux中与网络相关的命令","slug":"018linux/常用网络命令汇总","date":"2016-12-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.059Z","comments":true,"path":"category/018linux/常用网络命令汇总.html","link":"","permalink":"http://yoursite.com/category/018linux/常用网络命令汇总.html","excerpt":"","text":"#linux网络命令汇总 ifconfigifconfig用于输出网络接口配置、调优和debug的各种选项。可以快捷地查看IP地址和其它网络接口的信息。键入ifconfig查看所有启用的网络接口的状态，包括它们的名字。可以指定网络接口的名字来只显示这一个接口的信息。 基本用法ifconfigifconfig eth0 1234567891011121314151617enp0s5 Link encap:Ethernet HWaddr 00:1c:42:4b:cf:a2 inet addr:10.5.20.17 Bcast:10.5.20.63 Mask:255.255.255.192 inet6 addr: fe80::3d36:c06:4ec7:1e97/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:555955 errors:0 dropped:267 overruns:0 frame:0 TX packets:157098 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:400812890 (400.8 MB) TX bytes:12261729 (12.2 MB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:3418 errors:0 dropped:0 overruns:0 frame:0 TX packets:3418 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:243353 (243.3 KB) TX bytes:243353 (243.3 KB) eth0 表示第一块网卡， 其中 HWaddr 表示网卡的物理地址， inet addr 用来表示网卡的IP地址，广播地址Bcast:10.5.20.63 掩码地址Mask:255.255.255.192 lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。 第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址）第二行：网卡的IP地址、子网、掩码第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节第四、五行：接收、发送数据包情况统计第七行：接收、发送数据字节数统计信息。 配置IP地址ifconfig eth0 192.168.1.10 netmask 255.255.255.0 upifconfig eth0 192.168.120.56ifconfig eth0 192.168.120.56 netmask 255.255.255.0 网卡绑定多IPifconfig eth0:1 192.168.1.99 netmask 255.255.255.0 traceroutetraceroute命令显示数据包到达目的主机所经过的路由 traceroute www.baidu.com route查看路由表route -n或者netstat -rn 添加默认的网关route add default gw 10.0.0.254相当有route add -net 0.0.0.0 netmask 0.0.0.0 gw 10.0.0.254 ##ping使用ping 命令来测试网络的连通性ping发送ECHO_REQUEST包到你指定的地址，使用 -c 开关，可以指定发送ECHO_REQUEST包的个数。ping 192.168.1.10ping -c 4 baidu.com ##arp 可以使用arp命令来配置并查看arp缓存 hosthost命令用来做DNS查询。如果命令参数是域名，命令会输出关联的IP；如果命令参数是IP，命令则输出关联的域名。 host www.baidu.com nslookupnslookup 用这个命令来显示主机名，可以找到给定域名的所有ip地址。 nslookup www.baidu.com会显示如下： 12345www.baidu.com canonical name = www.a.shifen.com.Name: www.a.shifen.comAddress: 115.239.211.112Name: www.a.shifen.comAddress: 115.239.210.27 hostnamehostname 没有选项，显示主机名字 hostname –d 显示机器所属域名hostname –f 显示完整的主机名和域名hostname –i 显示当前机器的ip地址 whoiswhois命令输出指定站点的whois记录，可以查看到更多如谁注册和持有这个站点这样的信息。域名人的联系方式也能查到 whois www.baidu.com 没有安装需要sudo apt install whois sar网络查看sar -n DEV 1 1 参考http://blog.csdn.net/u013281361/article/details/52021993http://www.cnblogs.com/lpfuture/p/5857738.htmlLinux ifconfig命令详解 http://blog.hehehehehe.cn/a/17571.htmhttp://www.2cto.com/os/201303/196496.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"如何预估系统容量?","slug":"006系统设计/系统容量预估","date":"2016-11-10T00:00:00.000Z","updated":"2018-12-10T17:02:27.634Z","comments":true,"path":"category/006系统设计/系统容量预估.html","link":"","permalink":"http://yoursite.com/category/006系统设计/系统容量预估.html","excerpt":"","text":"背景随着业务的快速成长，日访问量越来越高，除了对功能要求很高以外，对性能要求也越来越高。那问题也来了，怎么知道机器够不够用？如果不够，需要多少台满足要求？或者运营需要在双11做个促销，服务器能抗住么？如果扛不住，需要加多少台？网络带宽够不够用？这些都是系统容量预估需要回答的问题。 怎么进行预估？可以遵循如下步骤 预估网站流量（PV）网站流量是指网站的访问量，用来描述访问网站的用户数量以及用户所浏览的网页数量等指标，常用的统计指标包括网站的独立用户数量、总用户数量（含重复访问者）、网页浏览数量、每个用户的页面浏览数量、用户在网站的平均停留时间等。 网站访问量的常用衡量标准：独立访客(UV) 和 综合浏览量（PV）,一般以日为单位来衡量和计算。 PV： 一定时间范围内页面浏览量或点击量，用户每次刷新即被计算一次UV：指一定时间范围内相同访客多次访问网站，只计算为1个独立访客 预估平均QPS什么是TPS?Tps即每秒处理事务数，包括了用户请求服务器/服务器自己的内部处理/服务器返回给用户.这三个过程，每秒能够完成N个这三个过程，Tps也就是3； 什么QPSQps基本类似于Tps，但是不同的是，对于一个页面的一次访问，形成一个Tps；但一次页面请求，可能产生多次对服务器的请求，服务器对这些请求，就可计入“Qps”之中例如访问一个页面会请求服务器3次，产生3个QPS QPS = req/sec = 请求数/秒 计算平均QPS总请求数 = 总PV * 页面衍生连接数 平均QPS = 总请求数 / 总时间 比如：页面1小时内的总访问量是30w pv，衍生连接数为30 ,那么平均QPS的计算如下： 平均QPS ＝ (30w * 30) /(60*60) = 2500 预估峰值QPS一般情况，峰值QPS大概是均值QPS的3-5倍，日均QPS为1000，于是评估出峰值QPS为1000*5 ＝ 5000具体情况需要业务来定。 计算服务器的极限QPS通过压力测试，算出服务器的单机极限QPS。一般的压力测试软件都能得到这个值。考虑到寿命和性能，单机线上允许跑到QPS为（极限QPS＊0.8) 计算需要的机器需要的机器 = 峰值QPS / 单机极限 QPS假设峰值QPS是5000，单机极限QPS是1000，线上部署了3台服务器。理论需要5台。通个水平扩展，需要再加上2台可以满足要求。 ##怎么计算带宽？ 网站带宽= （PV / 统计时间（单位S））*平均页面大小（单位KB）* 8 字节的单位是Byte，而带宽的单位是bit，1Byte=8bit,所以转换为带宽的时候，要乘以 8 假设网站的平均日PV：10w 的访问量，页面平均大小0.4 M 。网站带宽 = （10w / （24 * 60 * 60））* 0.4M * 8 =3.7 Mbps 在实际的网站运行过程中，我们的网站必须要在峰值流量时保持正常的访问，假设，峰值流量是平均流量的5倍，按照这个计算，实际需要的带宽大约在 3.7 Mbps * 5=18.5 Mbps 。 通过压力测试QPS，得到网站支持的pv`平均QPS = （总PV * 页面衍生连接数） / 总时间 一台机器`每天总PV = （2000 24 60 * 60）/30 ＝ 57.6万","categories":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}],"tags":[],"keywords":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}]},{"title":"理解函数式编程/面向对象编程","slug":"006系统设计/函数VS面向对象","date":"2016-11-07T00:00:00.000Z","updated":"2018-12-10T17:01:56.564Z","comments":true,"path":"category/006系统设计/函数VS面向对象.html","link":"","permalink":"http://yoursite.com/category/006系统设计/函数VS面向对象.html","excerpt":"","text":"函数式编程和面向对象编程面向对象编程是一种自顶向下的程序设计方法。用面向对象方法构造软件时，我们将代码以名词（对象）做切割，每个对象有某种形式的标识符（self/this）、行为（方法）、和状态（成员变量）。识别出名词并且定义出它们的行为后，再定义出名词之间的交互。 现代面向对象设计倾向于定义出“服务类”，将操作多个领域对象的方法集合放在里面。这些服务类，虽然也是对象，但通常不具有独立状态，也没有与它们所操作的对象无关的独立行为。 函数式编程倾向于将软件分解为其需要执行的行为或操作，而且通常采用自底向上的方法。 函数式编程中的函数概念具有一定的数学上的含义，纯粹是对输入进行操作，产生结果。所有变量都被认为是不可变的。函数式编程中对不变性的强调有助于编写并发程序。函数式编程试图将副作用推迟到尽可能晚。 在面向对象中，采用继承，而在面向函数的过程，可以将行为封装为行为函数。 函数式编程的精髓在于尽可能地推迟副作用。 人类在认识和分析软件所要解决的业务领域问题时，一般由两个部分组成：业务流程与 业务规则。前者，回答了业务活动中先做什么后做什么的问题；后者，则回答了遇到什么情况时应该怎么做的问题。两者结合后，得到我们需要的业务结果，或者叫作“实现业务目标”。 如何将上述思维结果映射到软件中去的呢？使用面向对象的思考方式：对于业务流程，我们将其表达为若干对象之间的合作，比如UML里序列图的对象与消息，进而具化为具体的类及其职责，比如类及其若干业务方法。对于业务规则，我们将其表达为若干的判断逻辑，比如UML流程图里的判断分支，进而具化为业务方法里的if-else语句，或者再复杂一点，表达为工厂、策略等设计模式的实际运用。 函数：对于复杂业务规则的梳理，可以象数学归纳法一样进行演绎：假设一个函数y=f(x)，给定x的定义域，确定y的值域。特别是在排列组合等方面的一些问题，也经常采用递归的方式来解决。 参考http://zhidao.baidu.com/link?url=5DFTKngwDswtymrJEeutBx5BvWo0vzmV9Omov8Vpdn8DBM8nVkb6UG7qir6TEJVnQxmUqcu9c00vFdLo3kMpKITEtI_XBYNJmHLYkline7i","categories":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}],"tags":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://yoursite.com/tags/函数式编程/"}],"keywords":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}]},{"title":"综合监控到线程－pidstat命令","slug":"018linux/pidstat","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.056Z","comments":true,"path":"category/018linux/pidstat.html","link":"","permalink":"http://yoursite.com/category/018linux/pidstat.html","excerpt":"","text":"#pidstat 用pidstat工具可以获取每个进程使用cpu、内存和磁盘等系统资源的统计信息。 pidstat看上去就像top，不过top的输出会覆盖掉之前的输出，而pidstat的输出则添加在之前的输出的后面。 基本使用以1秒为信息采集周期，分别获取cpu、内存和磁盘IO的统计信息pidstat -u 1 pidstat -r 1 pidstat -d 1 字段描述pidstat 输出结果如下： 123456linux 4.4.0-53-generic (ubuntu) 12/15/2016 _x86_64_ (3 CPU)03:20:10 PM UID PID %usr %system %guest %CPU CPU Command03:20:10 PM 0 1 0.01 0.02 0.00 0.02 0 systemd03:20:10 PM 0 3 0.00 0.00 0.00 0.00 0 ksoftirqd/003:20:10 PM 0 7 0.00 0.04 0.00 0.04 2 字段 说明 PID 进程pid %usr 进程在用户态运行所占cpu时间比率 %system 进程在内核态运行所占cpu时间比率 %CPU 进程运行所占cpu时间比率,在各个CPU上的使用量的总和,例如1600%,表示消耗了将近16个CPU CPU 指示进程在哪个核运行 Command 进程对应的命令 执行pidstat默认输出信息为系统启动后到执行时间点的统计信息，因而即使当前某进程的cpu占用率很高，输出中的值有可能仍为0。 指定采样周期和采样次数pidstat 2 2 参数cpu使用情况统计(-u)使用-u选项，pidstat将显示各活动进程的cpu使用统计，执行”pidstat -u”与单独执行”pidstat”的效果一样 pidstat -p 1080 -u 1 3使用-p选项，我们可以查看特定进程的系统资源使用情况 使用t参数显示线程pidstat -p 1080 -u 1 3 -t 内存使用情况统计(-r)pidstat -r -p 927 1123403:30:13 PM UID PID minflt/s majflt/s VSZ RSS %MEM Command03:30:14 PM 122 927 0.00 0.00 1234292 149376 7.31 mysqld03:30:15 PM 122 927 0.00 0.00 1234292 149376 7.31 mysqld03:30:16 PM 122 927 0.00 0.00 1234292 149376 7.31 mysqld 字段 说明 minflt/s 每秒次缺页错误次数(minor page faults)，次缺页错误次数意即虚拟内存地址映射成物理内存地址产生的page fault次数 majflt/s 每秒主缺页错误次数(major page faults)，当虚拟内存地址映射成物理内存地址时，相应的page在swap中，这样的page fault为major page fault，一般在内存使用紧张时产生 VSZ 该进程使用的虚拟内存(以kB为单位) RSS 该进程使用的物理内存(以kB为单位) %MEM 该进程使用内存的百分比 Command 拉起进程对应的命令 IO情况统计(-d)pidstat -d 1 2 1203:40:17 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command03:40:18 PM 1000 1676 0.00 1441476.00 0.00 0 dd 字段 说明 kB_rd/s 每秒进程从磁盘读取的数据量(以kB为单位) kB_wr/s 每秒进程向磁盘写的数据量(以kB为单位) Command 拉起进程对应的命令 用法举例","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"多线程并发控制方法","slug":"022java/thread/多线的常用使用","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:44:12.478Z","comments":true,"path":"category/022java/thread/多线的常用使用.html","link":"","permalink":"http://yoursite.com/category/022java/thread/多线的常用使用.html","excerpt":"","text":"多线程并发控制方法等待多线程完成场景：我们需要解析一个Excel里多个sheet的数据，此时可以考虑使用多线程，每个线程解析一个sheet里的数据，等到所有的sheet都解析完之后，程序需要提示解析完成。在这个需求中，要实现主线程等待所有线程完成sheet的解析操作。 使用join代码实例如下： 1234567891011121314151617181920public static void main(String[] args) throws InterruptedException &#123; Thread parser1 = new Thread(new Runnable() &#123; @Override public void run() &#123; &#125; &#125;); Thread parser2 = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;parser2 finish&quot;); &#125; &#125;); parser1.start(); parser2.start(); parser1.join(); parser2.join(); System.out.println(&quot;all parser finish&quot;); &#125; join用于让当前执行线程等待join线程执行结束。其实现原理是不停检查join线程是否存活，如果join线程存活则让当前线程永远等待。join的代码如下123456789101112131415161718192021222324public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; 当线程处于运行状态时，isAlive返回true。wait（0）表示永远在调用join的线程上等待下去。直到join线程中止后，线程的this.notifyAll()方法会被调用，调用notifyAll()方法是在JVM里实现的。 使用CountDownLatch12345678910111213141516171819public class CountDownLatchTest &#123; static CountDownLatch latch = new CountDownLatch(2); public static void main(String[] args) throws InterruptedException &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(1); latch.countDown(); System.out.println(2); latch.countDown(); &#125; &#125;).start(); latch.await(); System.out.println(&quot;3&quot;); &#125;&#125; 等待多线程在一点同时执行CyclicBarrierCyclicBarrier的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。 CyclicBarrier初始化时规定一个屏障拦截的线程数量，然后计算调用了CyclicBarrier.await()进入等待的线程数。当线程数达到了这个数目时，所有进入等待状态的线程被唤醒并继续。CyclicBarrier初始时还可带一个Runnable的参数， 此Runnable任务在CyclicBarrier的数目达到后，所有其它线程被唤醒前被执行。 实例代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class TestCyclicBarrier &#123; private static final int THREAD_NUM = 5; public static class WorkerThread implements Runnable &#123; CyclicBarrier barrier; public WorkerThread(CyclicBarrier b) &#123; this.barrier = b; &#125; @Override public void run() &#123; // TODO Auto-generated method stub try &#123; System.out.println(&quot;Worker&apos;s waiting&quot;); //线程在这里等待，直到所有线程都到达barrier。 barrier.await(); System.out.println(&quot;ID:&quot; + Thread.currentThread().getId() + &quot; Working&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * @param args */ public static void main(String[] args) &#123; // TODO Auto-generated method stub CyclicBarrier cb = new CyclicBarrier(THREAD_NUM, new Runnable() &#123; //当所有线程到达barrier时执行 @Override public void run() &#123; // TODO Auto-generated method stub System.out.println(&quot;Inside Barrier&quot;); &#125; &#125;); for (int i = 0; i &lt; THREAD_NUM; i++) &#123; new Thread(new WorkerThread(cb)).start(); &#125; &#125;&#125; CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置。所以CyclicBarrier能处理更为复杂的业务场景。例如，如果计算发生错误，可以重置计数器，并让线程重新执行一次。 控制并发访问特定资源的线程数量Semaphore（信号量）是用来控制同时访问特定资源的线程数量。 1234567891011121314151617181920212223242526public class SemaphoreTest &#123; private static final int THREAD_COUNT = 30; private static ExecutorService threadPool = Executors.newFixedThreadPool(THREAD_COUNT); private static Semaphore s = new Semaphore(10); public static void main(String[] args) &#123; for (int i = 0; i &lt; THREAD_COUNT; i++) &#123; threadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; s.acquire(); System.out.println(&quot;save data&quot;); s.release(); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125;); &#125; threadPool.shutdown(); &#125;&#125; 为了创建信号量，必须使用可选的公平策略来确定许可的数量。任务通过调用信号量acquire（） 方法来获得许可，可通过调用信号量的release（）方法来释放许可。一旦获得许可，信号量中可用许可的数量减一。一旦释放，信号量的可用许可的总数加1。 Semaphore vs 线程池线程池控制的是线程数量，而信号量控制的是并发数量， 信号量的调用，当达到数量后，线程还是存在的，只是被挂起了而已。而线程池，同时执行的线程数量是固定的，超过了数量的只能等待。线程池是线程复用的；信号量是线程同步的。 两个线程之间交换数据使用Exchanger当线程A调用Exchange对象的exchange()方法后，他会陷入阻塞状态，直到线程B也调用了exchange()方法，然后以线程安全的方式交换数据，之后线程A和B继续运行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class ExchangerTest &#123; private static volatile boolean isDone = false; static class ExchangerProducer implements Runnable &#123; private Exchanger&lt;Integer&gt; exchanger; private static int data = 1; ExchangerProducer(Exchanger&lt;Integer&gt; exchanger) &#123; this.exchanger = exchanger; &#125; @Override public void run() &#123; while (!Thread.interrupted() &amp;&amp; !isDone) &#123; for (int i = 1; i &lt;= 3; i++) &#123; try &#123; TimeUnit.SECONDS.sleep(1); data = i; System.out.println(&quot;producer before: &quot; + data); data = exchanger.exchange(data); System.out.println(&quot;producer after: &quot; + data); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; isDone = true; &#125; &#125; &#125; static class ExchangerConsumer implements Runnable &#123; private Exchanger&lt;Integer&gt; exchanger; private static int data = 0; ExchangerConsumer(Exchanger&lt;Integer&gt; exchanger) &#123; this.exchanger = exchanger; &#125; @Override public void run() &#123; while (!Thread.interrupted() &amp;&amp; !isDone) &#123; data = 0; System.out.println(&quot;consumer before : &quot; + data); try &#123; TimeUnit.SECONDS.sleep(1); data = exchanger.exchange(data); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;consumer after : &quot; + data); &#125; &#125; &#125; /** * @param args */ public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); Exchanger&lt;Integer&gt; exchanger = new Exchanger&lt;Integer&gt;(); ExchangerProducer producer = new ExchangerProducer(exchanger); ExchangerConsumer consumer = new ExchangerConsumer(exchanger); exec.execute(producer); exec.execute(consumer); exec.shutdown(); try &#123; exec.awaitTermination(30, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/tags/高并发/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"},{"name":"源码阅读","slug":"源码阅读","permalink":"http://yoursite.com/tags/源码阅读/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"java并发机制和内存模型总结","slug":"022java/thread/java并发知识总结","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:44:57.118Z","comments":true,"path":"category/022java/thread/java并发知识总结.html","link":"","permalink":"http://yoursite.com/category/022java/thread/java并发知识总结.html","excerpt":"","text":"术语临界区临界区表示一种公共资源或者共享数据。每一刻只能被一个线程使用，如果临界区资源被占用，其他线程想使用这个资源，就必须等待。 CASCompare and Swap，比较并设置。 用于在硬件层面上提供原子性操作。在Intel 处理器中，比较并交换通过指令cmpxchg实现。比较是否和给定的数值一致，如果一致则修改，不一致则不修改。 并发编程的挑战并发编程的目的是为了让程序运行得更快，但是，并不是启动更多的线程就能让程序最大限度地并发执行。 线程有创建和上下文切换的开销、死锁的问题，以及受限于硬件和软件的资源限制。 amdah1定律：多核cpu优化的效果取决于cpu数量和系统中串行化程序的比重。只提供cpu数量也无法提高系统性能。 #java中并发编程模型 Java中所使用的并发机制依赖于JVM的实现和CPU的指令。 在并发编程中，需要处理两个关键问题：线程之间如何通信及线程之间如何同步。 通信是指线程之间以何种机制来交换信息。线程之间的通信机制有两种：共享内存和消息传递。 Java的并发采用的是共享内存模型。由Java内存模型（本文简称为JMM）控制。 在共享内存的并发模型里，线程之间共享程序的公共状态，通过写-读内存中的公共状态进行隐式通信。 同步是指程序中用于控制不同线程间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 操作系统层面：为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作。 现代的处理器使用写缓冲区临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，减少对内存总线的占用。 虽然写缓冲区有很多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。 如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。 如何解决其他处理器缓存的值还是旧的？ 为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议。每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了。 操作系统层面：在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。指令重排,同时带来了乱序的问题。在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 为了保证内存可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。 JMM总结JMM从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的Local内存（Local Memory），本地内存中存储了该线程以读/写共享变量的副本。 本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 共享变量：在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享。这些变量统称为共享变量。 JMM关键技术点JMM遵循的基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都可以。 JMM的关键技术点主要为多线程的原子性／可见性／有序性。 原子性处理器如何实现原子操作32位IA-32处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作 Java如何实现原子操作在Java中可以通过锁和循环CAS的方式来实现原子操作。 从Java 1.5开始，JDK的并发包里提供了一些类来支持原子操作，如AtomicBoolean（用原子方式更新的boolean值）、AtomicInteger（用原子方式更新的int值）和AtomicLong（用原子方式更新的long值） 使用循环CAS实现原子操作JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止 使用锁机制实现原子操作锁机制保证了只有获得锁的线程才能够操作锁定的内存区域。JVM内部实现了很多种锁机制，有偏向锁、轻量级锁和互斥锁。有意思的是除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。 可见性JMM通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证。 有序性在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。指令重排,同时带来了乱序的问题。 JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障（Memory Barriers，Intel称之为Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。 那些指令不能进行重排？ happen-before规则。 #常用同步方式 synchronized、volatile、Lock volatile在多线程并发编程中synchronized和volatile都扮演着重要的角色，volatile可以认为是轻量级的synchronized。它在多处理器开发中保证了共享变量的“可见性”。 Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 一个volatile变量的单个读/写操作，与一个普通变量的读/写操作都是使用同一个锁来同步，它们之间的执行效果相同。 volatile的定义与实现原理Java语言规范第3版中对volatile的定义如下：Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 volatile读的内存语义：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 volatile读的内存语义：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。例如在每个volatile写操作的前面插入一个StoreStore屏障。 为了提供一种比锁更轻量级的线程之间通信的机制，JSR-133专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序，确保volatile的写-读和锁的释放-获取具有相同的内存语义。 ##Lock对象 volatile仅仅保证对单个volatile变量的读/写具有原子性，而锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性。 内存内存语义线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。·线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享变量所做修改的）消息。 代码12345678910111213141516171819202122232425import java.util.concurrent.locks.ReentrantLock;class ReentrantLockExample &#123; int a = 0; ReentrantLock lock = new ReentrantLock(); public void writer() &#123; lock.lock(); // try &#123; a++; &#125; finally &#123; lock.unlock(); // &#125; &#125; public void reader() &#123; lock.lock(); // try &#123; int i = a; // &#125; finally &#123; lock.unlock(); // &#125; &#125;&#125; 在ReentrantLock中，调用lock()方法获取锁；调用unlock()方法释放锁。内部使用整型的volatile变量（命名为state）来维护同步状态。 整型的volatile变量（命名为state）来维护同步状态。加锁方法首先读volatile变量state。释放锁的最后写volatile变量state。 concurrent包通用化的实现模式首先，声明共享变量为volatile。然后，使用CAS的原子条件更新来实现线程之间的同步。同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。 synchronizedVS locksynchronized是托管给JVM执行的，而lock是Java写的控制锁的代码。 synchronized称为“重量级锁”,在1.5中，synchronize是性能低效的。导致有可能加锁消耗的系统时间比加锁以外的操作还多。到了Java1.6发生了变化。synchronize在语义上很清晰，可以进行很多优化（为了减少上下文切换），有适应自旋，锁消除，锁粗化，轻量级锁，偏向锁等等。导致在Java1.6上synchronize的性能并不比Lock差。 synchronized原始采用的是CPU悲观锁机制，即线程获得的是独占锁。独占锁意味着其他线程只能依靠阻塞来等待线程释放锁，而在CPU转换线程阻塞时会引起线程上下文切换。 Lock用的是乐观锁方式。每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。获得锁的方法是compareAndSetState，就是调用的CPU提供的特殊指令。 Synchonized在JVM里的实现原理synchronized实现同步的基础：Java中的每一个对象都可以作为锁。 代码块同步是使用monitorenter和monitorexit指令实现monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处。 任何对象都有一个monitor与之关联，当一个monitor被持有后，它将处于锁定状态。 模型介绍happen-before规则顺序一致性内存模型参考 各种 Java Thread State 第一分析法则","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/tags/高并发/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"grep＋find学习笔记","slug":"018linux/grep＋find学习笔记","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.055Z","comments":true,"path":"category/018linux/grep＋find学习笔记.html","link":"","permalink":"http://yoursite.com/category/018linux/grep＋find学习笔记.html","excerpt":"","text":"grep学习指南grep -［acinv］ ‘搜索内容串’ filename 参数 -a：表示以文本文件方式搜索。 -c：表示计算找到符合行的次数。 -i：表示忽略字母大小写。 -n：表示顺便输出行号。 -v：表示反向选择，即找到没有搜索字符串的行。 用法搜索没有the的行，并输出行号grep -nv &#39;the&#39; regular_express.txt find学习指南find pathname -options ［-print -exec -ok 例如：find /opt/applications/ -name *.jar | grep ucs-memcached pathname：是find命令所查找的目录路径。例如用符号.来表示当前目录，用/来表示系统根目录。 find命令选项 参数 说明 -name filename 查找名为filename的文件 -perm 按执行权限来查找 -user username 按文件属主来查找 -group groupname 按组来查找 -mtime -n +n 按文件更改时间来查找文件，-n指n天以内，+n指n天以前 -atime -n +n 按文件访问时间来查GIN: 0px”&gt; -ctime -n +n 按文件创建时间来查找文件，-n指n天以内，+n指n天以前 -nogroup 查无有效属组的文件，即文件的属组在/etc/groups中不存在 -nouser 查无有效属主的文件，即文件的属主在/etc/passwd中不存 -newer f1 !f2 找文件，-n指n天以内，+n指n天以前 -ctime -n +n 按文件创建时间来查找文件，-n指n天以内，+n指n天以前 -nogroup 查无有效属组的文件，即文件的属组在/etc/groups中不存在 -nouser 查无有效属主的文件，即文件的属主在/etc/passwd中不存 -newer f1 !f2 查更改时间比f1新但比f2旧的文件 -type b/d/c/p/l/f 查是块设备、目录、字符设备、管道、符号链接、普通文件 -size n[c] 查长度为n块[或n字节]的文件 -depth 使查找在进入子目录前先行查找完本目录 -fstype 查更改时间比f1新但比f2旧的文件 -type b/d/c/p/l/f 查是块设备、目录、字符设备、管道、符号链接、普通文件 -size n[c] 查长度为n块[或n字节]的文件 -depth 使查找在进入子目录前先行查找完本目录 -fstype 查位于某一类型文件系统中的文件，这些文件系统类型通常可 在/etc/fstab中找到 -mount 查文件时不跨越文件系统mount点 -follow 如果遇到符号链接文件，就跟踪链接所指的文件 -cpio %; 查位于某一类型文件系统中的文件，这些文件系统类型通常可 在/etc/fstab中找到 -mount 查文件时不跨越文件系统mount点 -follow 如果遇到符号链接文件，就跟踪链接所指的文件 -cpio 对匹配的文件使用cpio命令，将他们备份到磁带设备中 -prune 忽略某个目录 -type 查找某一类型的文件b - 块设备文件。d - 目录。c - 字符设备文件。p - 管道文件。l - 符号链接文件。f - 普通文件 find应用范例使用xargsfind命令把匹配到的文件传递给xargs命令，而xargs命令每次只获取一部分文件而不是全部 execexec来配合find查找find . -type f -exec ls -l {} \\; 删除目录中访问时间在7日以内且含有数字后缀的admin.log文件find . -name “admin.log［0-9]” -atime -7 -exec rm {} \\; 文件字符替换find -type f ! -path &#39;*/.svn/*&#39; | xargs sed -i &#39;s/netfinworks/klwork/g&#39; 文件批量删除find . -name .svn -print | xargs rm -rf 只要我们把find的输出作为xargs的输入，就必须将-print0与find结合使用，以字符null来分隔输出。 ###用find匹配并列出所有的.txt文件，然后用xargs将这些文件删除 find . -type f -name “*.txt” -print0 | xargs -0 rm -f ### find .|xargs grep -ri “iframe” find -name “*.html” |xargs grep -ri “iframe” 参考 sed命令详解 sed 简明教程 awk案例学习","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"共享锁分析","slug":"022java/thread/共享锁","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:44:07.453Z","comments":true,"path":"category/022java/thread/共享锁.html","link":"","permalink":"http://yoursite.com/category/022java/thread/共享锁.html","excerpt":"","text":"共享锁代码分析参考代码ReentrantReadWriteLock ReadLock中的sync是一个Sync对象，Sync继承于AQS类，即Sync就是一个锁。ReentrantReadWriteLock中也有一个Sync对象，而且ReadLock中的sync和ReentrantReadWriteLock中的sync是对应关系。即ReentrantReadWriteLock和ReadLock共享同一个AQS对象，共享同一把锁。 获取共享锁通过tryAcquireShared()尝试获取共享锁。尝试成功的话，则直接返回；尝试失败的话，则通过doAcquireShared()不断的循环并尝试获取锁，若有需要，则阻塞等待。 lockReadLock.java 123public void lock() &#123; sync.acquireShared(1);&#125; acquireSharedSync继承于AQS，acquireShared()定义在AQS中。源码如下： public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);} 通过tryAcquireShared()来尝试获取锁成功的话，则不再做任何动作失败的话，则通过doAcquireShared()来获取锁。doAcquireShared()会获取到锁了才返回 tryAcquireShared()试获取锁123456789101112131415161718192021222324252627282930313233343536protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); // 获取“锁”的状态 int c = getState(); // 如果“锁”是“互斥锁”，并且获取锁的线程不是current线程；则返回-1。 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; // 获取“读取锁”的共享计数 int r = sharedCount(c); // 如果“不需要阻塞等待”，并且“读取锁”的共享计数小于MAX_COUNT； // 则通过CAS函数更新“锁的状态”，将“读取锁”的共享计数+1。 if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) &#123; // 第1次获取“读取锁”。 if (r == 0) &#123; firstReader = current; firstReaderHoldCount = 1; // 如果想要获取锁的线程(current)是第1个获取锁(firstReader)的线程 &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; // HoldCounter是用来统计该线程获取“读取锁”的次数。 HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != current.getId()) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); // 将该线程获取“读取锁”的次数+1。 rh.count++; &#125; return 1; &#125; return fullTryAcquireShared(current);&#125; exclusiveCount,sharedCount 计算锁的数量。 123456789static final int SHARED_SHIFT = 16; static final int SHARED_UNIT = (1 &lt;&lt; SHARED_SHIFT); static final int MAX_COUNT = (1 &lt;&lt; SHARED_SHIFT) - 1; static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1; /** Returns the number of shared holds represented in count */ static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125; /** Returns the number of exclusive holds represented in count */ static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125; 如果从二进制来看EXCLUSIVE_MASK的表示，这个值的低16位全是1，而高16位则全是0，所以exclusiveCount是把state的低16位取出来，表示当前这个锁的写锁加锁次数。 再来看sharedCount，取出了state的高16位，用来表示这个锁的读锁加锁次数。也就是说state的高16位和低16位来分别表示这个锁上的读锁和写锁的加锁次数。看源代码 compareAndSetState(c, c + SHARED_UNIT))，将“读取锁”的共享计数+1。SHARED_UNIT正好是高16位的加1操作。 HoldCounter是线程变量，用来统计该线程获取“读取锁”的次数。 说明： tryAcquireShared()的作用是尝试获取“共享锁”。如果在尝试获取锁时，“不需要阻塞等待”并且“读取锁的共享计数小于MAX_COUNT”，则直接通过CAS函数更新“读取锁的共享计数”，以及将“当前线程获取读取锁的次数+1”。否则，通过fullTryAcquireShared()获取读取锁。 fullTryAcquireShared12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758final int fullTryAcquireShared(Thread current) &#123; HoldCounter rh = null; for (;;) &#123; // 获取“锁”的状态 int c = getState(); // 如果“锁”是“互斥锁”，并且获取锁的线程不是current线程；则返回-1。 if (exclusiveCount(c) != 0) &#123; if (getExclusiveOwnerThread() != current) return -1; // 如果“需要阻塞等待”。 // (01) 当“需要阻塞等待”的线程是第1个获取锁的线程的话，则继续往下执行。 // (02) 当“需要阻塞等待”的线程获取锁的次数=0时，则返回-1。 &#125; else if (readerShouldBlock()) &#123; // 如果想要获取锁的线程(current)是第1个获取锁(firstReader)的线程 if (firstReader == current) &#123; &#125; else &#123; if (rh == null) &#123; rh = cachedHoldCounter; if (rh == null || rh.tid != current.getId()) &#123; rh = readHolds.get(); if (rh.count == 0) readHolds.remove(); &#125; &#125; // 如果当前线程获取锁的计数=0,则返回-1。 if (rh.count == 0) return -1; &#125; &#125; // 如果“不需要阻塞等待”，则获取“读取锁”的共享统计数； // 如果共享统计数超过MAX_COUNT，则抛出异常。 if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // 将线程获取“读取锁”的次数+1。 if (compareAndSetState(c, c + SHARED_UNIT)) &#123; // 如果是第1次获取“读取锁”，则更新firstReader和firstReaderHoldCount。 if (sharedCount(c) == 0) &#123; firstReader = current; firstReaderHoldCount = 1; // 如果想要获取锁的线程(current)是第1个获取锁(firstReader)的线程， // 则将firstReaderHoldCount+1。 &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != current.getId()) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); // 更新线程的获取“读取锁”的共享计数 rh.count++; cachedHoldCounter = rh; // cache for release &#125; return 1; &#125; &#125;&#125; doAcquireShared()1234567891011121314151617181920212223242526272829303132private void doAcquireShared(int arg) &#123; // addWaiter(Node.SHARED)的作用是，创建“当前线程”对应的节点，并将该线程添加到CLH队列中。 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; // 获取“node”的前一节点 final Node p = node.predecessor(); // 如果“当前线程”是CLH队列的表头，则尝试获取共享锁。 if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; // 如果“当前线程”不是CLH队列的表头，则通过shouldParkAfterFailedAcquire()判断是否需要等待， // 需要的话，则通过parkAndCheckInterrupt()进行阻塞等待。若阻塞等待过程中，线程被中断过，则设置interrupted为true。 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 说明：doAcquireShared()的作用是获取共享锁。它会首先创建线程对应的CLH队列的节点，然后将该节点添加到CLH队列中。CLH队列是管理获取锁的等待线程的队列。如果“当前线程”是CLH队列的表头，则尝试获取共享锁；否则，则需要通过shouldParkAfterFailedAcquire()判断是否阻塞等待，需要的话，则通过parkAndCheckInterrupt()进行阻塞等待。doAcquireShared()会通过for循环，不断的进行上面的操作；目的就是获取共享锁。","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/tags/高并发/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"},{"name":"源码阅读","slug":"源码阅读","permalink":"http://yoursite.com/tags/源码阅读/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"Linux shell 总结","slug":"018linux/linuxshell总结","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-25T08:31:06.886Z","comments":true,"path":"category/018linux/linuxshell总结.html","link":"","permalink":"http://yoursite.com/category/018linux/linuxshell总结.html","excerpt":"","text":"管道 与 xargs管道将一个命令的stdout（标准输出）重新定向到另一个命令的stdin（标准输入)，也就是说接收标准输入的命令才可以用作管道右边。 常用来作为接收数据管道命令有：sed,awk,cut,head,top,less,more,wc,join,sort,split 等等，都是些文本处理命令 为什么要有xargs?有些命令例如cp，echo等，只能接受参数，就需要使用xargs。 xargs是一个很有用的命令，它擅长将标准输入数据转换成命令行参数。xargs能够处理stdin并将其转换为特定命令的命令行参数。 使用例子将单行输入转换成多行输出echo &quot;1 2 3 4 5 6 7 8 9&quot; | xargs -n 3将输出1 2 34 5 67 8 9 用-d选项为输入指定一个定制的定界符echo &quot;splitXsplitXsplitXsplit&quot; | xargs -d X 从文本读取，一行一个cat args.txt | xargs -n 1 ./cecho.sh 固定不变的命令参数，使用I例如./cecho.sh –p arg1 –larg1是唯一的可变文本，其余部分都保持不变 xargs有一个选项-I，指定一个替换字符串，这个字符串在xargs扩展时会被替换掉。当-I与xargs结合使用时，对于每一个参数，命令都会被执行一次。 cat args.txt | xargs -I {} ./cecho.sh -p {} -l 参考 sed命令详解 sed 简明教程 awk案例学习","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"linux shell编程基础","slug":"018linux/linux-shell","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-25T11:48:23.944Z","comments":true,"path":"category/018linux/linux-shell.html","link":"","permalink":"http://yoursite.com/category/018linux/linux-shell.html","excerpt":"","text":"#Shell脚本的基本规则 start在 Shell 脚本的第 1 行开始处指定“#!/bin/sh” 双引号中包含的变量可以用其值来替换，而如果是单引号，则依然保持变量名。 当需要将变量名和其他字符串连接在一起时，请用如下的 {} 符号将变量名括起来echo “${message}World” Shell脚本运行时的参数的引用可以通过变量 $1~$9 来访问 Shell 脚本运行时的参数。用于启动 Shell 脚本的命令的名称本身也可以通过 $0 访问。 “$@”为一个包含所有参数的列表 条件判断123456789101112131415#!/bin/shmessage=&quot;Hello&quot;if test &quot;$message&quot; = &quot;Hello&quot;; then echo &quot;Hello World&quot;fiif [ &quot;$message&quot; = &quot;Hello&quot; ]; then echo &quot;Hello World&quot;fiif [[ $message == &quot;Hello&quot; ]]; then echo &quot;Hello World&quot;fi","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"查看发包的端口－netstat","slug":"018linux/netstat","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.056Z","comments":true,"path":"category/018linux/netstat.html","link":"","permalink":"http://yoursite.com/category/018linux/netstat.html","excerpt":"","text":"#netstat 可用于列出系统上所有的网络套接字连接情况，包括 tcp, udp 以及 unix 套接字，另外它还能列出处于监听状态（即等待接入请求）的套接字。 基本使用netstat －an 参数 参数说明 说明 -a 显示所有socket，包括正在监听的。 -c 每隔1秒就重新显示一遍，直到用户中断它。 -i 显示所有网络接口的信息，格式同“ifconfig -n 以网络IP地址代替名称，显示出网络连接情形。 -n 选项禁用域名解析功能 -r 显示核心路由表，格式同“route -t 显示TCP协议的连接情况。 -u 显示UDP协议的连接情况。 -v 显示正在进行的工作。 字段描述用法举例列出所有连接列出所有当前的连接。使用 -a 选项即netstat -a netstat -an | grep LISTENnetstat -an | grep ESTABLISHED 参数中state的含义如下所示 名称 说明 LISTEN 侦听来自远方的TCP端口的连接请求。 SYN_SENT 在发送连接请求后等待匹配的连接请求。 SYN_RECEIVED 在收到和发送一个连接请求后等待对方对连接请求的确认。 ESTABLISHED 代表一个打开的连接，我们常用此作并发连接数。 FIN_WAIT1 等待远程TCP连接中断请求，或先前的连接中断请求的确认。 FIN_WAIT2 从远程TCP等待连接中断请求。 CLOSE_WAIT 等待从本地用户发来的连接中断请求。 CLOSING 等待远程TCP对连接中断的确认。 LAST_ACK 等待原来发向远程TCP的连接中断请求的确认。 TIME_WAIT 等待足够的时间以确保远程TCP接收到连接中断请求的确认。 CLOSED 没有任何连接状态 只列出 TCP 或 UDP 协议的连接使用 -t 选项列出 TCP 协议的连接netstat -at 使用 -u 选项列出 UDP 协议的连接netstat -au 禁用反向域名解析，加快查询速度 netstat 会通过反向域名解析技术查找每个 IP 地址对应的主机名。这会降低查找速度。如果你觉得 IP 地址已经足够，而没有必要知道主机名，就使用 -n 选项禁用域名解析功能 netstat -ant 只列出监听中的连接任何网络服务的后台进程都会打开一个端口，用于监听接入的请求。这些正在监听的套接字也和连接的套接字一样，也能被 netstat 列出来。使用 -l 选项列出正在监听的套接字。netstat -tnl 不要使用a 获取进程名、进程号以及用户 ID使用 -p 选项查看进程信息sudo netstat -nlpt 使用 -p 选项时，netstat 必须运行在 root 权限之下，不然它就不能得到运行在 root 权限下的进程名 sudo netstat -ltpe -ep 选项可以同时查看进程名和用户名假如你将 -n 和 -e 选项一起使用，User 列的属性就是用户的 ID 号，而不是用户名 所有网络包的统计情况netstat -s包含Icmp，ip,tcp等协议 显示多播组信息netstat -g 找出运行在指定端口的进程root用户netstat -anop | grep 3306 参考https://linux.cn/article-2434-1.htmlhttp://blog.csdn.net/u013281361/article/details/52021993","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"多CPU监控-mpstat","slug":"018linux/mpstat","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.056Z","comments":true,"path":"category/018linux/mpstat.html","link":"","permalink":"http://yoursite.com/category/018linux/mpstat.html","excerpt":"","text":"#mpstat mpstat是MultiProcessor Statistics的缩写，是实时系统监控工具。其报告与CPU的一些统计信息，这些信息存放在/proc/stat文件中。在多CPUs系统里，不但能查看所有CPU的平均状况信息，而且能够查看特定CPU的信息。 这个命令显示每个CPU的时间使用百分比，你可以用它来检查CPU是否存在负载不均衡。单个过于忙碌的CPU可能意味着整个应用只有单个线程在工作。 如果没有安装sudo apt install sysstat 参数mpstat的语法如下： mpstat [-P {|ALL}] [internal [count]] 参数的含义如下： 参数 解释 -P {|ALL} 表示监控哪个CPU， cpu在[0,cpu个数-1]中取值 internal 相邻的两次采样的间隔时间 count 采样的次数，count只能和delay一起使用 当没有参数时，mpstat则显示系统启动以后所有信息的平均值。有interval时，第一行的信息自系统启动以来的平均信息。 基本使用mpstat -P ALL 1输出如下1234567Linux 4.4.0-47-generic (ubuntu) 12/14/2016 _x86_64_ (3 CPU)10:27:08 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle10:27:09 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0010:27:09 PM 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0010:27:09 PM 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0010:27:09 PM 2 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 99.00 参数 解释 CPU 处理器ID user 用户态的CPU时间（%） ，不包含 nice值为负 进程 dusr/dtotal*100 nice nice值为负进程的CPU时间（%） dnice/dtotal*100 system 核心时间（%） dsystem/dtotal*100 iowait 硬盘IO等待时间（%） diowait/dtotal*100 irq 软中断时间（%） dirq/dtotal*100 soft 软中断时间（%） dsoftirq/dtotal*100 idle CPU除去等待磁盘IO操作外的因为任何原因而空闲的时间闲置时间 （%） didle/dtotal*100 intr/s 每秒CPU接收的中断的次数 dintr/dtotal*100 以上针对在internal时间段里，统计。 %iowait列，CPU等待I/O操作所花费的时间。这个值持续很高通常可能是I/O瓶颈所导致的。通过这个参数可以比较直观的看出当前的I/O操作是否存在瓶颈","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"Linux id 命令","slug":"018linux/linx-id","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.055Z","comments":true,"path":"category/018linux/linx-id.html","link":"","permalink":"http://yoursite.com/category/018linux/linx-id.html","excerpt":"","text":"id命令用来做什么？id 命令可以显示真实有效的用户 ID(UID) 和组 ID(GID)。UID 是对一个用户的单一身份标识。组 ID（GID）则对应多个UID。 一些程序可能需要 UID/GID 来运行，我们想知道某个用户的 UID 和 GID ，这时 id命令是非常有用的，id使我们更加容易地找出用户的 UID 以 GID， 而不必在 /etc/group 文件中搜寻。 ＃id命令的输出 12ww@ubuntu:~$ iduid=1000(ww) gid=1000(ww) groups=1000(ww),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),113(lpadmin),128(sambashare) 用户 ww 的 UID 号码= 1000, GID 号码= 1000 adm 的 GID 号码= 4cdrom 的 GID 号码= 24sudo 的 GID 号码= 27dip 的 GID 号码= 30plugdev 的 GID 号码= 46lpadmin 的 GID 号码= 113sambashare 的 GID 号码= 128 通过查询/etc/group进行验证cat /etc/group | grep 128显示如下`sambashare:x:128:ww 其他用户输出特定用户信息id hadoop 只输出有效的组IDid -g","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"网络命令-sar","slug":"018linux/sar","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.057Z","comments":true,"path":"category/018linux/sar.html","link":"","permalink":"http://yoursite.com/category/018linux/sar.html","excerpt":"","text":"#sar sar基本用法sar 如果出现错误Cannot open /var/log/sysstat/sa14: No such file or directory直接使用sar就会查找历史文件，因为刚装还没有文件14代表当天日期，使用参数-o 让其生成该文件sar -o 14 查看某天的系统状态 sar -f /var/log/sysstat/sa14 默认情况是对过去时间段进行数据统计，一般从最近的0：00开始显示。如果想继续查看一天前的报告，可以用-f选项指定保存在/var/log/sysstat目录下的日志文件中。如果想周期性的查看当前数据可以命令后面加上数字参数，如sar 1 3 ，表示：1秒1次，共3次。 sar 1 3 1234567Linux 4.4.0-47-generic (ubuntu) 12/14/2016 _x86_64_ (3 CPU)11:31:33 PM CPU %user %nice %system %iowait %steal %idle11:31:34 PM all 0.33 0.00 0.00 0.00 0.00 99.6711:31:35 PM all 0.00 0.00 0.33 0.00 0.00 99.6711:31:36 PM all 0.00 0.00 0.00 0.00 0.00 100.00Average: all 0.11 0.00 0.11 0.00 0.00 99.78 参数 参数 解释 -A 等价于 -bBcdqrRuvwWy -I SUM -I XALL -n ALL -P ALL -b 显示I/O和传送速率的统计信息 -B 输出内存页面的统计信息 -c 输出进程统计信息，每秒创建的进程数 -d 输出每一个块设备的活动信息 -i interval 指定间隔时长，单位为秒 -p 显示友好设备名字，以方便查看，也可以和-d 和-n 参数结合使用，比如 -dp 或-np -q 输出进程队列长度和平均负载状态统计信息 -r 输出内存和交换空间的统计信息 -R 输出内存页面的统计信息 -t 读取 /var/log/sa/saDD 的数据时显示其中记录的原始时间，如果没有这个参数使用用户的本地时间 -u 输出CPU使用情况的统计信息 -v 输出inode、文件和其他内核表的统计信息 -V 输出版本号信息 -w 输出系统交换活动信息 -W 输出系统交换的统计信息 -y 输出TTY设备的活动信息 -n {DEV EDEV NFS NFSD SOCK ALL} 分析输出网络设备状态统计信息。 DEV 报告网络设备的统计信息 EDEV 报告网络设备的错误统计信息 NFS 报告 NFS 客户端的活动统计信息 NFSD 报告 NFS 服务器的活动统计信息 SOCK 报告网络套接字（sockets）的使用统计信息 ALL 报告所有类型的网络活动统计信息 -x {pid or SELF or ALL} 输出指定进程的统计信息。 -X {pid or SELF or ALL} 输出指定进程的子进程的统计信息 -I {irq or SUM or ALL orXALL} 输出指定中断的统计信息。 -P {cpu or ALL} 输出指定 CPU 的统计信息 -o filename 将输出信息保存到文件 filename -f filename 从文件 filename 读取数据信息。filename 是使用-o 选项时生成的文件。 -s hh:mm:ss 指定输出统计数据的起始时间 -e hh:mm:ss 指定输出统计数据的截至时间，默认为18:00:00 用法汇总sar –u 查看CPU使用率sar -u 1234567891009时39分42秒 LINUX RESTART09时40分01秒 CPU %user %nice %system %iowait %steal %idle 09时50分01秒 all 0.14 0.00 0.58 0.12 0.00 99.15 10时00分01秒 all 0.06 0.00 0.50 0.16 0.00 99.27 10时10分01秒 all 0.11 0.06 0.95 2.58 0.00 96.30 10时20分01秒 all 0.12 0.19 0.82 1.41 0.00 97.46 10时30分01秒 all 0.14 0.00 0.54 0.12 0.00 99.20 10时40分01秒 all 0.15 0.00 0.54 0.16 0.00 99.15 Average: all 0.12 0.04 0.65 0.76 0.00 98.43 这里： %user ： 用户模式下消耗的CPU时间的比例； %nice：通过nice改变了进程调度优先级的进程，在用户模式下消耗的CPU时间的比例； %system：系统模式下消耗的CPU时间的比例； %iowait：CPU等待磁盘I/O而导致空闲状态消耗时间的比例； %steal：利用Xen等操作系统虚拟化技术时，等待其他虚拟CPU计算占用的时间比例； %idle：CPU没有等待磁盘I/O等的空闲状态消耗的时间比例； 注： 如果 %iowait 的值过高，表示硬盘存在I/O瓶颈如果 %idle 的值高但系统响应慢时，有可能是 CPU 等待分配内存，此时应加大内存容量如果 %idle 的值持续低于 10，则系统的 CPU 处理能力相对较低，表明系统中最需要解决的资源是 CPU。 sar –q 查看平均负荷sar -q 123456789101109时39分42秒 LINUX RESTART09时40分01秒 runq-sz plist-sz ldavg-1 ldavg-5 ldavg-15 09时50分01秒 0 152 0.00 0.02 0.05 10时00分01秒 0 152 0.00 0.00 0.00 10时10分01秒 0 156 0.39 0.09 0.03 10时20分01秒 0 151 0.00 0.03 0.01 10时30分01秒 0 151 0.00 0.00 0.00 10时40分01秒 0 151 0.00 0.00 0.00 10时50分01秒 0 151 0.00 0.00 0.00 Average: 0 152 0.06 0.02 0.01 runq-sz： 运行队列的长度（等待运行的进程数）plist-sz： 进程列表中进程（processes）和线程（threads）的数量ldavg-1： 最后1分钟的系统平均负载（System load average）ldavg-5： 过去5分钟的系统平均负载ldavg-15： 过去15分钟的系统平均负载 sar –r 查看内存使用情况sar -r 12345678909时39分42秒 LINUX RESTART09时40分01秒 kbmemfree kbmemused %memused kbbuffers kbcached kbswpfree kbswpused %swpused kbswpcad 09时50分01秒 481572 553492 53.47 35592 384508 2097144 0 0.00 0 10时00分01秒 480960 554104 53.53 36032 384512 2097144 0 0.00 0 10时10分01秒 404952 630112 60.88 77764 399432 2097144 0 0.00 0 10时20分01秒 375824 659240 63.69 87356 410892 2097144 0 0.00 0 10时30分01秒 371860 663204 64.07 87756 411064 2097144 0 0.00 0 … kbmemfree：空闲物理内存量； kbmemused：使用中的物理内存量； %memused：物理内存量使用率； kbbuffers：内核中作为缓冲区使用的物理内存容量； kbcacheed：内核中作为缓存使用的物理内存容量； kbswpfree：交换区的空闲容量； kbswpused：使用中的交换区容量； sar –W 查看页面交换发生状况[root@localhost ~]# sar -W 123456714时30分01秒 pswpin/s pswpout/s14时40分01秒 0.00 0.0014时50分01秒 0.00 0.0015时00分01秒 0.00 0.00Average: 0.00 0.00… sar –b 查看I/O和传送速率的统计信息[root@localhost ~]# sar -b 1 5 15时08分18秒 tps rtps wtps bread/s bwrtn/s15时08分19秒 0.00 0.00 0.00 0.00 0.0015时08分20秒 0.00 0.00 0.00 0.00 0.0015时08分21秒 0.00 0.00 0.00 0.00 0.0015时08分22秒 13.27 0.00 13.27 0.00 220.4115时08分23秒 0.00 0.00 0.00 0.00 0.00Average: 2.66 0.00 2.66 0.00 44.17 tps： 每秒钟物理设备的 I/O 传输总量rtps: 每秒钟从物理设备读入的数据总量wtps: 每秒钟向物理设备写入的数据总量bread/s: 每秒钟从物理设备读入的数据量，单位为 块/sbwrtn/s: 每秒钟向物理设备写入的数据量，单位为 块/s sar -n DEV查看当前网卡流量状态sar -n DEV 1 10 //每1秒显示1次，一共显示10次 -n 的其他参数DEV 报告网络设备的统计信息EDEV 报告网络设备的错误统计信息NFS 报告 NFS 客户端的活动统计信息NFSD 报告 NFS 服务器的活动统计信息SOCK 报告网络套接字（sockets）的使用统计信息ALL 报告所有类型的网络活动统计信息 123Average: IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutilAverage: lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: enp0s5 0.66 0.66 0.04 0.19 0.00 0.00 0.00 0.00 参数 解释 IFACE 网络设备名 rxpck/s 每秒接收的包总数 txpck/s 每秒传输的包总数 rxbyt/s 每秒接收的字节（byte）总数 txbyt/s 每秒传输的字节（byte）总数 rxcmp/s 每秒接收压缩包的总数 txcmp/s 每秒传输压缩包的总数 rxmcst/s 每秒接收的多播（multicast）包的总数 sar -n TCP,ETCP 1sar -n TCP,ETCP 11234512:17:19 AM active/s passive/s iseg/s oseg/s12:17:20 AM 1.00 0.00 10233.00 18846.00 12:17:19 AM atmptf/s estres/s retrans/s isegerr/s orsts/s12:17:20 AM 0.00 0.00 0.00 0.00 0.00 命令显示一些关键TCP指标的汇总。其中包括：active/s：本地每秒创建的TCP连接数（比如concept()创建的）passive/s：远程每秒创建的TCP连接数（比如accept()创建的）retrans/s：每秒TCP重传次数。 主动连接数（active）和被动连接数（passive）通常可以用来粗略地描述系统负载。可以认为主动连接是对外的，而被动连接是对内的。 重传是网络或系统问题的一个信号；它可能是不可靠的网络（比如公网）所造成的，也有可能是服务器已经过载并开始丢包。 参考http://blog.chinaunix.net/uid-25266990-id-2950467.htmlhttp://blog.sina.com.cn/s/blog_3d5b39820101n6rk.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"systemtap学习总结","slug":"018linux/systemtap","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.057Z","comments":true,"path":"category/018linux/systemtap.html","link":"","permalink":"http://yoursite.com/category/018linux/systemtap.html","excerpt":"","text":"systemtap简介systemtap安装测试安装安装elfutils，提供分析调试信息的库函数，及libcap-dev sudo apt-get install elfutils sudo apt-get install libcap-dev sudo apt-get install systemtap 测试 stap-prep 出现如下错误You need package linux-image-4.4.0-47-generic-dbgsym but it does not seem to be available 由于发行版的内核默认无内核调试信息，所以需要一个调试内核镜像 wget http://ddebs.ubuntu.com/pool/main/l/linux/linux-image-4.4.0-47-generic-dbgsym_4.4.0-47.68_amd64.ddeb dpkg -i linux-image-4.4.0-47-generic-dbgsym_4.4.0-47.68_amd64.ddeb 查询内核版本的方法cat /proc/version_signature或者uname -r 执行测试stap -ve &#39;probe begin { log(&quot;hello world&quot;) exit() }&#39; 没有出现错误，表示成功.-v 打印编译阶段的详细信息 systemtap原理SystemTap采用其他的内核框架做源：静态指针使用tracepoints,动态指针使用Kprobe。用户级别的使用uprobes。 Kprobes 从 2.6.9 版本开始就添加到主流的 Linux 内核中。它提供一些不同的服务，但最重要的两种服务是 Kprobe 和 Kretprobe。 Kprobe 特定于架构，它在需要检查的指令的第一个字节中插入一个断点指令。当调用该指令时，将执行针对探针的特定处理函数。执行完成之后，接着执行原始的指令（从断点开始）。 断点指令(breakpoint instruction)：__asm INT 3，机器码为CC。断点中断(INT3)是一种软中断，当执行到INT 3指令时，CPU会把当时的程序指针(CS和EIP)压入堆栈保存起来，然后通过中断向量表调用INT 3所对应的中断例程。INT是软中断指令，中断向量表是中断号和中断处理函数地址的对应表。INT 3即触发软中断3，相应的中断处理函数的地址为：中断向量表地址 + 4 * 3。 Kretprobes 有所不同，它操作调用函数的返回结果。注意，因为一个函数可能有多个返回点，所以听起来事情有些复杂。不过，它实际使用一种称为 trampoline 的简单技术。您将向函数条目添加一小段代码，而不是检查函数中的每个返回点。这段代码使用 trampoline 地址替换堆栈上的返回地址 （Kretprobe 地址）。当该函数存在时，它没有返回到调用方，而是调用 Kretprobe（执行它的功能），然后从 Kretprobe 返回到实际的调用方。 stap 实用程序将 stap 脚本转换成提供探针行为的内核模块。 SystemTap 脚本查看一个脚本，nethist.stp 123456789101112131415161718global histogramprobe begin &#123; printf(&quot;Capturing...\\n&quot;)&#125;probe netdev.receive &#123; histogram &lt;&lt;&lt; length&#125;probe netdev.transmit &#123; histogram &lt;&lt;&lt; length&#125;probe end &#123; printf( &quot;\\n&quot; ) print( @hist_log(histogram) )&#125; 运行stap nethist.stp 用probe指定一个探测点(probe-point)(探针)，以及在这个探测点处执行的处理函数 常用探针类型 探针类型 说明 begin 在脚本开始时触发 end 在脚本结束时触发 kernel.function(“sys_sync”) 调用 sys_sync 时触发 kernel.function(“sys_sync”).call 同上 kernel.function(“sys_sync”).return 返回 sys_sync 时触发 kernel.syscall.* 进行任何系统调用时触发 kernel.function(“*@kernel/fork.c:934”) 到达 fork.c 的第 934 行时触发 module(“ext3”).function(“ext3_file_write”) 调用 ext3 write 函数时触发 timer.jiffies(1000) 每隔 1000 个内核 jiffy 触发一次 timer.ms(200).randomize(50) 每隔 200 毫秒触发一次，带有线性分布的随机附加时间（-50 到 +50） 探测点的一般语法形式kernel.function(PATTERN)kernel.function(PATTERN).callkernel.function(PATTERN).return 缺少后缀时意味着探测函数的进入点，默认为call。 .function 使探针定位在命名函数的开始之处，因此探针可用上下文变量的方式来获取函数参数。.return 让探针定位到命名函数返回的那一时刻.call 调用时触发,默认。.statement,使探针探测到确切的代码行例如：kernel.statement(&quot;bio_init@fs/bio.c+3&quot;)//引用文件fs/bio.c 内bio_init+3 这一行语句 PATTERN 是一个字符串字面值，它标识程序中的代码点。它由3 部分构成。 第一部分是函数名字，可使用星号和问号通配符来匹配多个函数名字。 第二部分是可选的，它以@ 字符开头，紧跟着此函数所在源文件的路径。默认为为从Linux 代码树顶层目录开始的相对路径。 如果给定文件名，第三部仍是可选的。它以“: ”或“+ ”开头，用来标识源文件的行号。 ”:” 后面跟的是绝对行号”+” 后面跟的是函数入口的相对行号”:*” 匹配函数的每一行”:x-y” 可以从x 行匹配到y行 内置变量 column column execname() uid ##内置函数 名称 说明 cpu() 执行当前进程的CPU number execname() 返回当前执行的进程名 tid pid() 当前进程的ID uid() 当前进程的user id stp_pid() pp() thread_indent() probefunc() 返回被探测的函数名 probemod() print_backtrace() 打印进程堆栈回朔方法 strlen() substr() isinstr() strtol() get_cycles() gettimeofday_s() gettimeofday_ns() 当前时间，自启动以来的纳秒数 target() task_current() 指向当前线程内核结构的指针 语法##基本格式 变量属于弱数据类型，不用事先声明，不用指定数据类型。probe-handler中定义的变量是局部的，不能在其它探测点处理函数中使用。global符号用于定义全局变量。 String连接符是“.”，比较符为“==”。next语句：执行到next语句时，会马上从探测点处理函数中返回。 变量的引用$varname // 引用变量varname$var-&gt;field // 引用结构的成员变量$var[N] // 引用数组的元素&amp;$var // 变量的地址另外的风格@var(“varname”) // 引用变量varname@var(“var@src/file.c”) // 引用src/file.c在被编译时的全局变量varname@var(“varname@file.c“)-&gt;field // 引用结构的成员变量@var(“var@file.c“)[N] // 引用数组的元素&amp;@var(“var@file.c“) // 变量的地址 关联数组关联数组是用哈希表实现的，最大大小在一开始就设定了。关联数组必须是全局的，不能在探测点处理函数内部定义。数组的索引最多可以有9个，用逗号隔开，可以是数字或字符串。元素的数据类型有三种：数值、字符串、统计类型。例如：foo[4, “hello”]++ 统计类型操作符“&lt;&lt;&lt;”例如：g_value &lt;&lt;&lt; b # 相当于C语言的g_value += b @count(g_value)：所有统计操作的操作次数@sum(g_value)：所有统计操作的操作数的总和@min(g_value)：所有统计操作的操作数的最小值@max(g_value)：所有统计操作的操作数的最大值@avg(g_value)：所有统计操作的操作数的平均值 SystemTap其他用法获取stap命令行参数假定该脚本的名字为example.stpprobe begin { printf(“%d, %s/n”, $1, @2) }运行如下：stap example.stp 10 mystring那么，$1 会被替换成10 ，而@2 会被替换成”mystring” ，结果输出：10, mystring $1 … $ 将参数转换成整数@1 … @ 将参数转换成字符串 查找匹配的内核函数和变量查找所有的系统调用stap -l &#39;syscall.*&#39;stap -L syscall.read.return open系统调用在内核的实现stap -l &#39;kernel.function(&quot;sys_open&quot;)&#39;kernel.function(&quot;SyS_open@/build/linux-xHzv4a/linux-4.4.0/fs/open.c:1038&quot;) 查找名字中包含nit的内核函数：stap -l &#39;kernel.function(&quot;*nit*&quot;)&#39; 查看执行到这个探测点时，哪些上下文变量是可用的stap -L &#39;kernel.function(&quot;vfs_write&quot;)&#39;stap -L &#39;netdev.transmit&#39; 官方提供的例子参考https://sourceware.org/systemtap/wiki/WarStories 参考 SystemTap_Beginners_Guide 内核调试神器SystemTap — 探测点与语法（二） Linux 自检和 SystemTap System语言详解——1.SystemTap概述","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"常用Linux系统调用","slug":"018linux/常用Linux系统调用","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.059Z","comments":true,"path":"category/018linux/常用Linux系统调用.html","link":"","permalink":"http://yoursite.com/category/018linux/常用Linux系统调用.html","excerpt":"","text":"#系统调用简介 linux内核中设置了一组用于实现系统功能的子程序，称为系统调用。系统调用和普通库函数调用非常相似，只是系统调用由操作系统核心提供，运行于核心态，而普通的函数调用由函数库或用户自己提供，运行于用户态。 #系统调用汇总 linux很多监控／调试命令如strace都需要知道系统调用的名字，因此摘录于此，供查询方便。 进程控制 名称 描叙 fork 创建一个新进程 clone 按指定条件创建子进程 execve 运行可执行文件 exit 中止进程 _exit 立即中止当前进程 getdtablesize 进程所能打开的最大文件数 getpgid 获取指定进程组标识号 setpgid 设置指定进程组标志号 getpgrp 获取当前进程组标识号 setpgrp 设置当前进程组标志号 getpid 获取进程标识号 getppid 获取父进程标识号 getpriority 获取调度优先级 setpriority 设置调度优先级 modify_ldt 读写进程的本地描述表 nanosleep 使进程睡眠指定的时间 nice 改变分时进程的优先级 pause 挂起进程，等待信号 personality 设置进程运行域 prctl 对进程进行特定操作 ptrace 进程跟踪 sched_get_priority_max 取得静态优先级的上限 sched_get_priority_min 取得静态优先级的下限 sched_getparam 取得进程的调度参数 sched_getscheduler 取得指定进程的调度策略 sched_rr_get_interval 取得按RR算法调度的实时进程的时间片长度 sched_setparam 设置进程的调度参数 sched_setscheduler 设置指定进程的调度策略和参数 sched_yield 进程主动让出处理器,并将自己等候调度队列队尾 vfork 创建一个子进程，以供执行新程序，常与execve等同时使用 wait 等待子进程终止 wait3 参见wait waitpid 等待指定子进程终止 wait4 参见waitpid capget 获取进程权限 capset 设置进程权限 getsid 获取会晤标识号 setsid 设置会晤标识号 文件系统 名称 描叙 文件读写操作 fcntl 文件控制 open 打开文件 creat 创建新文件 close 关闭文件描述字 read 读文件 write 写文件 readv 从文件读入数据到缓冲数组中 writev 将缓冲数组里的数据写入文件 pread 对文件随机读 pwrite 对文件随机写 lseek 移动文件指针 _llseek 在64位地址空间里移动文件指针 dup 复制已打开的文件描述字 dup2 按指定条件复制文件描述字 flock 文件加/解锁 poll I/O多路转换 truncate 截断文件 ftruncate 参见truncate umask 设置文件权限掩码 fsync 把文件在内存中的部分写回磁盘 文件系统操作 access 确定文件的可存取性 chdir 改变当前工作目录 fchdir 参见chdir chmod 改变文件方式 fchmod 参见chmod chown 改变文件的属主或用户组 fchown 参见chown lchown 参见chown chroot 改变根目录 stat 取文件状态信息 lstat 参见stat fstat 参见stat statfs 取文件系统信息 fstatfs 参见statfs readdir 读取目录项 getdents 读取目录项 mkdir 创建目录 mknod 创建索引节点 rmdir 删除目录 rename 文件改名 link 创建链接 symlink 创建符号链接 unlink 删除链接 readlink 读符号链接的值 mount 安装文件系统 umount 卸下文件系统 ustat 取文件系统信息 utime 改变文件的访问修改时间 utimes 参见utime quotactl 控制磁盘配额 系统控制 名称 描叙 ioctl I/O总控制函数 _sysctl 读/写系统参数 acct 启用或禁止进程记账 getrlimit 获取系统资源上限 setrlimit 设置系统资源上限 getrusage 获取系统资源使用情况 uselib 选择要使用的二进制函数库 ioperm 设置端口I/O权限 iopl 改变进程I/O权限级别 outb 低级端口操作 reboot 重新启动 swapon 打开交换文件和设备 swapoff 关闭交换文件和设备 bdflush 控制bdflush守护进程 sysfs 取核心支持的文件系统类型 sysinfo 取得系统信息 adjtimex 调整系统时钟 alarm 设置进程的闹钟 getitimer 获取计时器值 setitimer 设置计时器值 gettimeofday 取时间和时区 settimeofday 设置时间和时区 stime 设置系统日期和时间 time 取得系统时间 times 取进程运行时间 uname 获取当前UNIX系统的名称、版本和主机等信息 vhangup 挂起当前终端 nfsservctl 对NFS守护进程进行控制 vm86 进入模拟8086模式 create_module 创建可装载的模块项 delete_module 删除可装载的模块项 init_module 初始化模块 query_module 查询模块信息 *get_kernel_syms 取得核心符号,已被query_module代替 restart_syscall 调用进入未完的那个系统调用 内存管理 名称 描叙 brk 改变数据段空间的分配 sbrk 参见brk mlock 内存页面加锁 munlock 内存页面解锁 mlockall 调用进程所有内存页面加锁 munlockall 调用进程所有内存页面解锁 mmap 映射虚拟内存页 munmap 去除内存页映射 mremap 重新映射虚拟内存地址 msync 将映射内存中的数据写回磁盘 mprotect 设置内存映像保护 getpagesize 获取页面大小 sync 将内存缓冲区数据写回硬盘 cacheflush 将指定缓冲区中的内容写回磁盘 网络管理 名称 描叙 getdomainname 取域名 setdomainname 设置域名 gethostid 获取主机标识号 sethostid 设置主机标识号 gethostname 获取本主机名称 sethostname 设置主机名称 socket控制 名称 描叙 socketcall socket系统调用 socket 建立socket bind 绑定socket到端口 connect 连接远程主机 accept 响应socket连接请求 send 通过socket发送信息 sendto 发送UDP信息 sendmsg 参见send recv 通过socket接收信息 recvfrom 接收UDP信息 recvmsg 参见recv listen 监听socket端口 select 对多路同步I/O进行轮询 shutdown 关闭socket上的连接 getsockname 取得本地socket名字 getpeername 获取通信对方的socket名字 getsockopt 取端口设置 setsockopt 设置端口参数 sendfile 在文件或端口间传输数据 socketpair 创建一对已联接的无名socket 用户管理 名称 描叙 getuid 获取用户标识号 setuid 设置用户标志号 getgid 获取组标识号 setgid 设置组标志号 getegid 获取有效组标识号 setegid 设置有效组标识号 geteuid 获取有效用户标识号 seteuid 设置有效用户标识号 setregid 分别设置真实和有效的的组标识号 setreuid 分别设置真实和有效的用户标识号 getresgid 分别获取真实的,有效的和保存过的组标识号 setresgid 分别设置真实的,有效的和保存过的组标识号 getresuid 分别获取真实的,有效的和保存过的用户标识号 setresuid 分别设置真实的,有效的和保存过的用户标识号 setfsgid 设置文件系统检查时使用的组标识号 setfsuid 设置文件系统检查时使用的用户标识号 getgroups 获取后补组标志清单 setgroups 设置后补组标志清单 进程间通信 名称 描叙 ipc 进程间通信总控制调用 信号 sigaction 设置对指定信号的处理方法 sigprocmask 根据参数对信号集中的信号执行阻塞/解除阻塞等操作 sigpending 为指定的被阻塞信号设置队列 sigsuspend 挂起进程等待特定信号 signal 参见signal kill 向进程或进程组发信号 *sigblock 向被阻塞信号掩码中添加信号,已被sigprocmask代替 *siggetmask 取得现有阻塞信号掩码,已被sigprocmask代替 *sigsetmask 用给定信号掩码替换现有阻塞信号掩码,已被sigprocmask代替 *sigmask 将给定的信号转化为掩码,已被sigprocmask代替 *sigpause 作用同sigsuspend,已被sigsuspend代替 sigvec 为兼容BSD而设的信号处理函数,作用类似sigaction ssetmask ANSI 消息 msgctl 消息控制操作 msgget 获取消息队列 msgsnd 发消息 msgrcv 取消息 管道 pipe 创建管道 信号量 semctl 信号量控制 semget 获取一组信号量 semop 信号量操作 共享内存 shmctl 控制共享内存 shmget 获取共享内存 shmat 连接共享内存 shmdt 拆卸共享内存 锁 futex fast userspace mutex,快速用户空间互斥体 compat_futex 参考列出所有的系统调用，可以使用systemtap stap -l &#39;syscall.*&#39; 本文摘录于http://www.ibm.com/developerworks/cn/linux/kernel/syscall/part1/appendix.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"tcp建立连接的状态说明","slug":"010基础技术/分布式理论/tcp建立连接的状态说明","date":"2016-11-06T00:00:00.000Z","updated":"2018-12-10T16:34:07.060Z","comments":true,"path":"category/010基础技术/分布式理论/tcp建立连接的状态说明.html","link":"","permalink":"http://yoursite.com/category/010基础技术/分布式理论/tcp建立连接的状态说明.html","excerpt":"","text":"#tcp建立连接的状态说明 TCP连接建立首先要说明的是要明确TCP连接建立的过程需要3次握手，下面举例说明各种状态存在的时刻： 首先在服务器A上开启FTP服务，开始侦听来自远端TCP端口的连接请求，这个时候查看服务器A状态为：LISTENING 在客户端B上向A发送FTP连接请求，这个时候数据包同步位置1，这是TCP三次握手的第一步。在发送后没收到确认时，在客户端B上其状态为：SYN-SENT。此时客户端B启动连接定时器。如果在75秒内没有收到应答，则放弃连接建立。 在服务器A上收到从B上发送的SYN同步包后，确认，然后再向B发送SYN的同步包，此数据包同时将TCP标记中的同步位和确认位置1，它既对第一步中的客户端同步数据包进行确认，表示愿意与客户端同步，同时再对客户端主机进行同步请求，这是TCP连接的第一步。这个时候在服务器A上，状态为：SYN-RECEIVED。此时服务器A启动连接定时器。如果在75秒内没有收到应答，则放弃连接建立。 在客户端B上接收到从A上发过来的确认同步包后进行确认，此数据包中将TCP标记中的确认位置1，表示这是一个确认数据包，此时在客户端B状态转换为：ESTABLISHED 服务器A接收到从B发过来的确认包后，状态转换为：ESTABLISHED此时TCP连接正式建立。TCP连接关闭 应用程序在在连接不需要的时候，通过客户端B向服务器A发送的终止信息的FIN包后，客户端B处于FIN-WAIT-1状态。 从服务器A接收到客户端B发送的终止数据包，它告诉客户端B已成功接收客户端的上数据包，此时等待应用程序来关闭连接，此时服务器A进入CLOSE_WAIT状态。 客户端B接收到带有确认位的数据包后，对此进行确认，同意关闭TCP连接此时客户端B转移到FIN-WAIT-2状态。当连接从FIN-WAIT-1状态转移到FIN-WAIT-2状态时，将一个FIN-WAIT-2定时器设置为10分钟。 服务器A在应用程序同意终止连接后，向客户端B发送终止FIN包，此时服务器状态转为LAST-ACT。 客户端B在接收到从服务器A发送的终止包后，同意终止连接，然后再向服务器端发送确认信息，此时客户端B转向TIME-WAIT状态。当连接进入TIME-WAIT状态时，该定时器被激活。 服务端A在收到客户端B的确认后，关闭连接，服务器A状态转向CLOSED。 客户端B在TIME-WAIT定时器超时时，与该连接相关的内核数据块被删除，连接终止，转向CLOSED状态。此时TCP连接正式关闭。","categories":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://yoursite.com/tags/TCP-IP/"}],"keywords":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}]},{"title":"系统的负载－uptime","slug":"018linux/uptime","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.058Z","comments":true,"path":"category/018linux/uptime.html","link":"","permalink":"http://yoursite.com/category/018linux/uptime.html","excerpt":"","text":"#uptime 这个命令可以快速查看机器的负载情况。在Linux系统中，这些数据表示等待CPU资源的进程和阻塞在不可中断IO进程（进程状态为D）的数量。这些数据可以让我们对系统资源使用有一个宏观的了解。 命令的输出分别表示1分钟、5分钟、15分钟的平均负载情况。通过这三个数据，可以了解服务器负载是在趋于紧张还是趋于缓解。如果1分钟平均负载很高，而15分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查CPU资源都消耗在了哪里。反之，如果15分钟平均负载很高，1分钟平均负载较低，则有可能是CPU资源紧张时刻已经过去。 如果负载值大于CPU数，这可能意味着CPU饱和了，或线程遭受调度延迟，也可能有磁盘IO的因素，使用其他工具进一步调查。 只要cpu的当前活动线程不大于3，我们认为它的负载时正常的。如果大于5，负载就非常高了","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"java并发机制和内存模型总结","slug":"022java/thread/java并发机制和内存模型总结","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:44:52.936Z","comments":true,"path":"category/022java/thread/java并发机制和内存模型总结.html","link":"","permalink":"http://yoursite.com/category/022java/thread/java并发机制和内存模型总结.html","excerpt":"","text":"术语临界区临界区表示一种公共资源或者共享数据。每一刻只能被一个线程使用，如果临界区资源被占用，其他线程想使用这个资源，就必须等待。 CASCompare and Swap，比较并设置。 用于在硬件层面上提供原子性操作。在Intel 处理器中，比较并交换通过指令cmpxchg实现。比较是否和给定的数值一致，如果一致则修改，不一致则不修改。 并发编程的挑战并发编程的目的是为了让程序运行得更快，但是，并不是启动更多的线程就能让程序最大限度地并发执行。 线程有创建和上下文切换的开销、死锁的问题，以及受限于硬件和软件的资源限制。 amdah1定律：多核cpu优化的效果取决于cpu数量和系统中串行化程序的比重。只提供cpu数量也无法提高系统性能。 #java中并发编程模型 Java中所使用的并发机制依赖于JVM的实现和CPU的指令。 在并发编程中，需要处理两个关键问题：线程之间如何通信及线程之间如何同步。 通信是指线程之间以何种机制来交换信息。线程之间的通信机制有两种：共享内存和消息传递。 Java的并发采用的是共享内存模型。由Java内存模型（本文简称为JMM）控制。 在共享内存的并发模型里，线程之间共享程序的公共状态，通过写-读内存中的公共状态进行隐式通信。 同步是指程序中用于控制不同线程间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 操作系统层面：为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作。 现代的处理器使用写缓冲区临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，减少对内存总线的占用。 虽然写缓冲区有很多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。 如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。 如何解决其他处理器缓存的值还是旧的？ 为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议。每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了。 操作系统层面：在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。指令重排,同时带来了乱序的问题。在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 为了保证内存可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。 JMM总结JMM从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的Local内存（Local Memory），本地内存中存储了该线程以读/写共享变量的副本。 本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 共享变量：在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享。这些变量统称为共享变量。 JMM关键技术点JMM遵循的基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都可以。 JMM的关键技术点主要为多线程的原子性／可见性／有序性。 原子性处理器如何实现原子操作32位IA-32处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作 Java如何实现原子操作在Java中可以通过锁和循环CAS的方式来实现原子操作。 从Java 1.5开始，JDK的并发包里提供了一些类来支持原子操作，如AtomicBoolean（用原子方式更新的boolean值）、AtomicInteger（用原子方式更新的int值）和AtomicLong（用原子方式更新的long值） 使用循环CAS实现原子操作JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止 使用锁机制实现原子操作锁机制保证了只有获得锁的线程才能够操作锁定的内存区域。JVM内部实现了很多种锁机制，有偏向锁、轻量级锁和互斥锁。有意思的是除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。 可见性JMM通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证。 有序性在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。指令重排,同时带来了乱序的问题。 JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障（Memory Barriers，Intel称之为Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。 那些指令不能进行重排？ happen-before规则。 #常用同步方式 synchronized、volatile、Lock volatile在多线程并发编程中synchronized和volatile都扮演着重要的角色，volatile可以认为是轻量级的synchronized。它在多处理器开发中保证了共享变量的“可见性”。 Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 一个volatile变量的单个读/写操作，与一个普通变量的读/写操作都是使用同一个锁来同步，它们之间的执行效果相同。 volatile的定义与实现原理Java语言规范第3版中对volatile的定义如下：Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 volatile读的内存语义：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 volatile读的内存语义：当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。例如在每个volatile写操作的前面插入一个StoreStore屏障。 为了提供一种比锁更轻量级的线程之间通信的机制，JSR-133专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序，确保volatile的写-读和锁的释放-获取具有相同的内存语义。 ##Lock对象 volatile仅仅保证对单个volatile变量的读/写具有原子性，而锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性。 内存内存语义线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。·线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享变量所做修改的）消息。 代码12345678910111213141516171819202122232425import java.util.concurrent.locks.ReentrantLock;class ReentrantLockExample &#123; int a = 0; ReentrantLock lock = new ReentrantLock(); public void writer() &#123; lock.lock(); // try &#123; a++; &#125; finally &#123; lock.unlock(); // &#125; &#125; public void reader() &#123; lock.lock(); // try &#123; int i = a; // &#125; finally &#123; lock.unlock(); // &#125; &#125;&#125; 在ReentrantLock中，调用lock()方法获取锁；调用unlock()方法释放锁。内部使用整型的volatile变量（命名为state）来维护同步状态。 整型的volatile变量（命名为state）来维护同步状态。加锁方法首先读volatile变量state。释放锁的最后写volatile变量state。 concurrent包通用化的实现模式首先，声明共享变量为volatile。然后，使用CAS的原子条件更新来实现线程之间的同步。同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。 synchronizedVS locksynchronized是托管给JVM执行的，而lock是Java写的控制锁的代码。 synchronized称为“重量级锁”,在1.5中，synchronize是性能低效的。导致有可能加锁消耗的系统时间比加锁以外的操作还多。到了Java1.6发生了变化。synchronize在语义上很清晰，可以进行很多优化（为了减少上下文切换），有适应自旋，锁消除，锁粗化，轻量级锁，偏向锁等等。导致在Java1.6上synchronize的性能并不比Lock差。 synchronized原始采用的是CPU悲观锁机制，即线程获得的是独占锁。独占锁意味着其他线程只能依靠阻塞来等待线程释放锁，而在CPU转换线程阻塞时会引起线程上下文切换。 Lock用的是乐观锁方式。每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。获得锁的方法是compareAndSetState，就是调用的CPU提供的特殊指令。 Synchonized在JVM里的实现原理synchronized实现同步的基础：Java中的每一个对象都可以作为锁。 代码块同步是使用monitorenter和monitorexit指令实现monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处。 任何对象都有一个monitor与之关联，当一个monitor被持有后，它将处于锁定状态。 模型介绍happen-before规则顺序一致性内存模型参考 各种 Java Thread State 第一分析法则","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/tags/高并发/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"监控IO使用－iostat命令","slug":"018linux/iostat","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.055Z","comments":true,"path":"category/018linux/iostat.html","link":"","permalink":"http://yoursite.com/category/018linux/iostat.html","excerpt":"","text":"#iostat iostat方便查看CPU、网卡、tty设备、磁盘、CD-ROM 等等设备的活动情况, 负载信息。如果没有安装，需要sudo apt-get install sysstat 基本使用iostat[参数][时间][次数] iostat显示所有设备负载情况 iostat 2 3 字段描述iostat -x 12345678Linux 4.4.0-53-generic (ubuntu) 12/15/2016 _x86_64_ (3 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.33 0.00 0.36 0.12 0.00 99.19Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.08 3.01 20.35 3.14 231.60 31400.10 2692.38 0.01 0.45 0.37 0.96 0.19 0.45scd0 0.00 0.00 0.01 0.00 0.03 0.00 8.00 0.00 0.31 0.31 0.00 0.31 0.00 cpu 字段 说明 %user CPU处在用户模式下的时间百分比。 %nice CPU处在带NICE值的用户模式下的时间百分比。 %system CPU处在系统模式下的时间百分比。 %iowait CPU等待输入输出完成时间的百分比。 %steal 管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。 %idle CPU空闲时间百分比。 如果%iowait的值过高，表示硬盘存在I/O瓶颈%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。%idle值如果持续低于10，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。 Device 字段 说明 rrqm/s 每秒进行 merge 的读操作数目。即 rmerge/s wrqm/s 每秒进行 merge 的写操作数目。即 wmerge/s r/s 每秒完成的读 I/O 设备次数。即 rio/s w/s 每秒完成的写 I/O 设备次数。即 wio/s rsec/s 每秒读扇区数。即 rsect/s wsec/s 每秒写扇区数。即 wsect/s rkB/s 每秒读K字节数。是 rsect/s 的一半，因为每扇区大小为512字节。 wkB/s 每秒写K字节数。是 wsect/s 的一半。 avgrq-sz 平均每次设备I/O操作的数据大小 (扇区)。 avgqu-sz 平均I/O队列长度(分配给设备的平均请求数)。大于1表示设备已经饱和了。（不过有些设备可以并行处理请求，比如由多个磁盘组成的虚拟设备） await 平均每次设备I/O操作的等待时间 (毫秒)。包括了队列时间和服务时间 svctm 平均每次设备I/O操作的服务时间 (毫秒)。 %util 一秒中有百分之多少的时间用于 I/O 操作，即被io消耗的cpu百分比 如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。可以结合vmstat 查看查看b参数(等待资源的进程数)和wa参数(IO等待所占用的CPU时间的百分比，高过30%时IO压力高)。%util低于60%通常是低性能的表现如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明I/O 队列太长，io响应太慢，则需要进行必要优化。如果avgqu-sz比较大，也表示有大量io在等待。 公式：设备IO操作:总IO(io)/s = r/s(读) +w/s(写)当前硬盘的超载比率： avgqu-sz /(r/s+r/w) 100处理的请求数/s＝(r/s+r/w)await / 1000ms util 表示I/O利用率，在统计时间内所有处理IO时间，除以总共统计时间。例如，如果统计间隔1秒，该设备有0.8秒在处理IO，而0.2秒闲置，那么该设备的%util = 0.8/1 = 80%，所以该参数暗示了设备的繁忙程度 监控磁盘使用率有两个目的。第一个目的与应用本身有关：如果应用正在做大量的磁盘 I/O 操作，那 I/O 就很容易成为瓶颈监控磁盘使用率的第二个理由是——即便预计应用不会有很高的 I/O——有助于监控系统是否在进行内存交换。正在内存交换的系统——从主内存移动数据到磁盘或者反过来——一般来说，性能比较差。还有其他系统工具可以报告系统交换，例如 vmstat 输出中有两列（si 是换进，so 是换出）可以警告我们系统是否正在交换。磁盘活动说明内存交换可能正在发生。 参数 参数 说明 -C 显示CPU使用情况 -d 显示磁盘使用情况 -k 以 KB 为单位显示 -m 以 M 为单位显示 -N 显示磁盘阵列(LVM) -n 显示NFS -p[磁盘] 显示磁盘和分区的情况 -t 显示终端和CPU的信息 -x 显示详细信息 -V 显示版本信息 用法举例查看TPS和吞吐量iostat -d -k 1 1 123Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 18.62 183.15 24832.22 403119 54655472scd0 0.01 0.02 0.00 52 0 字段 说明 tps 该设备每秒的传输次数,一次传输意思是一次I/O请求。多个逻辑请求可能会被合并为“一次I/O请求”。“一次传输”请求的大小是未知的。 kB_read/s 每秒从设备（drive expressed）读取的数据量； kB_wrtn/s 每秒向设备（drive expressed）写入的数据量； kB_read 读取的总数据量；kB_wrtn 查看设备使用率（%util）和响应时间（await）iostat -d -x -k 1 1 ##查看某个磁盘的使用情况iostat -dmx 1 2 /dev/sda iostat -x sda sdb 2 6 #每隔2秒显示6次sda和sdb的扩展统计$ iostat -p sda 2 6 #每隔2秒显示6次sda和它的分区的统计","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"参看线程堆栈－jstack","slug":"022java/jvm/参看线程堆栈－jstack","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:46:04.258Z","comments":true,"path":"category/022java/jvm/参看线程堆栈－jstack.html","link":"","permalink":"http://yoursite.com/category/022java/jvm/参看线程堆栈－jstack.html","excerpt":"","text":"#jstack介绍 jstack用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。 另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 命令介绍基本用法jstack -l 1086 &gt; /tmp/t.txt 基本参数-F当’jstack [-l] pid’没有相应的时候强制打印栈信息-l长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表。-m打印java和native c/c++框架的所有栈信息。-h | -help打印帮助信息。pid 需要被打印配置信息的java进程id,可以用jps查询。 常见的jstack分析步骤1.linux下使用top或者pidstat命令查看进程消耗情况 假设进程号为26147，（通过 pidstat -p 26147 -t -u 1 找到线程号26160 )，也可以通过top －H 2. 3.进行分析printf %x 26160 得到311f 4.jstack命令定位代码 jstack 26147 | grep 6630 -A 50 也可以先输出到文件，然后进行分析or jstack -l 26147 &gt; t.txtgrep java.lang.Thread.State t.txt | awk &#39;{print $2$3$4$5}&#39; | sort | uniq -c 123456789101112131415161718192021&quot;Thread-0&quot; #8 prio=5 os_prio=0 tid=0x00007f64800d9000 nid=0x709f runnable [0x00007f646b039000] java.lang.Thread.State: RUNNABLE at java.io.FileOutputStream.writeBytes(Native Method) at java.io.FileOutputStream.write(FileOutputStream.java:326) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) - locked &lt;0x00000000e0c13c60&gt; (a java.io.BufferedOutputStream) at java.io.PrintStream.write(PrintStream.java:482) - locked &lt;0x00000000e0c08200&gt; (a java.io.PrintStream) at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221) at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291) at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:104) - locked &lt;0x00000000e0c081a8&gt; (a java.io.OutputStreamWriter) at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:185) at java.io.PrintStream.write(PrintStream.java:527) - eliminated &lt;0x00000000e0c08200&gt; (a java.io.PrintStream) at java.io.PrintStream.print(PrintStream.java:669) at java.io.PrintStream.println(PrintStream.java:806) - locked &lt;0x00000000e0c08200&gt; (a java.io.PrintStream) at CpuAll.run(CpuAll.java:15) at java.lang.Thread.run(Thread.java:745) 如果判断CpuAll.run方法有问题。 使用jstack进行分析使用jstack进行分析，需要了解线程的各种状态，可以参考线程基本学习 线程状态java.lang.Thread.State枚举类中定义了如下几种类型： NEW：线程创建尚未启动。 RUNNABLE：包括操作系统线程状态中的Ready和Running，可能在等待时间片或者正在执行。 BLOCKED：线程被阻塞。 WAITING：不会分配CPU执行时间，直到别的线程显式的唤醒，否则无限期等待。LockSupport.park()，没有设置Timeout参数的Object.wait()和Thread.join()，会导致此现象。 TIMED_WAITING：不会分配CPU执行时间，直到系统自动唤醒，不需要别的线程显示唤醒。Thread.sleep()，LockSupport.parkNanos()，LockSupport.parkUntil()，设置了超时时间的Object.wait()和Thread.join()，会让线程进入有限期等待。 TERMINATED：线程执行结束 阻塞状态与等待状态的区别是：阻塞状态的线程是在等待一个排它锁，直到别的线程释放该排它锁，该线程获取到该锁才能退出阻塞状态； 而等待状态的线程则是等待一段时间，由系统唤醒或者别的线程唤醒，该线程便退出等待状态。 线程状态转化图如下： 文件分析生成的dump文件的内容如下 123456789101112131415161718192021222324&quot;Thread-0&quot; #8 prio=5 os_prio=0 tid=0x00007fb8a80d9000 nid=0x312c runnable [0x00007fb89222b000] java.lang.Thread.State: RUNNABLE at java.io.FileOutputStream.writeBytes(Native Method) at java.io.FileOutputStream.write(FileOutputStream.java:326) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) - locked &lt;0x00000000e0c17848&gt; (a java.io.BufferedOutputStream) at java.io.PrintStream.write(PrintStream.java:482) - locked &lt;0x00000000e0c04920&gt; (a java.io.PrintStream) at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221) at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291) at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:104) - locked &lt;0x00000000e0c048c8&gt; (a java.io.OutputStreamWriter) at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:185) at java.io.PrintStream.write(PrintStream.java:527) - eliminated &lt;0x00000000e0c04920&gt; (a java.io.PrintStream) at java.io.PrintStream.print(PrintStream.java:669) at java.io.PrintStream.println(PrintStream.java:806) - locked &lt;0x00000000e0c04920&gt; (a java.io.PrintStream) at CpuAll.run(CpuAll.java:15) at java.lang.Thread.run(Thread.java:745) Locked ownable synchronizers: - None 12345&quot;Service Thread&quot; #7 daemon prio=9 os_prio=0 tid=0x00007fb8a80bb800 nid=0x312a runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE Locked ownable synchronizers: - None Service Thread：线程的名字daemon：守护线程prio=10：线程的优先级（默认是5）tid：Java的线程Id（线程在当前虚拟机中的唯一标识）nid：线程本地标识runnable：线程的状态[0x00007fb89222b000]：当前运行的线程在堆中的地址范围 #常见情况如下 线程状态为“waiting for monitor entry”意味着它 在等待进入一个临界区 ，所以它在”Entry Set“队列中等待。此时线程状态一般都是 Blocked：java.lang.Thread.State: BLOCKED (on object monitor) 线程状态为“waiting on condition”说明它在等待另一个条件的发生，来把自己唤醒，或者干脆它是调用了 sleep(N)。此时线程状态大致为以下几种：java.lang.Thread.State: WAITING (parking)：一直等那个条件发生；java.lang.Thread.State: TIMED_WAITING (parking或sleeping)：定时的，那个条件不到来，也将定时唤醒自己。 大量线程在“waiting for monitor entry”可能是一个全局锁阻塞住了大量线程。如果短时间内打印的 thread dump 文件反映，随着时间流逝，waiting for monitor entry 的线程越来越多，没有减少的趋势，可能意味着某些线程在临界区里呆的时间太长了，以至于越来越多新线程迟迟无法进入临界区。 大量线程在“waiting on condition”可能是它们又跑去获取第三方资源，尤其是第三方网络资源，迟迟获取不到Response，导致大量线程进入等待状态。所以如果你发现有大量的线程都处在 Wait on condition，从线程堆栈看，正等待网络读写，这可能是一个网络瓶颈的征兆，因为网络阻塞导致线程无法执行。 线程状态为“in Object.wait()”说明它获得了监视器之后，又调用了 java.lang.Object.wait() 方法。每个 Monitor在某个时刻，只能被一个线程拥有，该线程就是 “Active Thread”，而其它线程都是 “Waiting Thread”，分别在两个队列 “ Entry Set”和 “Wait Set”里面等候。在 “Entry Set”中等待的线程状态是 “Waiting for monitor entry”，而在 “Wait Set”中等待的线程状态是 “in Object.wait()”。当线程获得了 Monitor，如果发现线程继续运行的条件没有满足，它则调用对象（一般就是被 synchronized 的对象）的 wait() 方法，放弃了 Monitor，进入 “Wait Set”队列。此时线程状态大致为以下几种：java.lang.Thread.State: TIMED_WAITING (on object monitor)；java.lang.Thread.State: WAITING (on object monitor)； #其他常用命令 统计所有线程分别处于什么状态grep java.lang.Thread.State dump17 | awk &#39;{print $2$3$4$5}&#39; | sort | uniq -c 参考 各种 Java Thread State 第一分析法则 关于Java线程转储分析","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"AbstractQueuedSynchronizer学习","slug":"022java/thread/AQS","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-28T09:49:35.270Z","comments":true,"path":"category/022java/thread/AQS.html","link":"","permalink":"http://yoursite.com/category/022java/thread/AQS.html","excerpt":"","text":"AQS简介AbstractQueuedSynchronizer，抽象的队列式的同步器，简称AQS。AQS是Java并发框架的一个基础组件，java.util.concurrent并发包下很多类都是基于它实现的，如：ReentrantLock、ReentrantReadWriterLock、信号量、线程池等。 AQS有两种工作模式：独占模式和共享模式。AQS是独占锁,例如ReentrantLock中Sync的公共父类。AQS为共享锁，例如ReentrantReadWriteLock,Semaphore的Sync公共父类。 其结构图如下： 它的所有子类中，要么实现并使用了它独占功能的API，要么使用了共享锁的功能，而不会同时使用两套API。 以ReentrantReadWriterLock为例：如果一个线程(写线程)以独占模式(acquire)获取到这个锁，那么其他线程(读或写线程)不管以哪种模式尝试获取锁(共享模式是通过acquireShared获取的)，都会失败;如果一个线程(读线程)以共享模式获取到这个锁，那么其他读线程可以获取到锁，而写线程会获取失败。 api简介不响应中断的独占锁(acquire)响应中断的独占锁(acquireInterruptibly)不响应中断的共享锁(acquireShared)响应中断的共享锁(acquireSharedInterruptibly)独占锁的释放(release)共享锁的释放(releaseShared) 公平与非公平通过类FairSync，NonfairSync进行区分 AQS实现同步状态AbstractQueuedSynchronizer内置一个int32位字段state来保存同步状态，并暴露出getState、setState以及compareAndSet操作来读取和更新这个同步状态。其中属性state被声明为volatile，并且通过使用CAS指令来实现compareAndSetState。 子类通过继承同步器并需要实现它的方法来管理其状态，管理的方式就是通过类似acquire和release的方式来操纵状态。 ReentrantLock使用AQS的时候，state被用来表示锁被重入的次数；当Semaphore使用AQS的时候，state则被用来表示当前还有多少信号量可被获取。 同步队列管理AQS会对进行 acquire 而被阻塞的线程进行管理，其实就是内部维护了一个FIFO队列，这个队列是一个双向链表。链头可以理解为是一个空的节点，除了链头以外，每个节点内部都持有着一个线程。 acquire操作 f(尝试获取成功){ return； }else{ 加入等待队列;park自己 } releas if(尝试释放成功){ unpark等待队列中第一个节点 }else{ return false } 整体实现为了实现上述操作，需要下面三个基本组件： 1) 同步状态的原子性管理； 2) 线程的阻塞与解除阻塞； 3) 队列的管理； 用一个列表表示 组件 实现 同步状态 volatile int state 阻塞 LockSupport类 同步队列 Node节点 条件队列 ConditionObject 代码整体结构AQS同步队列的主要功能是将无法获得资源的线程放入同步队列中，进行等待，它是通过链表来实现的，每一个节点对应一个任务线程。在AbstractQueuedSynchronizer类中用静态内部类Node来作为链表的数据结构。 AQS参考了CLH锁的设计,但AQS没有采用CLH中的自旋来查看前驱（prev）节点的状态AQS中，队列节点是放在一个双向链表结构中的。 静态内部类Node类的主要属性： 123456789Node &#123; int waitStatus; //节点状态 Node prev; Node next; /** 因为条件只能是独占的，所以nextWaiter指向的是下一个等待该条件的Node。当然，在共享模式中，nextWaiter会被设置成一个特殊值：SHARED*/ Node nextWaiter; /** Node上绑定的线程，由构造函数传入，用完后需要set null */ Thread thread;&#125; Node的非静态属性，因为是双向链表，所以有prev和next，还有一个thread，用来记录该节点对应的线程，还有一个表示该节点状态的waitStatus，它有四种状态： SIGNAL，值为-1，表示当前节点的next节点需要获取资源数，也就是需要unparkCANCELLED，值为1，表示当前的线程被取消CONDITION，值为-2，表示当前节点在等待condition，也就是在condition队列中PROPAGATE，值为-3，表示当前场景下后续的acquireShared能够执行0，新建的Node的状态都是0，表示初始状态。 AbstractQueuedSynchronizer类拥有三个成员变量：sync队列的头结点head、sync队列的尾节点tail和状态state。 AQS中资源数（或者锁）的个数是放在state，private volatile int state; 对于锁的获取，请求形成节点，将其挂载在尾部，而锁资源的转移（释放再获取）是从头部开始向后进行。 tryAcquire通过返回一个boolean值来告诉调用者获取资源数是否成功，tryAcquireShared则返回的是个int型, 小于0,表示获取失败， 等于0,表示获取成功但后续还有其他获取将失败， 大于0,表示获取成功但后续的获取有可能成功。 代码细节先从一个例子看起,类似一个互斥锁的lock方法如下： 123public void lock() &#123; sync.acquire(1);&#125; acquire()acquire()在AQS中实现：12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 当前线程首先通过tryAcquire()尝试获取锁。获取成功的话，直接返回；尝试失败的话，进入到等待队列排序等待。tryAcquire由子类实现。 尝试获取锁失败，通过addWaiter(Node.EXCLUSIVE)来将当前线程加入到CLH队列，也就是线程等待队列。 调用acquireQueued()来获取锁。 当前线程在执行acquireQueued()时，会进入到CLH队列中休眠等待，直到获取锁了才返回！如果当前线程在休眠等待过程中被中断过，acquireQueued会返回true，此时当前线程会调用selfInterrupt()来自己给自己产生一个中断。 addWaiter源码12345678910111213141516private Node addWaiter(Node mode) &#123; // 新建一个Node节点，节点对应的线程是“当前线程”，“当前线程”的锁的模型是mode。 Node node = new Node(Thread.currentThread(), mode); Node pred = tail; // 若CLH队列不为空，则将“当前线程”添加到CLH队列末尾 if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 若CLH队列为空，则调用enq()新建CLH队列，然后再将“当前线程”添加到CLH队列中。 enq(node); return node;&#125; acquireQueued()源码acquireQueued()的作用就是逐步的去执行CLH队列的线程，如果当前线程获取到了锁，则返回；否则，当前线程进行休眠，直到唤醒并重新获取锁了才返回。 节点以“死循环”的方式获取同步状态。如果获取不到则阻塞节点中的线程，而被阻塞线程的唤醒主要依靠前驱节点的出队或阻塞线程被中断来实现。 12345678910111213141516171819202122232425 final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; // interrupted表示在CLH队列的调度中， // “当前线程”在休眠时，有没有被中断过。 boolean interrupted = false; for (;;) &#123; // 获取上一个节点。 // node是“当前线程”对应的节点，这里就意味着“获取上一个等待锁的线程”。 final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 在acquireQueued(final Node node,int arg)方法中，当前线程在“死循环”中尝试获取同步状态，而只有前驱节点是头节点，当前线程就调用tryAcquire尝试获取锁，如果获取成功就将头结点设置为当前结点，返回； 不是头结点的情况决定是否应该挂起，shouldParkAfterFailedAcquire返回“当前线程是否应该阻塞。 shouldParkAfterFailedAcquire()12345678910111213141516171819// ”private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; // 前继节点的状态 int ws = pred.waitStatus; // 如果前继节点是SIGNAL状态，则意味这当前线程需要被unpark唤醒。此时，返回true。 if (ws == Node.SIGNAL) return true; // 如果前继节点是“取消”状态，则设置 “当前节点”的 “当前前继节点” 为 “‘原前继节点’的前继节点”。 if (ws &gt; 0) &#123; do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 如果前继节点为“0”或者“共享锁”状态，则设置前继节点为SIGNAL状态。 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; CANCELLED：因为超时或者中断，结点会被设置为取消状态，被取消状态的结点不应该去竞争锁，只能保持取消状态不变，不能转换为其他状态。处于这种状态的结点会被踢出队列，被GC回收； SIGNAL：表示这个结点的继任结点被阻塞了，到时需要通知它； CONDITION：表示这个结点在条件队列中，因为等待某个条件而被阻塞； PROPAGATE：使用在共享模式头结点有可能牌处于这种状态，表示锁的下一次获取可以无条件传播；0：None of the above，新结点会处于这种状态。 该方法首先检查前趋结点的waitStatus位，如果为SIGNAL,表示前趋结点会通知它，那么它可以放心大胆地挂起了； 如果前趋结点是一个被取消的结点怎么办呢？那么就向前遍历跳过被取消的结点，直到找到一个没有被取消的结点为止，将找到的这个结点作为它的前趋结点，将找到的这个结点的waitStatus位设置为SIGNAL,返回false表示线程不应该被挂起。 parkAndCheckInterrupt上面的代码返回true,需要进行阻塞。则会调用parkAndCheckInterrupt()阻塞当前线程，直到当前先被唤醒才从parkAndCheckInterrupt()中返回。 123456private final boolean parkAndCheckInterrupt() &#123; // 通过LockSupport的park()阻塞“当前线程”。 LockSupport.park(this); // 返回线程的中断状态。 return Thread.interrupted();&#125; 线程被阻塞之后如何唤醒。一般有2种情况：第1种情况：unpark()唤醒。“前继节点对应的线程”使用完锁之后，通过unpark()方式唤醒当前线程。第2种情况：中断唤醒。其它线程通过interrupt()中断当前线程。 LockSupport中的park() 和 unpark() 的作用分别是阻塞线程和解除阻塞线程。Thread.interrupted()清除中断状态。 selfInterrupt()如果阻塞后，中断唤醒 123private static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125; “当前线程”自己产生一个中断。 在上面的acquireQueued()中，线程在阻塞状态被中断唤醒而获取到cpu执行权利；但是，如果该线程的前面还有其它等待锁的线程，根据公平性原则，该线程依然无法获取到锁。它会再次阻塞！ 该线程再次阻塞，直到该线程被它的前面等待锁的线程锁采用unpark()唤醒；线程才会获取锁，然后“真正执行起来”！ 在parkAndCheckInterrupt()中，线程的中断状态被清除了。 再看acquire()函数12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 先是通过tryAcquire()尝试获取锁。获取成功的话，直接返回；尝试失败的话，再通过acquireQueued()获取锁。 尝试失败的情况下，会先通过addWaiter()来将“当前线程”加入到”CLH队列”末尾；然后调用acquireQueued()，在CLH队列中排序等待获取锁，在此过程中，线程处于休眠状态。直到获取锁了才返回。 如果在休眠等待过程中被中断过，则调用selfInterrupt()来自己产生一个中断。 参考https://www.cnblogs.com/noahsark/p/sbstract_queued_synchronizer.htmlhttps://www.cnblogs.com/2015110615L/p/6754529.htmlhttp://ifeve.com/introduce-abstractqueuedsynchronizer/http://blog.csdn.net/sherld/article/details/42492259AbstractQueuedSynchronizer源码解析http://blog.csdn.net/yuenkin/article/details/50867530 AbstractQueuedSynchronizer(AQS)源码解析（一）http://www.tuicool.com/articles/INfEj23 http://manzhizhen.iteye.com/blog/2305890 Java多线程系列目录(共43篇)http://www.cnblogs.com/skywang12345/p/java_threads_category.html http://blog.csdn.net/yanlinwang/article/details/41172697","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"更新","slug":"更新","permalink":"http://yoursite.com/tags/更新/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"},{"name":"源码阅读","slug":"源码阅读","permalink":"http://yoursite.com/tags/源码阅读/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"ssh免秘密登录","slug":"018linux/ssh免秘密登录","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.057Z","comments":true,"path":"category/018linux/ssh免秘密登录.html","link":"","permalink":"http://yoursite.com/category/018linux/ssh免秘密登录.html","excerpt":"","text":"#ssh免秘密登录 公钥登录过程使用密码登录，每次都必须输入密码，非常麻烦。SSH还提供了公钥登录，过程如下：用户将自己的公钥储存在远程主机上。登录的时候，远程主机会向用户发送一段随机字符串，用户用自己的私钥加密后，再发回来。远程主机用事先储存的公钥进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求密码。 这种方法要求用户必须提供自己的公钥。可以直接用ssh-keygen生成一个。 使用ssh-keygen生成公钥与私钥对直接运行ssh-keygen 运行结束以后，在$HOME/.ssh/目录下，会新生成两个文件：id_rsa.pub和id_rsa。前者是你的公钥，后者是你的私钥。 使用ssh-copy-id拷贝公钥都远程机器将本机的公钥复制到远程机器的authorized_keys文件中从此使用ssh再登录，就不需要输入密码了。 ssh-copy-id原理远程主机将用户的公钥，保存在登录后的用户主目录的$HOME/.ssh/authorized_keys文件中。 可以等价如下命令ssh user@host &#39;mkdir -p .ssh &amp;&amp; cat &gt;&gt; .ssh/authorized_keys&#39; &lt; ~/.ssh/id_rsa.pub","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"Java Native Interface","slug":"022java/Java-native","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:44:59.916Z","comments":true,"path":"category/022java/Java-native.html","link":"","permalink":"http://yoursite.com/category/022java/Java-native.html","excerpt":"","text":"#什么是JNI？ JNI是Java Native Interface的英文缩写, 中文翻译为本地调用, 自从Java 1.1开始就成为了Java标准的一部分. Java不是万能的，java是跨平台的语言，所付出的代价就是牺牲一些对底层的控制，而java要实现对底层的控制，就需要一些其他语言的帮助，这个就是native的作用。 Java的不足除了体现在运行速度上要比传统的C++慢许多之外，Java无法直接访问到操作系统底层（如系统硬件等)，为此Java使用native方法来扩展Java程序的功能。 Java Native适用的情况 为了使用底层的主机平台的某个特性，而这个特性不能通过JAVA API访问 为了访问一个老的系统或者使用一个已有的库，而这个系统或这个库不是用JAVA编写的 为了加快程序的性能，而将一段时间敏感的代码作为本地方法实现。 Java调用C/C++的基本步骤 在Java中声明native()方法，然后编译； 用javah产生一个.h文件； 写一个.cpp（.c)文件实现native导出方法，其中需要包含第二步产生的.h文件（注意其中包含了JDK带的jni.h文件） 将第三步的.cpp文件编译成动态链接库文件； 在Java中用System.loadLibrary()方法加载第四步产生的动态链接库文件，这个native()方法就可以在Java中被访问了。 下面将演示一个具体的例子 mac环境下用Java调用C的具体步骤目标：调用c程序打印，“Hello，JNI”. 创建一个Java类，里面包含 native 方法和加载库loadLibrary方法1234567891011121314151617/** * Hello world! */public class HelloNative &#123; static &#123; System.out.println(System.getProperty(&quot;java.library.path&quot;)); System.loadLibrary(&quot;HelloNative&quot;); //System.load(&quot;/p/p-study/java-core/simple/src/main/java/HelloNative.so&quot;); &#125; public static native void sayHello(); @SuppressWarnings(&quot;static-access&quot;) public static void main(String[] args) &#123; new HelloNative().sayHello(); &#125;&#125; 如果直接运行，会出现 Exception in thread &quot;main&quot; java.lang.UnsatisfiedLinkError: no HelloNative in java.library.path 虚拟机说不知道如何找到sayHello 生成.class文件，并使用javah命令，将.class文件生成.h文件生成class文件javac HelloNative.java 把java代码声明的JNI方法转化成C\\C++头文件 javah HelloNative 会生成一个HelloNative.h文件 /* DO NOT EDIT THIS FILE - it is machine generated */ #include &lt;jni.h&gt; /* Header for class HelloNative */ #ifndef _Included_HelloNative #define _Included_HelloNative #ifdef __cplusplus extern &quot;C&quot; { #endif /* * Class: HelloNative * Method: sayHello * Signature: ()V */ JNIEXPORT void JNICALL Java_HelloNative_sayHello (JNIEnv *, jclass); #ifdef __cplusplus } #endif #endif 用C语言实现这些本地方法新建HelloNative.c，将上面步骤生成的.h文件包含在头文件中，并用C语言实现这些本地方法 代码如下 1234567#include &lt;jni.h&gt;#include &quot;HelloNative.h&quot;#include &lt;stdio.h&gt;JNIEXPORT void JNICALL Java_HelloNative_sayHello(JNIEnv *env,jclass obj)&#123; printf(&quot;Hello，JNI&quot;);&#125; 编译刚才所写的C代码，生成.so文件123gcc -Wall -c HelloNative.c -I ./-I /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home/include -I /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home/include/darwin 生成动态连接库gcc -shared HelloNative.o -o HelloNative.so 如果是linux,需要将darwin改为linux 运行java HelloNative 如果出现“no HelloNative in java.library.path”的错误，修改java的代码，改为绝对路径 System.load(“/p/p-study/java-core/simple/src/main/java/HelloNative.so”); 终于出现了久违的“Hello，JNI“ 参考 Linux平台Java调用so库-JNI使用例子","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"我的Vi常用操作","slug":"018linux/0Vi常用操作","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T10:22:56.166Z","comments":true,"path":"category/018linux/0Vi常用操作.html","link":"","permalink":"http://yoursite.com/category/018linux/0Vi常用操作.html","excerpt":"","text":"Vi常用命令定位gg: 文章头部G:移至文章尾，行首:3 到第3行 A: 进入行末，并进入编辑$光标移至行尾数字0，移至行首 ctrl+F:下一屏文本，ctrl+B:上一屏文本 删除相关d$删除至行末尾，d＋ shiftdG删除到文章尾部，包含本行x,删除光标后的字母s 删除当前字母，并进入删除模式dd 删除一行:5d删除第5行文本，：6，15d删除第6-15行之间的文本 ##delete􏲓、􏲘􏺴􏲑yank􏲓与􏺵􏺶􏲑put􏲓􏰚􏰛yy 复制行p为复制x p 剪切一个字符 y$截出从光标到行末之间的文本 ##移动:1,2 move 3将1，2行移到3行下 查找搜索多个/ ? n N 下一个/上一个 搜索一个字符f F 行内搜索 , ；下一个/上一个 替换r R 替换当前字符／后面的字符直到按下「ESC」键为止 % s/10.65.215.13:8150/127.0.0.1:8090/g如果包含特殊符号% s/base.vfinance.cn\\/pns/func2intra.vfinance.cn\\/pns/g 注意不要忘了% 撤销与重做u 撤销3u 三次撤销U 撤销本行所作的修改ctrl +r 重做 综合操作技巧以root权限进行保存:w !sudo tee % &gt; /dev/null 复制一行，并粘贴yyp 剪切一行和粘贴ddp 参看当前的文件信息ctrl + g 替换指定的行set number:39,44s/task/abc/g 进行大小写转换先选中v,u 小写U 大写 ##跳到多少行11G 跳到11行 ##使用.进行重复操作 删除列光标先移动到第一行，第一列，然后按ctrl + v，进入visual block模式 然后把光标移动到第三行，第三列，再按x键 一边搜索一边替换例如需要把hello替换为你好/Applayvs你好使用n,进行查询，然后使用.号进行重复 ##合并J:将下一行同当前行合并","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"常用","slug":"常用","permalink":"http://yoursite.com/tags/常用/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"系统的整体资源使用情况－top","slug":"018linux/top","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.057Z","comments":true,"path":"category/018linux/top.html","link":"","permalink":"http://yoursite.com/category/018linux/top.html","excerpt":"","text":"top123456789top - 22:13:32 up 133 days, 1:38, 1 user, load average: 0.46, 0.46, 0.45Tasks: 141 total, 1 running, 140 sleeping, 0 stopped, 0 zombieCpu(s): 23.9%us, 2.8%sy, 0.0%ni, 72.6%id, 0.7%wa, 0.0%hi, 0.0%si, 0.0%st Mem: 3921472k total, 2955224k used, 966248k free, 295112k buffersSwap: 2097144k total, 82756k used, 2014388k free, 542824k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 25477 root 20 0 21556 728 208 S 51.9 0.0 10579:38 leczbhrtmj 9 root 20 0 0 0 0 S 2.3 0.0 1497:44 ksoftirqd/1 load average周期为1分钟，5分钟，15分钟zombie表示僵尸进程数第三行cpu统计数据 基本使用也可以使用 12top | grep CpuCpu(s): 7.4%us, 2.7%sy, 4.6%ni, 84.9%id, 0.3%wa, 0.0%hi, 0.0%si, 0.0%st 字段描述cpu统计数据 Cpu(s) CPU的整体负载信息（其显示为从现在的时间到上一次刷新期间的百分比） 1.0%us CPU运行用户的进程所花的时间百分比（不包含改变过nice值的进程），通常情况下希望us占比越高越好。 0.7%sy CPU运行内核态所花的时间百分比，sy占比较高，通常意味着系统的某些方面设计的不合理。 0.0%ni CPU调整进程优先级所花的时间百分比 98.3%id CPU空闲时间百分比 0.0%wa CPU用在IO等待的时间百分比，不用花费大量的时间进行等待，否则表示有可能某些地方设计不合理。 0.0%hi CPU硬中断时间百分比 0.0%si CPU软中断时间百分比 0.0%st CPU为了其他任务从虚拟机管理程序窃取的时间，st越高表示与其他虚拟机cpu争用比较频繁。 内存使用情况 Mem: 物理内存使用情况 total 物理内存总量 used 使用的物理内存总量 free 空闲的内存量 buffers 用作缓存内存量 每个process使用的资源情況 PID 进程号 USER 进程所有者 PR 进程的优先级，值越小越优先被执行 NI 进程的nice值 VIRT 进程占用的虚拟内存 RES 进程占用的物理内存 SHR 进程使用的共享内存 S 进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态等 %CPU 进程占用CPU的使用率 %MEM 进程使用的物理内存的百分比 TIME+ 该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。 COMMAND 进程启动命令名称 参数指定监测某个进程-p #后面加PID号，指定监测某个进程 按键指令按照线程来显示cpu的消耗情况shift + H 排序以PID排序显示 N以cpu的使用率排序显示 P以Memory的使用资源排序显示 M以TIME+为准进行排序显示 T 指定显示用户进程 u #uptime uptime其分别显示1,5,15分钟的cpu的平均负载 通过比较这三个数字，可以判断负载是上升还是下降，或平稳。如果负载值大于CPU数，这可能意味着CPU饱和了，或线程遭受调度延迟，也可能有磁盘IO的因素，使用其他工具进一步调查。 只要cpu的当前活动线程不大于3，我们认为它的负载时正常的。如果大于5，负载就非常高了","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"CLH","slug":"022java/thread/CLH","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-27T07:53:57.912Z","comments":true,"path":"category/022java/thread/CLH.html","link":"","permalink":"http://yoursite.com/category/022java/thread/CLH.html","excerpt":"","text":"CLHCLH(Craig, Landin, and Hagersten)锁，简单的说，它使用队列的方式来解决n个线程来争夺m把锁的问题，每当一个新的线程需要获取锁，为其创建一个节点并放到队尾，如果该线程是队列中的第一个节点，则节点的locked设置成false，如果它不是队列的第一个节点，则它的节点的prev指向原来的队尾节点，并不断自旋查看prev指向节点的locked属性，如果该值变为false，表示轮到它来尝试获取锁了，如果获取成功并最终用完释放后，则将自己的locked设置成false，如果获取失败，locked值不变，还是true，并不断尝试获取锁。 MSC也是可扩展、高性能的自旋锁，它和CLH不同的是，它是对自己节点的locked属性进行自旋，这意味着prev节点释放锁后，需要去主动改变它的后继next节点的locked的状态。","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/tags/高并发/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"},{"name":"源码阅读","slug":"源码阅读","permalink":"http://yoursite.com/tags/源码阅读/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"监控cpu和内存－vmstat命令","slug":"018linux/vmstat","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.059Z","comments":true,"path":"category/018linux/vmstat.html","link":"","permalink":"http://yoursite.com/category/018linux/vmstat.html","excerpt":"","text":"#vmstat 基本使用每秒采集一次，共计3次 vmstat 1 3 12345procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 11300 235384 166844 1332584 0 0 5 27 119 65 0 0 99 0 0 0 0 11300 234580 166844 1332616 0 0 0 0 235 170 0 0 100 0 0 0 0 11300 234952 166844 1332616 0 0 0 0 221 157 0 0 100 0 0 字段描述 字段 说明 Procs r 等待运行的进程的数量 b 睡眠的进程的数量 Memory swpd 已使用的虚拟内存大小，如果大于0，表示机器物理内存不足 free 空闲的内存大小 buff 缓冲区的内存大小 cache 用于高速缓存的内存大小 inact 不活跃的内存大小(-a option) active 活跃的内存大小(-a option) Swap si 从磁盘交换到内存的数量(/s)，大于0，发生了swap交换，系统的内存不足 so 从内存交换到磁盘的的数量(/s) IO bi 从一个块设备接收到的块数量 bo 发送给一个块设备的块数量(blocks/s) System in 每秒的中断数,包含时间中断 cs 每秒上下文切换的数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好 CPU us 运行非内核程序的时间(user time, including nice time)，用户时间＋nice sy 运行内核程序的时间，如果太高，表示系统调用时间长，例如IO操作频繁 id 系统空闲时间(Prior to Linux 2.5.41, this includes IO-wait time) wa 用在IO等待的时间，线程被阻塞等待磁盘IO时的CPU空闲时间 st CPU在虚拟化的环境下在其他租户上的开销 r为总数，其他的数为平均数。 si,so,如果有大于0的数，说明发生了swap交换，系统的内存不足。cs(上下文切换)很高us,表示cpu占有率很高 参数 参数 说明 -a 显示活动/非活动的内存 -f 显示从开机到现在的forks的数目,等同于任务的创建总数 -n 只显示头部一次 -s 显示一个表包含各种事件计数器和内存统计 delay 刷新时间间隔 count 刷新次数 -d 显示磁盘相关统计信息 -D 显示活动的磁盘概要统计信息 -p 指定磁盘分区统计信息 -S 使用指定单位显示。参数有k,m,M -V 显示vmstat版本信息。 用法举例指定单位vmstat 1 -Sm","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"awk＋sed学习笔记","slug":"018linux/Awk+sed","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.054Z","comments":true,"path":"category/018linux/Awk+sed.html","link":"","permalink":"http://yoursite.com/category/018linux/Awk+sed.html","excerpt":"","text":"awk学习指南awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 输出第1列和第4例awk &#39;{print $1, $4}&#39; netstat.txt 其中的$1..$n表示第几例。注：$0表示整个行 过滤记录awk &#39;$3==0 &amp;&amp; $6==&quot;LISTEN&quot; &#39; netstat.txt 需要表头的话，我们可以引入内建变量NR awk &#39;$3==0 &amp;&amp; $6==&quot;LISTEN&quot; || NR==1 &#39; netstat.txt 进行格式化 awk &#39;$3==0 &amp;&amp; $6==&quot;LISTEN&quot; || NR==1 {printf &quot;%-20s %-20s %s\\n&quot;,$4,$5,$6}&#39; netstat.txt 指定分隔符awk &#39;BEGIN{FS=&quot;:&quot;} {print $1,$3,$6}&#39; /etc/passwd等价下面的awk -F: &#39;{print $1,$3,$6}&#39; /etc/passwd-F的指定分隔符，如果有多个，可以使用awk -F ‘[;:]’ 改变输出字段分隔符默认是空格，下面为一个以\\t作为分隔符输出的例子awk -F: &#39;{print $1,$3,$6}&#39; OFS=&quot;\\t&quot; /etc/passwd 字符串匹配awk &#39;/ESTABLISHED/&#39; netstat.txt awk &#39;$6 ~ /FIN/ || NR==1 {print NR,$4,$5,$6}&#39; OFS=&quot;\\t&quot; netstat.txt~ 表示模式开始，/ /中是模式 文件分割按第6例分隔文件，会产生很多小文件awk &#39;NR!=1{print &gt; $6}&#39; netstat.txt awk的一些内建变量 column column ARGC 命令行参数个数 ARGV 命令行参数排列 ENVIRON 支持队列中系统环境变量的使用 FILENAME awk浏览的文件名 FNR 浏览文件的记录数 FS 设置输入域分隔符，等价于命令行 NF 浏览记录的域的个数 NR 已读的记录数 OFS 输出域分隔符 ORS 输出记录分隔符 RS 控制记录分隔符 应用举例如果文档为类似如RS ＃控制记录分隔符这样的构成，我们需要Markdown的表格的生成，可以使用如下 cat c.txt | awk -F &#39;#&#39; &#39;{print $1,$2}&#39; OFS=&quot;|&quot; | sed -e &#39;s/^/|/g&#39; -e &#39;s/$/|/g&#39; sed学习指南sed全名叫stream editor，是一种在线编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。 哪些行需要编辑－定址定址用于决定对哪些行进行编辑。地址的形式可以是数字、正则表达式、或二者的结合。如果没有指定地址，sed将处理输入文件的所有行 地址是一个数字，则表示行号；是“$”符号，则表示最后一行。 地址是逗号分隔的，那么需要处理的地址是这两行之间的范围（包括这两行在内）。范围可以用数字、正则表达式、或二者的组合表示。 sed &#39;/My/,10d&#39; datafile上面的删除包含”My”的行到第十行的内容 Sed的使用参数-n ：安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。-e ：表示直接在命令行模式上进行Sed的操作，是默认选项，不用写；-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作；-r ：表示使Sed支持扩展正则表达式。(默认是基础正规表示法语法)-i ：直接修改读取的文件内容，而不是输出到终端n1，n2：不一定需要，选择要进行处理的行。如10，20表示在10~20行之间处理 Sed的action（动作）Sed的action（动作）支持如下参数。 a：表示添加，后接字符串，添加到当前行的下一行。 c：表示替换，后接字符串，用它替换n1到n2之间的行。 d：表示删除符合模式的行，它的语法为sed ‘/regexp/d’，斜杠之间是正则表达式，模式在d前面，d后面一般不接任何内容。 i：表示插入，后接字符串，添加到当前行的上一行。 p：表示打印，打印某个选择的数据，通常与-n安静模式一起使用。 s：表示搜索，还可以替换，类似于Vim里的搜索替换功能。例如：1，20s/old/new/g表示替换1~20行的old为new，g在这里表示处理这一行所有匹配的内容。 sed应用范例用s命令替换将hello替换为“你好” sed &quot;s/hello/你好/g&quot; test.txt 加上注释 sed &#39;s/^/#/g&#39; test.txt -i 参数直接修改文件内容sed -i &quot;s/hello/你好/g&quot; test.txt 删除显示passwd内容，将2~5行删除后显示cat -n /etc/passwd |sed &#39;2,5d&#39; 添加行，使用a命令a命令就是append 在第2行后面的一行加上“Hello China！”字符串 cat -n /etc/passwd |sed &#39;2a Hello China!&#39; i命令就是insertcat -n /etc/passwd |sed &#39;2i Hello China!&#39; 最后一行后追加一行cat -n /etc/passwd |sed &#39;$a Over!&#39; n参数，只显示第5~7行cat -n /etc/passwd |sed -n ‘5,7p’ 一次替换多个－使用分号把第一行到第三行的my替换成your，第二个则把第3行以后的This替换成了Thatsed &#39;1,3s/my/your/g; 3,$s/This/That/g&#39; test.txt 用&amp;来当做被匹配的变量sed &#39;s/Hao/[&amp;]/g&#39; test.txt s命令－数据的搜寻并替换sed &#39;s/要被取代的字串/新的字串/g&#39; c命令－替换匹配行sed &quot;2 c 喝牛奶哦&quot; test.txt -i d命令－删除匹配行sed &#39;/牛奶/d&#39; test.txt数据的搜寻并删除 替换换行Windows下换行符号是“\\r\\n”，而linux下是“\\n”没有”\\r”; sed -i &#39;s/\\r$//&#39; 嵌套命令对3行到第6行，匹配/This/成功后，再匹配/fish/，成功后执行d命令sed &#39;3,6 {/This/{/fish/d}}&#39; pets.txt 从第一行到最后一行，如果匹配到This，则删除之；如果前面有空格，则去除空格sed &#39;1,${/This/d;s/^ *//g}&#39; pets.txt{中的;相当于 如果 .. 则 多点编辑ecat /etc/passwd | sed -e &#39;3,$d&#39; -e &#39;s/bash/blueshell/&#39;-e表示多点编辑，第一个编辑命令删除/etc/passwd第三行到末尾的数据，第二条命令搜索bash替换为blueshell。相当于 and .. and ###操作匹配连续的行sed &#39;/name/,+3s/^/# /g&#39; test.txt 参考 sed命令详解 sed 简明教程 awk案例学习","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"git命令","slug":"020programming/配置管理/0常用git命令","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T10:20:53.385Z","comments":true,"path":"category/020programming/配置管理/0常用git命令.html","link":"","permalink":"http://yoursite.com/category/020programming/配置管理/0常用git命令.html","excerpt":"","text":"git分支管理Git分支是轻量且高效的，为什么呢？答案是：传统的版本控制系统存储的数据是文件的变更，而Git则是存储一系列的文件快照（snapshot）。 vi .git/HEAD,其内保存了指向当前分支最新提交的指针。 该指针指向refs/heads/分支名文件，我们进入.git/refs/heads/目录，其下以分支名为文件名列出了所有分支。如master 执行vi master:可以看到，其内存储的就是当前分支的最新一次提交对象ID。 命令查看详情 git log --decorate git log --graph git log –graph –decorate git log –pretty=oneline //显示简单信息 git log命令显示从最近到最远的提交日志 git status告诉你有文件被修改过，用git diff可以查看修改内容 git add 的各种区别git add -A // 添加所有改动 git add * // 添加新建文件和修改，但是不包括删除 git add . // 添加新建文件和修改，但是不包括删除 git add -u // 添加修改和删除，但是不包括新建文件 ###怎么回退 首先查看版本git log git reset –hard HEAD^ 在Git中，用HEAD表示当前版本，也就是最新的提交上上一个版本就是HEAD^^ git reset –hard 3628164 恢复到新版本Git提供了一个命令git reflog用来记录每一次命令 比较git diff readme.txt 创建一个分支我们创建一个分支work-a git checkout -b work-a 等价于以下两条指令： git branch 分支名 git checkout 分支名 提交git add .git commit -a -m “fix” 合并先切换到master分支git merge work-a该指令告诉Git将指定分支合并到当前分支 指定–no-ff即声明进行非快速推进合并 rebase删除分支当分支合并入主干后，也许我们不再需要那个分支了，我们需要将其删除，使用指令： git branch -d 分支名 远程分支远程分支和本地分支基本理论概念还是相同的，区别是有些指令不同git checkout -b test origin/develop以上指令即从远程分支(远端主机origin上的develop分支)切出新的本地分支test分支。 当我们从一个远程分支切出（创建）一个本地分支时，这个分支就叫跟踪分支（tracking branch）,而远程分支叫上游分支（upstream branch）。 当我们克隆一个远端仓库时，会默认创建一个跟踪分支master，其上游分支就是远端主机别名/master 手动建立追踪关系git branch –set-upstream master origin/next指定master分支追踪origin/next分支。 查看远程和分支git branch -r git branch命令的-r选项，可以用来查看远程分支，-a选项查看所有分支git branch查看本地分支 创建跟踪分支创建跟踪分支指令如下： git checkout -b 本地分支名 远端主机别名/远程分支名 git checkout -b ccc origin/work-a 本地分支设置其上游分支需要为本地分支设置其上游分支，添加-u参数： git branch -u 远端主机别名/远程分支名 如何参看当前分支对应的上游分支git branch -vv 删除远程分支对于不再需要的远程分支，是可以删除的： git push origin –delete test 推送数据到远程分支git push &lt;远程主机名&gt; &lt;本地分支名&gt; &lt;远程分支名&gt;git push origin ccc master git push &lt;远程主机名&gt; &lt;本地分支&gt;如果远程分支被省略，如上则表示将本地分支推送到与之存在追踪关系的远程分支（通常两者同名），如果该远程分支不存在，则会被新建。 git push -u origin master-u 参数指定一个默认主机，这样后面就可以不加任何参数使用git push pull分支git pull命令的作用是，取回远程主机某个分支的更新，再与本地的指定分支合并git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 例如远程分支如下： git branch -r origin/HEAD -&gt; origin/master origin/ccc origin/master origin/work-a git pull origin ccc:master 如果远程分支是与当前分支合并，则冒号后面的部分可以省略。git pull origin ccc git pull origin本地的当前分支自动与对应的origin主机”追踪分支”（remote-tracking branch）进行合并。 如果当前分支只有一个追踪分支，远程主机名都可以省略git pull 远端主机参看远端主机git remotegit remote -v使用-v选项，可以参看远程主机的网址 添加主机git remote add &lt;主机别名&gt; &lt;远端地址&gt;git remote add origin http://112.124.0.156:9258/klwork/test.git 查看特定主机信息git remote show origin 参考http://www.cnblogs.com/qianqiannian/p/6008140.htmlhttp://www.ruanyifeng.com/blog/2014/06/git_remote.htmlhttp://blog.csdn.net/wh_19910525/article/details/7554489http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"常用","slug":"常用","permalink":"http://yoursite.com/tags/常用/"},{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"gradle入门","slug":"020programming/配置管理/gradle入门","date":"2016-11-06T00:00:00.000Z","updated":"2018-12-10T15:04:49.306Z","comments":true,"path":"category/020programming/配置管理/gradle入门.html","link":"","permalink":"http://yoursite.com/category/020programming/配置管理/gradle入门.html","excerpt":"","text":"gradle入门gradle跟ivy/maven一样，是一种依赖管理/自动化构建工具。但是跟ivy/maven不一样，它并没有使用xml语言，而是采用了Groovy语言。更加强大的是，gradle完全兼容maven和ivy。 gradle跟maven一样，也有一个配置文件，maven里面是叫pom.xml，而在gradle中是叫build.gradle。 ＃目录结构在一个空目录下，执行gradle init，假设gradle已经安装，并且配置到了环境变量中。 执行tree,可以看到如下目录。 ├── build.gradle├── gradle│ └── wrapper│ ├── gradle-wrapper.jar│ └── gradle-wrapper.properties├── gradlew├── gradlew.bat└── settings.gradle gradle.properties它是一个配置文件，里面可以定义一些常量供build.gradle使用，比如可以配置签名相关信息如keystore位置，密码，keyalias等。 settings.gradle 这个文件是用来配置多模块的，比如你的项目有两个模块module-a,module-b,那么你就需要在这个文件中进行配置，格式如下：include ‘:module-a’,’:module-b’ gradle文件夹 这里面有两个文件，gradle-wrapper.jar和gradle-wrapper.properties,它们就是gradle wrapper。 gradlew和gradlew.bat这分别是linux下的shell脚本和windows下的批处理文件，它们的作用是根据gradle-wrapper.properties文件中的distributionUrl下载对应的gradle版本。这样就可以保证在不同的环境下构建时都是使用的统一版本的gradle，即使该环境没有安装gradle也可以，因为gradle wrapper会自动下载对应的gradle版本。 gradlew的用法跟gradle一模一样，比如执行构建gradle build命令，你可以用gradlew build。gradlew即gradle wrapper的缩写。 配置说明123456789101112131415161718192021// Top-level build file where you can add configuration options common to all sub-projects/modules. buildscript &#123; //构建过程依赖的仓库 repositories &#123; jcenter()&#125;//构建过程需要依赖的库dependencies &#123; //下面声明的是gradle插件的版本 classpath &apos;com.android.tools.build:gradle:1.1.0&apos; // NOTE: Do not place your application dependencies here; they belong // in the individual module build.gradle files&#125;&#125;//这里面配置整个项目依赖的仓库,这样每个module就不用配置仓库了allprojects &#123; repositories &#123; jcenter() &#125;&#125; repositories，构建过程依赖的仓库 dependencies，构建过程需要依赖的库 为什么仓库repositories需要声明两次，这其实是由于它们作用不同，buildscript中的仓库是gradle脚本自身需要的资源，而allprojects下的仓库是项目所有模块需要的资源。 gradle仓库：gradle有三种仓库，maven仓库，ivy仓库以及flat本地仓库。声明方式如下： maven{ url “…”}ivy{ url “…”}flatDir{ dirs ‘xxx’}有一些仓库提供了别名，可直接使用： repositories{ mavenCentral() jcenter() mavenLocal()} gradle任务gradle中有一个核心概念叫任务，跟maven中的插件目标类似。gradle的android插件提供了四个顶级任务 assemble 构建项目输出check 运行检测和测试任务build 运行assemble和checkclean 清理输出任务gradlew assemble相当于执行gradlew assembleDebuggradlew assembleRelease 检查依赖报告gradle dependencies 常见问题下载很慢将下载好的gradle-2.14-bin.zip放着相应的目录下/Users/ww/.gradle/wrapper/dists/gradle-2.14-bin/76oc0mnc3ieqtsukq90mp0rxk 参考http://www.open-open.com/lib/view/open1431391503529.html","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"gradle","slug":"gradle","permalink":"http://yoursite.com/tags/gradle/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"查看内存使用情况－free","slug":"018linux/free","date":"2016-11-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.054Z","comments":true,"path":"category/018linux/free.html","link":"","permalink":"http://yoursite.com/category/018linux/free.html","excerpt":"","text":"#free 基本使用free –m 123 total used free shared buff/cache availableMem: 2044300 298928 1456556 12268 288816 1565128Swap: 2095100 56 2095044 userd：当前已使用的内存总量。free：空闲的或可以使用的内存总量。shared：共享内存大小，主要用于进程间通信。 buffers：用于块设备I/O的缓冲区缓存。cached：用于文件系统的页缓存。它们的值接近于0时，往往导致较高的磁盘I/O（可以通过iostat确认）和糟糕的性能。 available:可以使用的内存总量。 对于应用程序来说，buffers/cached 是等于可用的，因为buffer/cached是为了提高文件读取的性能，当应用程序需在用到内存的时候，buffer/cached会很快地被回收。 所以从应用程序的角度来说 可用内存=系统free+buffers+cached.真正用的内存: used-buffers-cached。 字段描述参数用法举例参考https://linux.cn/article-2434-1.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"btrace学习","slug":"022java/jvm/btrace学习","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:55:28.919Z","comments":true,"path":"category/022java/jvm/btrace学习.html","link":"","permalink":"http://yoursite.com/category/022java/jvm/btrace学习.html","excerpt":"","text":"#功能介绍？ Btrace (Byte Trace)是sun推出的一款java 动态、安全追踪（监控）工具，可以不停机的情况下监控线上情况，并且做到最少的侵入，占用最少的系统资源。 BTrace利用了java.lang.instrument包实现代码注入通过VirtualMachine.attach(pid)连接远程JVM，然后通过VirtualMachine.loadAgent(“*.jar”)加载一个btrace的jar包通过ASM动态生成字节码。 基本使用运行一个Java程序123456789101112131415161718192021222324252627public class MockApp &#123; public static void main(String[] argv) &#123; new MockApp().run(); &#125; public void run() &#123; for (int i = 0; i &lt; 100000; i++) &#123; sleep(1000); new MyObj().life(i); &#125; &#125; private static class MyObj &#123; public void life(int n) &#123; System.out.println(n); &#125; &#125; private void sleep(int n) &#123; try &#123; Thread.sleep(n); &#125; catch (InterruptedException e) &#123; &#125; &#125;&#125; jps命令查出需要监控的jvm pidjps 4144 Jps5864141 AppMain4142 Launcher 可以看到pid为4142 每次都找，很麻烦，可以使用：jps | grep AppMain | awk &#39;{print $1}&#39; | xargs -I {} btrace {} sample/HelloBtrace.java 编写BTrace跟踪程序12345678910111213141516171819package sample;import com.sun.btrace.annotations.BTrace;import com.sun.btrace.annotations.Kind;import com.sun.btrace.annotations.Location;import com.sun.btrace.annotations.OnMethod;import static com.sun.btrace.BTraceUtils.println;@BTracepublic class HelloBtrace &#123; // app.MockApp.sleep方法返回时，执行该方法 @OnMethod(clazz=&quot;app.MockApp&quot;, method=&quot;sleep&quot;, location=@Location(Kind.RETURN)) public static void onSleep() &#123; println(&quot;Hello world&quot;); &#125;&#125; 执行btrace 跟踪程序btrace 4141 sample/HelloBtrace.java 常用注解类上的注解 名称 作用域 作用 @BTrace 类 用来指定该java类为一个btrace脚本文件 @DTrace 类 指定btrace脚本与内置在其脚本中的D语言脚本关联 @DTraceRef 类 指定btrace脚本与另一个D语言脚本文件关联 方法上的注解 名称 作用域 作用 @OnMethod(clazz,method,location) 方法 当指定方法被调用时 @OnMethod(method=”“) 方法 当构造函数被调用时 @OnMethod(clazz=”/java\\.io\\..*Input/“)) 方法 方法名称正则匹配 @Location(kind) @OnMethod 指定监控方法调用前还是调用后 @Location(value=Kind.NEWARRAY,clazz=”char”) @OnMethod @OnTimer(interval) 方法 定时调用某个方法 @OnLowMemory(pool,threshold) 方法 当内存不足时 @OnExit 方法 当程序退出时 @OnProbe(namespace=”java.net.socket”,name=”bind”) 方法 监控socket中的bind方法 参数上的注解 名称 作用域 作用 @Self 参数 表示被监控的对象 @Return 参数 用来指定被trace方法的返回值 @ProbeMethodName 参数 被监控的方法名称 @ProbeClassName 参数 被监控的类名 非注解的方法参数未使用注解的方法参数一般都是用来做方法签名匹配用的, 他们一般和被trace方法中参数出现的顺序一致. 不过他们也可以与注解方法交错使用, 如果一个参数类型声明为AnyType[], 则表明它按顺序”通吃”方法所有参数. 未注解方法需要与Location结合使用: 方法 作用 Kind.ENTRY 被trace方法参数 Kind.RETURN 被trace方法返回值 Kind.THROW 抛异常 Kind.ARRAY_SET, Kind.ARRAY_GET 数组索引 Kind.CATCH 捕获异常 Kind.FIELD_SET 属性值 Kind.LINE 行号 Kind.NEW 类名 Kind.ERROR 抛异常 Kind.ENTRY 是和 Kind.RETURN 对应的， @OnMethod(location = @Location(Kind.RETURN)) 表示目标方法返回时触发；ENTRY表示进入时触发。 Kind.CALL 和 Kind.LINE 作用类似@OnMethod(location = @Location(value = Kind.CALL, clazz = “a”, method = “b”)) 指目标方法体如果调用了a.b()方法时触发； @OnMethod(location = @Location(value = Kind.LINE, line = 5)) 指目标方法体执行到第n行代码时触发 属性上的注解常用方法 方法 作用 println 在本地控制台输出一行 print 在本地控制台输出 printArray 在本地控制台输出数组 jstack 打印远程方法的调用调用栈 jstackAll 输出所有线程的调用栈 exit 退出跟踪脚本 Strings.strcat 连接字符串 Reflactive.name 获取类名 Threads.name 线程名 Threads.currentThread 当前线程 deadlocks 打出死锁线程 sizeof 获取对象的大小，比如List对象就返回List.size() Sys.Env.property 获取系统变量 参考例子打印堆/非堆内存信息12345678910111213141516171819202122232425262728293031323334package sample; /** * Created by ww on 16/11/15. */import com.sun.btrace.annotations.BTrace;import com.sun.btrace.annotations.OnTimer;import static com.sun.btrace.BTraceUtils.*;@BTracepublic class TraceMemory &#123; //heapUsage()/nonHeapUsage() – 打印堆/非堆内存信息，包括init、used、commit、max //init = 134217728(131072K) used = 16793216(16399K) committed = 128974848(125952K) max = 1908932608(1864192K) @OnTimer(4000) public static void printM()&#123; //打印内存信息 println(&quot;heap:&quot;); println(heapUsage()); println(&quot;no-heap:&quot;); println(nonHeapUsage()); &#125;&#125; 打印参数123456789101112131415161718192021222324252627282930package sample;import com.sun.btrace.AnyType;import com.sun.btrace.BTraceUtils;import com.sun.btrace.annotations.BTrace;import com.sun.btrace.annotations.Kind;import com.sun.btrace.annotations.Location;import com.sun.btrace.annotations.OnMethod;import com.sun.btrace.annotations.Return;import com.sun.btrace.annotations.Self;import java.lang.reflect.Field;import static com.sun.btrace.BTraceUtils.*;import static com.sun.btrace.BTraceUtils.Reflective.field;import static com.sun.btrace.BTraceUtils.println;@BTracepublic class PrintParameterTest &#123; @OnMethod(clazz=&quot;app.BTraceServer&quot;, method=&quot;sayHello&quot;, location=@Location(Kind.RETURN)) public static void onSayHello(@Self Object s, AnyType parames, @Return String ret)&#123; Field maxTotalField = field(&quot;app.User&quot;, &quot;name&quot;); println(strcat(&quot;Ps Key: &quot;, str(get(maxTotalField, parames)))); println(&quot;sdfsdf&quot;); &#125;&#125; 其中，parames 是一个User对象。切记只能为AnyType对象，我开始时使用的是Object,输出就为空。 123456789101112131415public class BTraceServer &#123; public String sayHello(User user) &#123; return &quot; 结果:&quot; + user.getName(); &#125; public static void main(String[] args) throws InterruptedException &#123; BTraceServer bt = new BTraceServer(); int i = 0; while (true) &#123; System.err.println(bt.sayHello(new User(i++,&quot;name&quot; +i))); Thread.sleep(1000); &#125; &#125;&#125; 参考 BTrace简介及使用 BTrace笔记 btrace一些你不知道的事(源码入手)","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"java线程状态总结","slug":"022java/thread/线程的状态","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:44:35.234Z","comments":true,"path":"category/022java/thread/线程的状态.html","link":"","permalink":"http://yoursite.com/category/022java/thread/线程的状态.html","excerpt":"","text":"线程的各种状态线程的状态跃迁 锁(monitor)池和等待池在java中，每个对象都有两个池，锁(monitor)池和等待池 锁池:假设线程A已经拥有了某个对象(注意:不是类)的锁，而其它的线程想要调用这个对象的某个synchronized方法(或者synchronized块)，由于这些线程在进入对象的synchronized方法之前必须先获得该对象的锁的拥有权，但是该对象的锁目前正被线程A拥有，所以这些线程就进入了该对象的锁池中。 等待池:假设一个线程A调用了某个对象的wait()方法，线程A就会释放该对象的锁(因为wait()方法必须出现在synchronized中，这样自然在执行wait()方法之前线程A就已经拥有了该对象的锁)，同时线程A就进入到了该对象的等待池中。如果另外的一个线程调用了相同对象的notifyAll()方法，那么处于该对象的等待池中的线程就会全部进入该对象的锁池中，准备争夺锁的拥有权。同样，如果另外的一个线程调用了相同对象的notify()方法，那么仅仅有一个处于该对象的等待池中的线程(随机)会进入该对象的锁池. ##什么时候使用wait? wait用来线程的协调。场景1.如果有空间则写入数据，没有等候。2.如空间已经有了，通知等候的线程。 有没有更好的协调方式？ ##什么时候使用synchronized? 当一个线程正在执行synchronized方法时，其他线程无法执行该方法，这就是简单的共享互斥。互斥。 ##sleep()和wait()的区别? sleep()方法，方法是属于Thread类中的。而wait()方法，则是属于Object类中的。Thread.sleep不会导致锁行为的改变。sleep()方法导致了程序暂停执行指定的时间，让出cpu该其他线程，但是他的监控状态依然保持者，当指定的时间到了又会自动恢复运行状态。在调用sleep()方法的过程中，线程不会释放对象锁。调用wait()方法的时候，线程会放弃对象锁，进入等待此对象的等待锁定池。 参考 各种 Java Thread State 第一分析法则 http://blog.csdn.net/jiangwei0910410003/article/details/19962627","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"线程中断","slug":"022java/thread/线程中断","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-27T14:18:31.269Z","comments":true,"path":"category/022java/thread/线程中断.html","link":"","permalink":"http://yoursite.com/category/022java/thread/线程中断.html","excerpt":"","text":"线程中断总结背景有些时候，需要一个线程死掉, 或者让它结束某种等待的状态，该怎么办呢?比较安全的做法是：1.使用等待/通知机制2.给那个线程一个中断信号, 让它自己决定该怎么办 线程中断使用场景: 场景1： 在某个子线程中为了等待一些特定条件的到来, 你调用了Thread.sleep(10000), 预期线程睡10秒之后自己醒来, 但是如果这个特定条件提前到来的话, 来通知一个处于Sleep的线程。 场景2： 线程通过调用子线程的join方法阻塞自己以等待子线程结束, 但是子线程运行过程中发现自己没办法在短时间内结束, 于是它需要想办法告诉主线程别等我了. 这些情况下, 就需要中断。 中断不是终止程序，程序需要终止由程序自己决定。 Interrupted的经典代码 12345678910111213public void run()&#123; try&#123; .... while(!Thread.currentThread().isInterrupted()&amp;&amp; more work to do)&#123; // do more work; &#125; &#125;catch(InterruptedException e)&#123; // thread was interrupted during sleep or wait &#125; finally&#123; // cleanup, if required &#125;&#125; 线程中断相关接口Thread.interruptThread.interrupt()将线程中断状态设置为true，表明此线程目前是中断状态。此时如果调用isInterrupted方法，将会得到true的结果。interrupt方法本质上不会进行线程的终止操作的，它不过是改变了线程的中断状态。 Thread.interrupted检测当前线程是否已经中断，此方法会清除中断状态。 假设当前线程中断状态为true，第一次调此方法，将返回true，表明的确已经中断了，同时将中断状态重新置为false。 第二次调用后，将会返回false。 Thread.isInterrupte检测调用该方法的对象所表示的线程是否已经中断，与上一方法的区别在于此方法不会清除中断状态。 如果进行中断，对于非阻塞中的线程, 只是改变了中断状态,此时通过Thread.isInterrupted()将返回true。对于阻塞状态中的线程，Thread.sleep(), Object.wait(), Thread.join(), 这个线程收到中断信号后, 会抛出InterruptedException, 当前线程的中断状态重新被置为false。 一个例子123456789101112131415161718192021222324252627282930313233343536373839class InterruptTest extends Thread &#123; volatile boolean stop = false; public static void main(String args[]) throws Exception &#123; InterruptTest thread = new InterruptTest(); System.out.println(&quot;Starting thread...&quot;); thread.start(); Thread.sleep(3000); System.out.println(&quot;Asking thread to stop...&quot;); thread.stop = true; thread.interrupt(); Thread.sleep(3000); System.out.println(&quot;Stopping application...&quot;); System.exit(0); &#125; public void run() &#123; while (!stop) &#123; System.out.println(&quot;Thread running...&quot;); try &#123; Thread.sleep(50000); &#125; catch (InterruptedException e) &#123; // 接收到一个中断异常（InterruptedException），从而提早地终结被阻塞状态 System.out.println(&quot;Thread interrupted...&quot;); &#125; &#125; System.out.println(&quot;Thread exiting under request...&quot;); &#125;&#125; 在这个例子中，thread.stop = true; 但此时线程阻塞，run方法不被执行。不能进行检查stop。语句后调用thread.interrupt()方法， 该方法将在线程阻塞时抛出一个中断。信号，该信号将被catch语句捕获到，一旦捕获到这个信号，线程就提前终结自己的阻塞状态，这样，它就能够 再次运行run方法了，然后检查到stop = true，while终止。 当代码调用中须要抛出一个InterruptedException, 你可以选择把中断状态复位, 也可以选择向外抛出InterruptedException, 由调用者来决定。","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/tags/高并发/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"线程池学习","slug":"022java/thread/线程池","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-20T07:44:30.215Z","comments":true,"path":"category/022java/thread/线程池.html","link":"","permalink":"http://yoursite.com/category/022java/thread/线程池.html","excerpt":"","text":"线程池学习schedule、scheduleAtFixedRate和scheduleWithFixedDelay的区别schedule(commod,delay,unit) ，这个方法是说系统启动后，需要等待多久执行，delay是等待时间。只执行一次，没有周期性。 **scheduleAtFixedRate(commod,initialDelay,period,unit)，这个是以period为固定周期时间，按照一定频率来重复执行任务，initialDelay是说系统启动后，需要等待多久才开始执行。优先保证任务执行的频率。 scheduleWithFixedDelay(commod,initialDelay,delay,unit)，这个是以delay为固定延迟时间，按照一定的等待时间来执行任务，initialDelay意义与上面的相同。不管线程任务的执行时间的，优先保证任务执行的间隔","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/tags/高并发/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"锁的基本概念","slug":"022java/thread/锁的基本概念","date":"2016-11-06T00:00:00.000Z","updated":"2019-02-26T08:31:17.694Z","comments":true,"path":"category/022java/thread/锁的基本概念.html","link":"","permalink":"http://yoursite.com/category/022java/thread/锁的基本概念.html","excerpt":"","text":"锁的学习锁的出现最早出现在操作系统中，用于进程和线程间的同步 临界区概念一个进程在进入临界区之间，必须得到锁。 123456789While(true) &#123; 请求锁 临界区 释放锁 剩余区&#125; 信号量作为同步工具锁是同步的一种，2值信号量就是一个互斥锁 锁的分类根据锁只能被单个线程持有还是能被多个线程共同持有，锁可以分为独占锁和共享锁。 独占锁（排它锁、互斥锁、写锁、X锁）独占锁保证任何时候都只有一个线程能得到锁，ReentrantLock就是以独占方式实现的。还有如java的内置锁（ synchronized） 如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的锁，直到在事务的末尾将资源上的锁释放为止。锁在一个时间点只能被一个线程锁占有。根据锁的获取机制，它又划分为“公平锁”和“非公平锁”。 共享锁（读锁、S锁）共享锁则可以同时由多个线程持有，例如ReadWriteLock读写锁，它允许一个资源可以被多线程同时进行读操作。 如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁，直到已释放所有共享锁。获准共享锁的事务只能读数据，不能修改数据能被多个线程同时拥有，能被共享的锁。 其他纬度的分类可重入锁如果锁具备可重入性，则称作为可重入锁。也就是说，锁可以被单个线程多次获取。当一个线程执行到某个synchronized方法时，比如说method1，而在method1中会调用另外一个synchronized方法method2，此时线程不必重新去申请锁，而是可以直接执行方法method2，synchronized和ReentrantLock都是可重入锁。 可中断锁如果某一线程A正在执行锁中的代码，另一线程B正在等待获取该锁，可能由于等待时间过长，线程B不想等待了，想先处理其他事情，我们可以让它中断自己或者在别的线程中中断它，这种就是可中断锁。 公平锁公平锁即尽量以请求锁的顺序来获取锁。比如同是有多个线程在等待一个锁，当这个锁被释放时，等待时间最久的线程（最先请求的线程）会获得该所，这种就是公平锁。 在Java中，synchronized就是非公平锁，它无法保证等待的线程获取锁的顺序, ReentrantLock和ReentrantReadWriteLock，它默认情况下是非公平锁，但是可以设置为公平锁。例如ReentrantLock pairLock = new ReentrantLock（true） 自旋锁自旋锁则是，当前线程在获取锁时，如果发现锁已经被其他线程占有，它不马上阻塞自己，在不放弃CPU使用权的情况下，多次尝试获取（默认次数是10，可以使用-XX：PreBlockSpinsh参数设置该值）。如果尝试指定的次数后仍没有获取到锁则当前线程才会被阻塞挂起。 不同视角数据库系统的角度锁分为以下三种类型。独占锁,共享锁,更新锁 从程序员的角度看乐观锁（Optimistic Lock）乐观锁的概念，他的核心思路就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。常见的乐观锁技术（CAS），乐观锁，是一种无锁方案。 悲观锁（Pessimistic Lock）需要程序员直接管理数据或对象上的加锁处理，并负责获取、共享和放弃正在使用的数据上的任何锁。 悲观锁，也是基于数据库的锁机制实现。","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"redis集群部署","slug":"010基础技术/redis/redis集群部署","date":"2016-11-06T00:00:00.000Z","updated":"2018-12-10T15:40:47.163Z","comments":true,"path":"category/010基础技术/redis/redis集群部署.html","link":"","permalink":"http://yoursite.com/category/010基础技术/redis/redis集群部署.html","excerpt":"","text":"redis集群部署cd /root/Downloadswget http://download.redis.io/releases/redis-3.2.8.tar.gzmkdir -p /opt/soft/ tar -zxf zxf redis-3.2.8.tar.gz -C /opt/soft/ cd /opt/softln -s redis-3.2.8 redis 下载新版rediscd /root/Downloadswget http://download.redis.io/releases/redis-3.2.8.tar.gz 解压mkdir -p /opt/soft/tar -zxf zxf redis-3.2.8.tar.gz -C /opt/soft/ 加软连接cd /opt/soft/ln -s redis-3.2.8 redis 编译redisyum -y install gcc tclcd /opt/soft/redis/srcmake MALLOC=libc检查有没有错误， make test 添加环境变量vim /etc/profileexport PATH=$PATH:/opt/soft/redis/srcsource /etc/profile 修改配置文件cd /opt/soft/rediscp redis.conf redis-7000.confvim redis-7000.confdaemonize no 改为 yes # 是否后台运行port 7000 #端口，如果在同一台机器使用，注意修改端口tcp-keepalive 60 ##bind1 27.0.0.1 #注解此配置，否则任何使用都需要符合此ip限制pidfile redis.pid #所有file默认在dir配置的目录下面stop-writes-on-bgsave-error no #rdb写失败不影响后续写入rdbchecksumno #检查rdb是否正确，损失10%性能dir /opt/soft/redis/data-7000 #data文件存放位置protected-mode no#关闭保护模式cluster-enabled yes #开启集群cluster-config-file nodes.conf #集群配置cluster-node-timeout 15000 #超时检测时间cluster-slave-validity-factor0#防止slave不再failover导致集群不可用cluster-require-full-coverage no #槽点丢失不影响其他部分使用appendonly no #关闭aof模式 建立7001文件和7002文件cp redis-7000.conf redis-7001.confcp redis-7000.conf redis-7002.conf 替换配置文件vim redis-7001.conf:%s/7000/7001/g vim redis-7002.conf:%s/7000/7002/g 7001，7002，copy到第二台服务器 建立数据目录redis集群最少3master，3slave。根据机器及其性能分配mkdir data-7000 data-7001 data-7002 启动6个redis实例服务脚本文件在redis安装目录的utils文件夹里redis_init_script修改下就可用cp utils/redis_init_script /etc/init.d/redis-7000 修改前5项配置REDISPORT=7000EXEC=/opt/soft/redis/src/redis-serverCLIEXEC=/opt/soft/redis/src/redis-cli PIDFILE=/opt/soft/redis/data-${REDISPORT}/redis.pidCONF=”/opt/soft/redis/redis-${REDISPORT}.conf”PASSWORD=wdzj2015 case “$1” instart)if [ -f $PIDFILE ]thenecho “$PIDFILE exists, process is already running or crashed”elseecho “Starting Redis server…” $EXEC $CONFfi ;;stop)if [ ! -f $PIDFILE ]thenecho “$PIDFILE does not exist, process is not running”else PID=$(cat $PIDFILE)echo “Stopping …”if [ -n $PASSWORD ]then $CLIEXEC -p $REDISPORT -a $PASSWORD shutdownelse $CLIEXEC -p $REDISPORT shutdownfiwhile [ -x /proc/${PID} ]doecho “Waiting for Redis to shutdown …”sleep 1doneecho “Redis stopped”fi ;; *)echo “Please use start or stop as first argument” ;;esaccp /etc/init.d/redis-7000 /etc/init.d/redis-7001 修改端口7000为7001即可cp /etc/init.d/redis-7000 /etc/init.d/redis-7002 修改端口7000为7002即可service redis-7000 startservice redis-7001 startservice redis-7002 start ps -ef | grep redis 安装执行集群需要的环境yum install -y rubyyum install -y rubygemsgem install redis 配置集群redis-trib.rb create –replicas 1 192.168.11.46:7000 192.168.11.47:7001 192.168.11.46:7002 192.168.11.47:7000 192.168.11.46:7001 192.168.11.47:7002redis-trib.rb最少3个参数，否则报错 默认前3个是master，后3个是slaveCan I set the above configuration? (type ‘yes’ to accept): yes允许redis修改节点信息 集群配置完成。如果出现长时间wait可以视为创建失败，停掉所有服务，删除所有nodes.conf，文件在conf的$dir下；删除后重启redis，重新执行第十步，直到成功 检查是否配置好ps-ef|grep redis 安装成功，检测节点状态登陆redisredis-cli -p 7000 -a wdzj2015 –credis-cli -h 192.168.11.123 -p 7006 -c执行clusternodes查看是否3master3slave，如图： 没有fail就表明安装成功。如果有fail检测网络情况。如果不足6个请重复第10步删除nodes.conf后重新创建节点。 检查节点./redis-trib.rb check 10.254.62.12:7001 redis-cli -p 7000 -a hyjk2017 -c测试 SET abc abcvalue EX 60 NX 参考http://blog.csdn.net/huwei2003/article/details/50973967http://blog.csdn.net/naixiyi/article/details/51339374http://blog.csdn.net/yyyuuueeee/article/details/52249732http://www.tuicool.com/articles/bUVRNn2https://yq.aliyun.com/articles/61734 temphttp://blog.csdn.net/fengyong7723131/article/details/52995592 http://blog.csdn.net/sanwenyublog/article/details/53167544","categories":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}],"keywords":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}]},{"title":"MQ简单学习","slug":"010基础技术/消息队列/mq学习","date":"2016-09-04T00:00:00.000Z","updated":"2018-12-10T15:44:05.254Z","comments":true,"path":"category/010基础技术/消息队列/mq学习.html","link":"","permalink":"http://yoursite.com/category/010基础技术/消息队列/mq学习.html","excerpt":"MQMQ全称为Message Queue, 消息队列（MQ）是一种应用程序对应用程序的通信方法。。","text":"MQMQ全称为Message Queue, 消息队列（MQ）是一种应用程序对应用程序的通信方法。。 #JMS规范# JMS(Java Message Service) 即Java消息服务。它提供标准的产生、发送、接收消息的接口简化企业应用的开发。它支持两种消息通信模型：点到点（point-to-point）（P2P）模型和发布/订阅（Pub/Sub）模型。 当采用点对点模型时，消息将发送到一个队列，该队列的消息只能被一个消费者消费。 而采用发布订阅模型时，消息可以被多个消费者消费。在发布订阅模型中，生产者和消费者完全独立，不需要感知对方的存在。 消息如何从producer端达到consumer端由message-routing来决定。在JMS中，消息路由非常简单，由producer和consumer链接到同一个queue（p2p）或者topic（pub/sub）来实现消息的路由。JMSconsumer同时支持message selector（消息选择器），通过消息选择器，consumer可以只消费那些通过了selector筛选的消息。在JMS中，消息路由机制的图示如下： 几个重要概念 Destination：消息发送的目的地，也就是前面说的Queue和Topic。创建好一个消息之后，只需要把这个消息发送到目的地，消息的发送者就可以继续做自己的事情，而不用等待消息被处理完成。至于这个消息什么时候，会被哪个消费者消费，完全取决于消息的接受者。 Message：从字面上就可以看出是被发送的消息。它有下面几种类型： StreamMessage：Java 数据流消息，用标准流操作来顺序的填充和读取。 MapMessage：一个Map类型的消息；名称为 string 类型，而值为 Java 的基本类型。 TextMessage：普通字符串消息，包含一个String。 ObjectMessage：对象消息，包含一个可序列化的Java 对象 BytesMessage：二进制数组消息，包含一个byte[]。 XMLMessage: 一个XML类型的消息。 最常用的是TextMessage和ObjectMessage。 Session：与JMS提供者所建立的会话，通过Session我们才可以创建一个Message。 Connection：与JMS提供者建立的一个连接。可以从这个连接创建一个会话，即Session。 ConnectionFactory:那如何创建一个Connection呢？这就需要下面讲到的ConnectionFactory了。通过这个工厂类就可以得到一个与JMS提供者的连接，即Conection。 Producer：消息的生产者，要发送一个消息，必须通过这个生产者来发送。 MessageConsumer：与生产者相对应，这是消息的消费者或接收者，通过它来接收一个消息。 收发消息的对象创建过程MS规范中，收发消息的对象创建过程如下： 1. 初始化ConnetionFactory 2. ConnetionFactory创建Connection 3. Connection创建Session 4. Session创建Destination（包括Queue 和 Topic两种） 5.发： Session创建消息生产者MessageProducer（收：Session创建消息消费者MessageConsumer） 6.Seesion创建Message，（发：）MessageProducer发送到Destination，（收：）MessageConsumer从Destination接受消息。 ActiveMQActiveMQ 是Apache出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现,尽管JMS规范出台已经是很久的事情了,但是JMS在当今的J2EE应用中间仍然扮演着特殊的地位。 主要特点 多种语言和协议编写客户端。语言: Java, C, C++, C#, Ruby, Perl, Python, PHP。应用协议: OpenWire,Stomp REST,WSNotification,XMPP,AMQP 完全支持JMS1.1和J2EE 1.4规范 (持久化,XA消息,事务) 对Spring的支持,ActiveMQ可以很容易内嵌到使用Spring的系统里面去,而且也支持Spring2.0的特性 通过了常见J2EE服务器(如 Geronimo,JBoss 4, GlassFish,WebLogic)的测试,其中通过JCA 1.5 resource adaptors的配置,可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上 支持多种传送协议:in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA 支持通过JDBC和journal提供高速的消息持久化 从设计上保证了高性能的集群,客户端-服务器,点对点 支持Ajax 支持与Axis的整合 可以很容易得调用内嵌JMS provider,进行测试 ActiveMQ速度非常快；一般要比jbossMQ快10倍。 ###优点 是一个快速的开源消息组件(框架)，支持集群，同等网络，自动检测，TCP，SSL，广播，持久化，XA，和J2EE1.4容器无缝结合，并且支持轻量级容器和大多数跨语言客户端上的Java虚拟机。消息异步接受，减少软件多系统集成的耦合度。消息可靠接收，确保消息在中间件可靠保存，多个消息也可以组成原子事务。缺点：ActiveMQ默认的配置性能偏低，需要优化配置，但是配置文件复杂，ActiveMQ本身不提供管理工具；示例代码少；主页上的文档看上去比较全面，但是缺乏一种有效的组织方式，文档只有片段，用户很难由浅入深进行了解，二、文档整体的专业性太强。在研究阶段可以通过查maillist、看Javadoc、分析源代码来了解。 ###安装和启动 输入activemq.bat start，启动AMQ。 AMQ的默认使用的TCP连接端口是61616，可以通过命令 netstat -an|find &quot;61616&quot;来测试是否启动。输入ctrl + c 或者使用activemq stop命令，停止AMQ。 admin工程是管理控制台 管理控制台地址：http://localhost:8161/admin 用户名密码都是admin demo地址：http://localhost:8161/demo 默认不启动，需要在jetty中配置 ###管理端说明 http://lhbthanks.iteye.com/blog/1940767 Number Of Consumers 消费者 这个是消费者端的消费者数量 Number Of Pending Messages 等待消费的消息 这个是当前未出队列的数量。可以理解为总接收数-总出队列数Messages Enqueued 进入队列的消息 进入队列的总数量,包括出队列的。 这个数量只增不减Messages Dequeued 出了队列的消息 可以理解为是消费这消费掉的数量 ActiveMQ应用场景1、 不同语言应用集成ActiveMQ 中间件用Java语言编写，因此自然提供Java客户端 API。但是ActiveMQ 也为C/C++、.NET、Perl、PHP、Python、Ruby 和一些其它语言提供客户端。在你考虑如何集成不同平台不同语言编写应用的时候，ActiveMQ 拥有巨大优势。在这样的例子中，多种客户端API通过ActiveMQ 发送和接受消息成为可能，无论使用的是什么语言。此外，ActiveMQ 还提供交叉语言功能，该功能整合这种功能，无需使用远程过程调用（RPC）确实是个优势，因为消息协助应用解耦。 2、 作为RPC的替代使用RPC同步调用的应用十分普遍。假设大多数客户端服务器应用使用RPC，包括ATM、大多数WEB应用、信用卡系统、销售点系统等等。尽管很多系统很成功，但是转换使用异步消息可以带来很多好处，而且也不会放弃响应保证。使用同步请求的系统在规模上有较大的限制，因为请求会被阻塞，从而导致整个系统变慢。如果使用异步消息替代，可以很容易增加额外的消息接收者，使得消息能被并发消耗，从而加快请求处理。当然，你的系统应用间应该是解耦的。 3、 应用之间解耦正如之前讨论的，紧耦合架构可以导致很多问题，尤其是如果他们是分布的。松耦合架构，在另一方面，证实了更少的依赖性，能够更好地处理不可预见的改变。不仅可以在系统中改变组件而不影响整个系统，而且组件交互也相当的简单。相比使用同步的系统（调用者必须等待被调用者返回信息），异步系统（调用方发送消息后就不管，即fire-and-forget）能够给我们带来事件驱动架构（event-driven architecture EDA）。 4、 作为事件驱动架构的主干解耦，异步架构的系统允许通过代理器自己配置更多的客户端，内存等（即vertical scalability）来扩大系统，而不是增加更多的代理器（即horizontal scalability）。考虑如亚马逊这样繁忙的电子商务系统。当用户购买物品，事实上系统需要很多步骤去处理，包括下单，创建发票，付款，执行订单，运输等。但是用户下单后，会立即返回“谢谢你下单”的界面。不只是没有延迟，而且用户还会受到一封邮件表明订单已经收到。在亚马逊下单的例子就是一个多步处理的例子。每一步都由单独的服务去处理。当用户下单是，有一个同步的体积表单动作，但整个处理流程并不通过浏览器同步处理。相反地，订单马上被接受和反馈。而剩下的步骤就通过异步处理。如果在处理过程中出错，用户会通过邮件收到通知。这样的异步处理能提供高负载和高可用性。 5、 提高系统扩展性很多使用事件驱动设计的系统是为了获得高可扩展性，例如电子商务，政府，制造业，线上游戏等。通过异步消息分开商业处理步骤给各个应用，能够带来很多可能性。考虑设计一个应用来完成一项特殊的任务。这就是面向服务的架构（service-oriented architecture SOA）。每一个服务完成一个功能并且只有一个功能。应用就通过服务组合起来，服务间使用异步消息和最终一致性。这样的设计便可以引入一个复杂事件处理概念（complex event processing CEP）。使用CEP，部件间的交互可以被记录追踪。在异步消息系统中，可以很容易在部件间增加一层处理。 http://shmilyaw-hotmail-com.iteye.com/blog/1897635 ###其他注意的地方接收和处理消息的方法有两种，分为同步和异步的，一般同步的方式我们是通过MessageConsumer.receive()方法来处理接收到的消息。而异步的方法则是通过注册一个MessageListener的方法，使用MessageConsumer.setMessageListener()。 RabbitMQhttp://www.cnblogs.com/leocook/p/mq_rabbitmq_0.html RabbitMQ是流行的开源消息队列系统，用erlang语言开发。RabbitMQ是AMQP（高级消息队列协议）的标准实现。他遵循Mozilla Public License开源协议。 ###安装和启动### wget http://www.erlang.org/download/otp_src_17.3.tar.gz tar zxvf otp_src_17.3.tar.gz ./configure --prefix=/home/ww/erlang sudo aptitude install libncurses5-dev make make install tar zxvf rabbitmq-server-generic-unix-3.3.5.tar.gz export PATH=$PATH:/usr/rabbitmq_server-3.3.5/sbin ./rabbitmq-server start abbitmq web管理页面插件安装 ./rabbitmq-plugins enable rabbitmq_management 之后，netstat -napt|grep 5672 就会看到 tcp 0 0 0.0.0.0:15672 0.0.0.0: LISTEN 2506/beam.smptcp 0 0 0.0.0.0:55672 0.0.0.0: LISTEN 2506/beam.smptcp 0 0 :::5672 :::* LISTEN 2506/beam.smp 通过 http://127.0.0.1:15672，和guest:guest的用户名密码就能登录管理页面了 流程图 RabbitMQ的消息发送模型核心思想是生产者不直接把消息发送到消息队列中 几个概念Broker：即消息队列服务器实体 Exchange：消息交换机，它指定消息按什么规则，路由到哪个队列。 Queue：消息队列载体，每个消息都会被投入到一个或多个队列。 Binding：绑定，它的作用就是把exchange和queue按照路由规则绑定起来。 Routing Key：路由关键字，exchange根据这个关键字进行消息投递。 vhost：虚拟主机，一个broker里可以开设多个vhost，用作不同用户的权限分离。 producer：消息生产者，就是投递消息的程序。 consumer：消息消费者，就是接受消息的程序。 channel：消息通道，在客户端的每个连接里，可建立多个channel，每个channel代表一个会话任务 ### 工作过程 生产者客户端： 客户端连接到RabbitMQ服务器上，打开一个消息通道（channel）； 客户端声明一个消息交换机（exchange），并设置相关属性。 客户端声明一个消息队列（queue），并设置相关属性。 客户端使用routing key在消息交换机（exchange）和消息队列（queue）中建立好绑定关系。 客户端投递消息都消息交换机（exchange）上 客户端关闭消息通道（channel）以及和服务器的连接。 服务器端： exchange接收到消息后，根据消息的key和以及设置的binding，进行消息路由，将消息投递到一个或多个消息队列中。 exchange类型(1). Direct交换机：完全根据key进行投递。例如，绑定时设置了routing key为abc，客户端提交信息提交信息时只有设置了key为abc的才会投递到队列； (2).Topic交换机：在key进行模式匹配后进行投递。例如：符号”#”匹配一个或多个字符，符号””匹配一串连续的字母字符，例如”abc.#”可以匹配”abc.def.ghi”，而”abc.”只可以匹配”abc.def”。 (3).Fanout交换机：它采取广播模式，消息进来时，将会被投递到与改交换机绑定的所有队列中。 RabbitMQ默认有一个exchange，叫default exchange，它用一个空字符串表示，它是direct exchange类型 RabbitMQ的消息持久化RabbitMQ支持数据持久化，也就是把数据写在磁盘上，可以增加数据的安全性。消息队列持久化包括三个部分： 消息交换机（exchange）持久化，在声明时指定durable为1消息队列（queue）持久化，在声明时指定durable为1消息持久化，在投递时指定delivery_mode为2（1是非持久化）如果消息交换机（exchange）和消息队列（queue）都是持久化的话，那么他们之间的绑定（Binding）也是持久化的。如果消息交换机和消息队列之间一个持久化、一个非持久化，那么就不允许绑定。 其他在向消费者推送某条消息后，RabbitMQ会立刻删除掉这条消息。这样的话，如果我们kill掉某个worker的话，那么我们将会流失掉该worker正在处理任务的消息（改任务未处理完成），我们也会丢失所有被发送到这个消费者且未处理完成的消息。 但是，我们不想丢失这部分消息，我们希望这类消息可以再次被发送到其它worker那。 为了保证永远不会丢失消息，RabbitMQ支持消息应答机制。当消费者接收到消息并完成任务后会往RabbitMQ服务器发送一条确认的命令，然后RabbitMQ才会将消息删除。 RabbitMQ接收到消息后，首先会把该消息写到内存缓冲区中，并不是直接把单条消息实时写到磁盘上的。消息的持久化不是健壮的，但是对于简单的任务队列是够用了。如果你需要一套很健壮的持久化方案，那么你可以使用publisher confirms 使用消息应答机制和prefetchCount可以实现一个工作队列了。持久化的选项可以使任务即使队列和消息即使在RabbitMQ重启后，依然不会丢失。 总结： 消费者端在信道上打开消息应答机制，并确保能返回接收消息的确认信息，这样可以保证消费者发生故障也不会丢失消息。 服务器端和客户端都要指定队列的持久化和消息的持久化，这样可以保证RabbitMQ重启，队列和消息也不会。 指定消费者接收的消息个数，避免出现消息均匀推送出现的资源不合理利用的问题。 ### 负载均衡 ### 实现——一个已经命名的队列里发送和接收消息实现——工作队列 使用Work Queue,rabbitMQ把每条任务消息只发给一个消费者。 工作队列的主要思想就是避开立刻处理某个资源消耗交大的任务并且需要等待它执行完成。取而代之的是我们可以将它加入计划列表，并在后边执行这些任务。我们将任务分装成一个消息，并发送到队列中。后台的工作程序在接收到消息后将会立刻执行任务。当运行多个执行器时，任务将会在他们之间共享。 这个概念在web应用程序中是比较实用的，对于一些在一个短的http请求里无法完成的复杂任务。 channel执行basicConsume方法时autoAck为false，这就意味着接受者在收到消息后需要主动通知RabbitMQ才能将该消息从队列中删除，否则该在接收者跟MQ连接没断的情况下，消息将会变为untracked状态，一旦接收者断开连接，消息重新变为ready状态。 RabbitMQ不允许你重新定义一个已经存在的消息队列，如果你尝试着去修改它的某些属性的话，那么你的程序将会报错。所以，这里你需要更换一个消息队列名称。 标记消息持久化并不能百分百的保证消息一定不会被丢失，虽然RabbitMQ会把消息写到磁盘上，但是从RabbitMQ接收到消息到写到磁盘上，这个短时间的过程中发生的RabbitMQ重启依然会使得为写入到磁盘的消息被丢失。事实上是这样的，RabbitMQ接收到消息后，首先会把该消息写到内存缓冲区中，并不是直接把单条消息实时写到磁盘上的。消息的持久化不是健壮的，但是对于简单的任务队列是够用了。如果你需要一套很健壮的持久化方案，那么你可以使用publisher confirms（稍后会更新详细的使用方法）。 发布/订阅 Publish/Subscribe把一条消息推送给多个消费者，这种模式被称为publish/subscribe（发布/订阅） 无论什么时候我们和RabbitMQ建立连接时，我们都要刷新、清空Queue。为了达到这一的目的，我们可以用一个随机的名字（随机性可由自己来定义）来创建Queue，也可以让服务器来自动建立一个随见的Queue。当消费者断开连接时，Queue能自动被删除。使用Java客户端时，我们使用无参数的queueDeclare方法，就可以创建一个已经生成名字的、排他性的且会自动删除的Queue： String queueName = channel.queueDeclare().getQueue(); 消费者端怎么才能拿到生产者发送消息中的部分消息?一个感兴趣的日志级别进行绑定 更复杂一点的使用Topic类型的exchange 用RabbitMQ来构建一个RPC系统RPC工作原理如下： 当Client启动时，它将会创建一个匿名的callback queue。对于一次RPC请求，client会发送一条含有两个属性的消息：replyTo和correlationId。Reply是设置的callback queue，correlationId是设置的当前请求的标示符。请求将会被发送到rpc_queue里。RPC的worker（RPC server）等待queue中的请求。当出现一个请求之后，他将会处理任务，并向replyTo队列中发送消息。客户端会等待callback queue上的消息。当消息出现时，它将会检查correlationId属性是否能与之前发送请求时的属性一直，若一致的话，client将会处理回复的消息。 RocketMQRocketMQ是一款分布式、队列模型的消息中间件，具有以下特点： 支持严格的消息顺序； 支持Topic与Queue两种模式； 亿级消息堆积能力； 比较友好的分布式特性； 同时支持Push与Pull方式消费消息； Kafka总结参考文献 ActiveMQ学习笔记 Kafka学习笔记01 ActiveMQ学习笔记01 - 客户端与服务器之间的传输连接 推库ActiveMQ ActiveMQ In Action 学习笔记 JMS、AMQP实例讲解 用Spring JMS使异步消息变得简单 RabbitMQ学习总结 CentOS 7 安装RabbitMQ 3.3 MQ双修之(ActiveMq &amp; RabbitMQ) 百度rabbitmq activemq的几种基本通信方式总结 ActiveMQ学习笔记-MQ、JMS以及ActiveMQ 关系的理解-深入掌握JMS-消息队列之JMS和AMQP对比","categories":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://yoursite.com/tags/消息队列/"}],"keywords":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}]},{"title":"容器构建微服务","slug":"011容器技术/容器构微服务","date":"2016-09-04T00:00:00.000Z","updated":"2018-12-10T16:08:12.049Z","comments":true,"path":"category/011容器技术/容器构微服务.html","link":"","permalink":"http://yoursite.com/category/011容器技术/容器构微服务.html","excerpt":"","text":"docker概念编写dockerfile docker build -t spring-boot-first . docker run -i -t –name maventest maven:latest /bin/bash http://blog.csdn.net/lsgqjh/article/details/72597786 Maven Docker镜像使用技巧http://www.cnblogs.com/ilinuxer/p/6649029.html #jeck #参考网站 # dock安装 Gitlab+Jenkins+docker完成Maven项目的自动部署https://wayearn.com/2016/12/gitlab-jenkins-docker-maven/ 创建Dockerfile文件http://www.cnblogs.com/softlin/archive/2015/03/30/4377160.html Git+Spring-boot+Docker+ Maven +Registry私有仓库 +jenkins 持续集成测试http://blog.csdn.net/lsgqjh/article/details/72597786 http://blog.csdn.net/u013257425/article/details/53493113 使用Docker搭建Java环境http://www.cnblogs.com/tianrongyao/articles/6046716.html","categories":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"}],"keywords":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}]},{"title":"Docker简介和安装","slug":"011容器技术/docker入门","date":"2016-09-04T00:00:00.000Z","updated":"2018-12-10T15:19:53.631Z","comments":true,"path":"category/011容器技术/docker入门.html","link":"","permalink":"http://yoursite.com/category/011容器技术/docker入门.html","excerpt":"","text":"Docker简介一个容器实质是运行在宿主机器上的一个进程，最小组成公式为：容器＝cgroups + namespace + rootfs + engine Docker历史 column column 2008 年 Solomon Hykes 为了建立一个与编程语言无关的平台即服务产品，创立了 dotCloud 公司。 2013 年 3 月 dotCloud 将其核心组件 Docker 开源 6 个月内 Docker 早期的版本只是在简单封装 LXC 以及联合文件系统（union filesystem）之上多加了点东西，而在随后的6 个月内，Docker 就在 GitHub 上获得了 6700 多颗星，以及 175 名非公司员工的贡献者。这导致 dotCloud 把公司名称改为 Docker，并将公司的商业模式重新定位 2013 年 9 月 红帽于 2013 年 9 月成为了它的主要合作伙伴，利用 Docker 来驱动它的 OpenShift 云业务。谷歌、亚马逊和 DigitalOcean 也迅速地在其云服务平台提供 Docker 的支持 2014 年 6 月 Docker 1.0发布，代表着稳定性与可靠性的飞跃，同时，Docker 推出了一个名为 Docker Hub 的公共容器仓库，这标志着 Docker 从一个单纯的容器引擎开始转变为一个完整的平台。 2014 年 12 月 在 2014 年 12 月举行的 DockerConEU 上，Docker Swarm 与 Docker Machine 同时面世。 Docker Swarm 是一个 Docker 集群管理工具，而 Docker Machine 是个部署 Docker 主机的命令行工具。这表明 Docker 的意图不仅仅是提供 Docker 引擎，而是提供一个完整且综合的容器运行方案。 2015 年 6 月 DockerCon 在旧金山举行，来自 Docker 的 Solomon Hykes 与来自 CoreOS 的 Alex Polvi 宣布开放容器促进会（当时称为开放容器计划，Open Container Project）正式成立，目的是要发展出一套通用的容器格式与运行环境的标准。 2015 年 6 月 FreeBSD 项目宣布支持 Docker 利用 ZFS 和 Linux 兼容层来运行 2015 年 8 月 Docker 与微软推出专为 Windows server开发的 Docker Engine“技术预览”版 Docker安装微服务是容器最主要的用例，也是容器技术兴起的最大推动力。64 位 Linux 是唯一一个能稳定运行 Docker 且适合用于生产环境的平台 简单的安装sudo apt-get update sudo apt-get install -y docker.io 官方源安装 curl https://get.docker.com &gt; /tmp/docker/install.sh chmod +x /tmp/docker/install.sh /tmp/docker/install.sh 进行测试开启docker的守护进程sudo service docker start 配置开机启动,执行sudo systemctl enable docker 不使用sudo命令执行Docker因为 Docker 运行时需要特殊权限，所以默认执行命令时都必须在前面加上 sudo。将当前用户加入到组docker下， 加入用户到docker组中, sudo usermod -aG docker myuser 以后就不需要使用sudo来执行了。可能需要重新启动，Docker 服务sudo service docker restart docker组是在安装docker的时候自动建立的可以通过cut -d: -f1 /etc/group | grep docker查看需要注意：重新启动计算机或者注销用户再登入，才能生效。 测试安装docker version 查看正在运行的容器 Client: Version: 1.12.6 API version: 1.24 Go version: go1.6.2 Git commit: 78d1802 Built: Tue Jan 31 23:35:14 2017 OS/Arch: linux/amd64 Server: Version: 1.12.6 API version: 1.24 Go version: go1.6.2 Git commit: 78d1802 Built: Tue Jan 31 23:35:14 2017 OS/Arch: linux/amd64 上面的结果表示已经准备就绪，如果有错误，可以sudo docker daemon 来手动启动 Docker 守护进程，并查看错误信息。 其他操作更新Docker`sudo apt-get upgrade docker-engine` 卸载Docker`sudo apt-get purge docker-engine` 进行加速Docker Hub是全球最大的Docker共用仓库，为了解决国内用户使用 Docker Hub 时遇到的稳定性及速度问题，可以改变默认镜像地址。可以参考http://guide.daocloud.io/dcs/daocloud-9153151.html curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://d22ab3fe.m.daocloud.io 执行成功后，如果Docker 版本在 1.12 或更高，会创建或修改 /etc/docker/daemon.json 文件{“registry-mirrors”: [“http://d22ab3fe.m.daocloud.io&quot;]} 进行重新启动 sudo systemctl restart docker.service 第一个命令 docker run ubuntu echo &quot;Hello World&quot; 出现hello Harbor安装 $ wget https://github.com/vmware/harbor/releases/download/v1.1.2/harbor-online-installer-v1.1.2.tgz $ tar xvf harbor-online-installer-v1.1.2.tgz 用户名： admin密码： Harbor12345 修改/lib/systemd/system/docker.service # Modified,origin: ExecStart=/usr/bin/dockerd -H fd:// ExecStart=/usr/bin/docker daemon -H fd:// --insecure-registry xx.xxx.xx.xx:5000 然后执行命令： sudo systemctl daemon-reload sudo systemctl restart docker 重启sudo docker-compose stopsudo docker-compose up -d 其他目录文件/etc/default/docker/etc/init/docker.conf/etc/init.d/docker 问题然而在某些情况下（例如， Ubuntu Docker 容器）， net-tools 工具包将不会被默认安装，这就意味着不能使用 ifconfig 。尽管如此，还是可以用软件仓库来安装 net-tools 。sudo apt-get install net-tools https://blog.csdn.net/aixiaoyang168/article/details/73549898https://www.cnblogs.com/huangjc/p/6420355.html #参考网站 # dock安装","categories":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}],"tags":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/tags/中间件/"},{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"}],"keywords":[{"name":"容器技术","slug":"容器技术","permalink":"http://yoursite.com/categories/容器技术/"}]},{"title":"Linux中十进制和其他进制的互换","slug":"018linux/Linux数字转换","date":"2016-05-06T00:00:00.000Z","updated":"2018-05-27T09:06:00.054Z","comments":true,"path":"category/018linux/Linux数字转换.html","link":"","permalink":"http://yoursite.com/category/018linux/Linux数字转换.html","excerpt":"","text":"Linux数字转换有时需要快速查看二进制的数字为多少 其他转为十进制((num=2#11011111)) echo $num ((num=0xff)); echo $num 十进制转为其它进制十进制转八进制echo “obase=8;01234567”|bc 十进制转2进制echo “obase=2;239”|bc","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"Mac常用操作技巧","slug":"018linux/mac/0mac操作技巧","date":"2016-05-04T00:00:00.000Z","updated":"2019-03-06T03:47:56.441Z","comments":true,"path":"category/018linux/mac/0mac操作技巧.html","link":"","permalink":"http://yoursite.com/category/018linux/mac/0mac操作技巧.html","excerpt":"","text":"mdfindmdfind命令就是Spotlight功能的终端界面，这意味着如果Spotlight被禁用，mdfind命令也将无法工作 通过mdfind命令搜索文件mdfind -name &quot;Photo 1.PNG&quot; 通过-onlyin参数搜索特定文件夹的内容mdfind -onlyin ./ 关于我们 linux文件字符集转换gb2312 转为 utf8iconv -t utf-8 -f gb2312 Join.java &gt; Join2.java 参看内存top -l 1 | head -n 10 | grep PhysMem dudu -h -d 2","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"常用","slug":"常用","permalink":"http://yoursite.com/tags/常用/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}]},{"title":"Java泛型介绍","slug":"020programming/scala/java范型介绍","date":"2016-02-07T00:00:00.000Z","updated":"2018-05-27T09:06:00.100Z","comments":true,"path":"category/020programming/scala/java范型介绍.html","link":"","permalink":"http://yoursite.com/category/020programming/scala/java范型介绍.html","excerpt":"","text":"super和extentsList&lt;? extends E&gt;表示该list集合中存放的都是E的子类型（包括E自身），由于E的子类型可能有很多，但是我们存放元素时实际上只能存放其中的一种子类型（这是为了泛型安全，因为其会在编译期间生成桥接方法该方法中会出现强制转换，若出现多种子类型，则会强制转换失败），例子如下： List&lt;? extends Number&gt; list=new ArrayList&lt;Number&gt;(); list.add(4.0);//编译错误 list.add(3);//编译错误 上例中添加的元素类型不止一种，这样编译器强制转换会失败，为了安全，Java只能将其设计成不能添加元素。 虽然List&lt;? extends E&gt;不能添加元素，但是由于其中的元素都有一个共性–有共同的父类，因此我们在获取元素时可以将他们统一强制转换为E类型，我们称之为get原则。 对于List&lt;? super E&gt;其list中存放的都是E的父类型元素（包括E），我们在向其添加元素时，只能向其添加E的子类型元素（包括E类型），这样在编译期间将其强制转换为E类型时是类型安全的，因此可以添加元素，例子如下： List&lt;? super Number&gt; list=new ArrayList(); list.add(2.0); list.add(3.0);但是，由于该集合中的元素都是E的父类型（包括E），其中的元素类型众多，在获取元素时我们无法判断是哪一种类型，故设计成不能获取元素，我们称之为put原则。 实际上，我们采用extends，super来扩展泛型的目的是为了弥补例如List只能存放一种特定类型数据的不足，将其扩展为List&lt;? extends E&gt; 使其可以接收E的子类型中的任何一种类型元素，这样使它的使用范围更广。 Java 和 Scala 均支持协变，逆变和非转化类型。然而，在 Scala 中，转化行为的定义是类型声明的一部分，称为转化标记（variance annotation）。 而Java 中参数化的类型在定义时并未声明继承转化行为，而是在使用该类型时，也就是在声明变量时，才指定参数化类型的转化行为。 WoodDuck 》 Duck 》Animal 定义一个zoo对象，可以新增动物。 public class Zoo&lt;T&gt; { List&lt;Object&gt; list = new ArrayList(); public void add(T animal) { list.add(animal); } } public class ZoosTest { public static void main(String[] args) { Zoo&lt;Animal&gt; zoo = new Zoo&lt;Animal&gt;(); zoo.add(new Duck()); // Zoo&lt;Animal&gt; zoo1 = new Zoo&lt;Duck&gt;(); //编译错误 } } Zoo zoo1 = new Zoo()出现编译错误？ 如何解决？ &lt;? extends T&gt;和&lt;? super T&gt;就是为此出现的。 super表示下界，而extends表示上界 super的使用举例， public static void main(String[] args) { List&lt;? super Duck&gt; list3 = new ArrayList&lt;Duck&gt;(); list3.add(new Duck()); list3.add(new WoodDuck()); // list3.add(new Animal()); List&lt;? super Duck&gt; list4 = new ArrayList&lt;Animal&gt;(); list4.add(new Duck()); list4.add(new WoodDuck()); //list4.add(new Animal()); //List&lt;? super Duck&gt; list5 = new ArrayList&lt;WoodDuck&gt;(); //不是父类,不能编译 } List&lt;? super Duck&gt; list3 表示元素父类至少为Duck extends举例 List&lt;? extends Duck&gt; list3 = new ArrayList&lt;Duck&gt;(); list3.add(new WoodDuck()); 会出现编译错误 下面的是正确的 List&lt; Duck&gt; list3 = new ArrayList&lt;Duck&gt;(); list3.add(new WoodDuck()); 参考网站 文档 http://hongjiang.info/message/http://zhuanlan.51cto.com/art/201703/534053.htm","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"JavaScript模块系统简介","slug":"020programming/javascript/AMD和CommonJS相比，UMD","date":"2016-02-07T00:00:00.000Z","updated":"2018-05-27T09:06:00.109Z","comments":true,"path":"category/020programming/javascript/AMD和CommonJS相比，UMD.html","link":"","permalink":"http://yoursite.com/category/020programming/javascript/AMD和CommonJS相比，UMD.html","excerpt":"","text":"#引言 由于Javascript的标准没有对模块的规范进行定义，人们又定义了一系列不同的模块定义： CommonJS 、 AMD 、 CMD 、 UMD。所幸的是ES6 Module的出现可能中止这种分裂的状态。 CommonJS主要是用于服务器端的规范，比如目前的nodeJS $ npm install react react-bootstrap var Alert = require(‘react-bootstrap/lib/Alert’);// orvar Alert = require(‘react-bootstrap’).Alert; CommonJS 加载模块是同步的.所以只有加载完成才能执行后面的操作。 像Node.js主要用于服务器的编程，加载的模块文件一般都已经存在本地硬盘，所以加载起来比较快，不用考虑异步加载的方式，所以CommonJS规范比较适用。 但如果是浏览器环境，要从服务器加载模块，这是就必须采用异步模式。所以就有了 AMD CMD 等解决方案 AMD (Asynchromous Module Definition)AMD 是 RequireJS 在推广过程中对模块定义的规范化产出 AMD异步加载模块。它的模块支持对象 函数 构造器 字符串 JSON等各种类型的模块。 适用AMD规范适用define方法定义模块。 $ bower install react react-bootstrap define([‘react-bootstrap’], function(ReactBootstrap) { var Alert = ReactBootstrap.Alert; … });AMD 运行时核心思想是「Early Executing」，也就是提前执行依赖。 CMD (Common Module Definition)通用模块定义,CMD是SeaJS 在推广过程中对模块定义的规范化产出CMD和AMD的区别有以下几点： 1.对于依赖的模块AMD是提前执行，CMD是延迟执行。 2.CMD推崇依赖就近，按需加载；AMD推崇依赖前置。 UMDumd是AMD和CommonJS的糅合 AMD 浏览器第一的原则发展 异步加载模块。 CommonJS 模块以服务器第一原则发展，选择同步加载，它的模块无需包装(unwrapped modules)。 这迫使人们又想出另一个更通用的模式UMD （Universal Module Definition）。希望解决跨平台的解决方案。 UMD先判断是否支持Node.js的模块（exports）是否存在，存在则使用Node.js模块模式。 在判断是否支持AMD（define是否存在），存在则使用AMD方式加载模块。 (function (window, factory) { if (typeof exports === &apos;object&apos;) { module.exports = factory(); } else if (typeof define === &apos;function&apos; &amp;&amp; define.amd) { define(factory); } else { window.eventUtil = factory(); } })(this, function () { //module ... }); ES6es6 中新增了两个命令 export 和 import , export 命令用于规定模块的对外接口，import 命令用于输入其他模块提供的功能。 一个模块就是一个独立的文件。该文件内部的所有变量，外部无法获取。如果你希望外部能够读取模块内部的某个 变量，就必须使用export关键字输出该变量。 $ npm install react react-bootstrap import Button from ‘react-bootstrap/lib/Button’;// orimport { Button } from ‘react-bootstrap’; 参考 关于 CommonJS AMD CMD UMD 规范的差异总结","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://yoursite.com/tags/JavaScript/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"架构师的能力和职责","slug":"050它山之玉/架构师的能力和职责","date":"2016-02-07T00:00:00.000Z","updated":"2018-12-10T15:51:53.895Z","comments":true,"path":"category/050它山之玉/架构师的能力和职责.html","link":"","permalink":"http://yoursite.com/category/050它山之玉/架构师的能力和职责.html","excerpt":"","text":"背景文章出自http://blogger.org.cn/blog/more.asp?name=hongrui&amp;id=49222 发现总结的很好，所以摘录如下。 观点架构师是技术领导架构师必须要有技术，而且还是领导。架构师要带领自己团队完成自己的任务，完全凭借自己的能力做事情，完全是匹夫之勇，根本不提倡。 架构师理解软件流程架构师必须了解软件流程，否则无法驱动整个团队前进，如果一个架构师不熟悉开发流程，无法协调产品线相关人员进行高效工作，也无法指导团队成员完成自己的工作。所以来说架构师一般不是空降兵（除非是全新的部门），因为空降兵一般不会熟悉新公司的开发流程，即使是同一个行业的，各个公司的差别还是很大。所以说听说某某去某某公司做首席架构师或者首席科学家，一般是高风险的事情，即使他对这个行业很了解。 架构师必须熟悉业务领域如果一个架构师不熟悉自己的行业，做的架构就是纸上谈兵，熟悉业务领域的架构师，才能很好的理解需求，做出合适的方案。互联网和网络安全是完全不同的两个方向，即使你熟悉里面的各种具体技术，但是以互联网的架构来做网络安全产品，肯定是100%的失败。 架构师必须要有广度的知识架构师考虑的问题必须全面，必须了解的要广，具体的细节可以不关注，因为细节变化很快。很多具体技术人员出身的架构师，只关注于具体的细节，某些方面做的很好，整体的性能很差。 架构师必须是写程序的高手架构师一般都是开发人员出身，一般都是团队的核心。优秀的架构师应该了解团队使用各种技术，有了这些知识，才能和开发人软进行有效沟通。 架构师是优秀的沟通人员架构师一定要会忽悠，至少要扯淡。架构师不但要指导本部门员工的工作，也要协调其他部门的资源，还要向用户收集需求，制定规格说明书，重要的把用户的不合理要求砍掉，合理需求遵循自己的思路。","categories":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}],"tags":[{"name":"架构","slug":"架构","permalink":"http://yoursite.com/tags/架构/"}],"keywords":[{"name":"他山之石","slug":"他山之石","permalink":"http://yoursite.com/categories/他山之石/"}]},{"title":"分布式基础","slug":"010基础技术/分布式理论/分布式基础","date":"2016-02-07T00:00:00.000Z","updated":"2018-12-10T16:33:35.858Z","comments":true,"path":"category/010基础技术/分布式理论/分布式基础.html","link":"","permalink":"http://yoursite.com/category/010基础技术/分布式理论/分布式基础.html","excerpt":"","text":"#分布式架构风格# 常见的分布式应用架构风格有三种： 分布式对象（Distributed Objects，简称DO） 远程过程调用（Remote Procedure Call，简称RPC） 表述性状态转移（Representational State Transfer，简称REST DO架构实例有CORBA/RMI/EJB/DCOM/.NET Remoting等等,DO风格的架构通常都是与某种编程语言绑定的。 RPC架构实例有SOAP/XML-RPC/Hessian/Flash AMF/DWR等等，RPC中没有使用超文本，响应的内容中只包含消息本身。 rest架构实例有HTTP/WebDAV,REST中的资源，则完全中立于开发平台和编程语言，可以使用任何编程语言来实现。REST使用了超文本，可以实现更大粒度的交互，交互的效率比RPC更高。 #架构模型# 采取SOA的架构模型采取EDA的架构模型 ##SOA ###Web services与SOA 根据2003年4月的Gartner报道，Yefim V. Natis就这个问题是这样解释的：“Web服务是技术规范，而SOA是设计原则。特别是Web服务中的WSDL，是一个SOA配套的接口定义标准：这是Web服务和SOA的根本联系。从本质上来说，SOA是一种架构模式，而Web服务是利用一组标准实现的服务,Web服务是实现SOA的方式之一。 ###Web services使用的技术 XML：XML是在web上传送结构化数据的伟大方式，Web services要以一种可靠的自动的方式操作数据 。 SOAP：SOAP使用XML消息调用远程方法，这样web services可以通过HTTP协议的post和get方法与远程机器交互，而且，SOAP更加健壮和灵活易用；SOAP基于HTTP的，在http协议下传输xml文件。 其他 : 像UDDI和WSDL技术,与XML和SOAP技术紧密结合用于服务发现 ###Web services调用过程 Web Service客户端调用Web Service的基本过程： 构造SOAP请求消息（将本地数据对象转换为SOAP消息） 发送SOAP消息到Web Service服务器的指定端口 接收SOAP响应消息 将SOAP响应消息转换为本地数据对象 webService效率较慢,传输的是文本, 而EJB和CORBA传输的是二进制,效率较好可以近视的理解SOAP=RPC+HTTP+XML：采用HTTP作为底层通讯协议；RPC作为一致性的调用途径，XML作为数据传送的格式，允许服务提供者和服务客户经过防火墙在INTERNET进行通讯交互。 但这个过程，由JAX-WS规范来处理，JAX-WS是一组XML web services的JAVA API，在JAX-WS中，一个远程调用可以转换为一个基于XML的协议例如SOAP，在使用JAX-WS过程中，开发者不需要编写任何生成和处理SOAP消息的代码。JAX-WS的运行时实现会将这些API的调用转换成为对应的SOAP消息。java中最流行的是cxf,提供了对 JAX-WS 全面的支持。 ###SOAP性能考虑 SOAP 使用 HTTP 传送 XML，尽管HTTP 不是有效率的通讯协议，而且 XML 还需要额外的文件解析（parse），两者使得交易的速度大大低于其它方案。但是XML 是一个开放、健全、有语义的讯息机制，而 HTTP 是一个广泛又能避免许多关于防火墙的问题，从而使SOAP得到了广泛的应用。但是如果效率对你来说很重要，那么你应该多考虑其它的方式，而不要用 SOAP。 XML-RPC和SOAP都是基于XML格式的消息交换.而JSON-RPC是基于JSON格式的消息交换，JSON比XML更加轻巧，并且非常容易在页面JS中使用 ###Webservice vs RPC rpc本身没有规范，只要是实现了远程调用的，都是rpc，实现方式有RMI,webservice. webservice只是一种实现方式rpc有一个通用的结构，就是serialization/deserialization+stub+skeletonrpc是一种远程调用设计思维，强调的是透明的调用远程的方法，看起来就跟本地调用一样。 ##EDA的架构模型 #架构选型要素 ##通信协议的选择## Http/Https, 例如webserivcetcp 例如HSF，Dubbo，Facebook thrift Reactor模型统一调度长连接和短连接协议栈，无论在性能/可靠性/还是可维护性，都可以“秒杀”基于传统的BIO开发的应用服务器和各种协议栈。 ##数据交换格式## 采用JSON协议格式进行数据交换xml协议格式进行数据交换二进制 ##数据传输安全## 为了保证数据传输过程中的数据真实性和完整性，我们需要对数据进行加密签名，在接收签名数据之后进行签名校验。 目前支持签名算法：RSA、MD5，默认RSA。 ##常用业务组件## 工作流 规则引擎 消息队列 缓存 统一存储 统一加密 消息队列 总结参考 WebService与使用风格RPC/SOA/REST RESTful Webservice 和 SOAP Webserivce 对比及区别 EventBus使用详解(一)——初步使用EventBus","categories":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"}],"keywords":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}]},{"title":"什么是流计算？","slug":"010基础技术/分布式理论/流式计算","date":"2016-02-07T00:00:00.000Z","updated":"2018-12-10T16:33:54.059Z","comments":true,"path":"category/010基础技术/分布式理论/流式计算.html","link":"","permalink":"http://yoursite.com/category/010基础技术/分布式理论/流式计算.html","excerpt":"","text":"计算分类大数据的计算模式主要分为批量计算、流式计算、交互计算、图计算(graph computing)等。其中，流式计算和批量计算是两种主要的大数据计算模式，分别适用于不同的大数据应用场景。 Google MapReduce、Hadoop等处理大规模 数据变得简单。但这些处理的技术的目标都不是实时技术。但是随着大数据业务的快速增长，针对大规模数据处理的实时计算变成了一种业务上的需求，Storm类似的框架正是在这样的需求背景下出现的。 适用性对于先存储后计算，实时性要求不高，同时数据的准确性、全面性更为重要的应用场景，批量计算更加适合。 对于无需先存储，可以直接进行数据计算，实时性要求很严格，但数据的精确度往往不太苛刻的 应用场景，流式计算具有明显优势。 流计算的原理实时流计算的场景归纳起来多半是：业务系统根据实时的操作,不断生成事件（消息/调用）,然后引起一系列的处理分析,这个过程是分散在多台计算机上并行完成的,看上去就像事件连续不断的流经多个计算节点处理,形成一个实时流计算系统。 流式计算特征流式计算有点像数据库领域的触发器，大数据流式计算目前主要用于对动态产生的数据进行实时计算并及时反馈结果，但往往不要求结果绝对精确的应用场景，在数据的有效时间内获取其价值，是大数据流式计算系统的首要设计目标。 流式计算中，数据往往是最近一个时间窗口内的增量数据，因此数据时延往往较短，实时性较强，但数据的信息量往往相对较少，只限于一个时间窗口内的信息，不具有全量信息。流式计算和批量计算具有明显的优劣互补特征，在多种应用场合下可以将两者结合起来使用，通过发挥流式计算的实时性优势和批量计算的计算精度优势，满足多种应用场景在不同阶段的数据计算要求。 通常情况下，大数据流式计算场景具有以下鲜明特征：在流式计算环境中，数据是以元组为单位，以连续数据流的形态，持续地到达大数据流式计算平台。数据并不是一次全部可用，不能够一次得到全量数据，只能在不同的时间点，以增量的方式，逐步得到相应数据。 流式计算应用场景具体应用场景金融在金融银行的实时监控场景中，大数据流式计算往往体现出自身的优势。例如：在风险管理方面，包括信用卡诈骗、保险诈骗、证券交易诈骗、程序交易等，需要实时跟踪发现；营销管理方面，根据客户信用卡消费记录，掌握客户的消费习惯和偏好，预测客户未来的消费需求，并为其推荐个性化的金融产品和服务。 互联网领域在互联网领域中，大数据流式计算的典型应用场景主要包括以下方面：搜索引擎提供商们往往会在反馈给客户的搜索页面中加入点击付费的广告信息，插入什么广告、在什么位置插入这些广告才能得到最佳效果，往往需要根据客户的查询偏好、浏览历史、地理位置等综合语义进行决定，而这种计算对于搜索服务器而言往往是大量的。一方面，每时每刻都会有大量客户进行搜索请求；另一方面，数据计算的时效性极低，需要保证极短的响应时间。同理，我们构建社交网站，一样需要实时分析用户的状态信息，及时提供最新的用户分享信息到相关的群体，准确地推荐朋友、推荐主题、提升用户体验，并能及时发现和屏蔽各种欺骗行为，至此大数据流式计算的方法就会帮助我们达成这种目标定位。 物联网领域在物联网领域中，大数据流式计算的典型应用场景主要有智能交通和环境监测。通过传感器实时感知车辆、道路的状态，并分析和预测一定范围、一段时间内的道路流量情况，以便有效地进行分流、调度和指挥；环境监控则通过传感器和移动终端，对一个地区的环境综合指标进行实时监控、远程查看、智能联动、远程控制，系统地解决综合环境问题。这些对计算系统的实时性、吞吐量、可靠性等方面都提出了很高要求。 流计算框架 流式大数据处理的三种框架：Storm，Spark和Samza 比较具体参见http://www.csdn.net/article/2015-03-09/2824135 参考网站 剖析 大数据流式计算场景特征 大数据流式计算应用的种种 实时计算，流数据处理系统简介与简单分析 Storm实时处理方案架构 实时处理方案架构","categories":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}],"tags":[{"name":"流计算","slug":"流计算","permalink":"http://yoursite.com/tags/流计算/"}],"keywords":[{"name":"基础技术","slug":"基础技术","permalink":"http://yoursite.com/categories/基础技术/"}]},{"title":"需要知道的ECMAScript 6语法","slug":"020programming/javascript/es6","date":"2016-02-07T00:00:00.000Z","updated":"2018-05-27T09:06:00.109Z","comments":true,"path":"category/020programming/javascript/es6.html","link":"","permalink":"http://yoursite.com/category/020programming/javascript/es6.html","excerpt":"","text":"#ECMAScript 6常用语法 let与const关键字let关键字可以由一个块作用域。由于var关键字的作用范围是一个函数，它有时候会导致一些意想不到的bug。为此，我们可以用let来代替var来避免此类bug。 const 用于定义常量，其值不可改变。 ##默认参数 在ES6之前，不能直接为函数的参数指定默认值,只能采用如下方式： function log(x, y) { y = y || &apos;World&apos;; console.log(x, y); } 使用ES6 function foo(bar=&quot;baz&quot;) { console.log(bar); } ##箭头函数 类似与Java 8，ES6允许使用“箭头”（=&gt;）定义函数。 var f = v =&gt; v; 等同于： var f = function(v) { return v; }; =&gt;操作符左边为输入的参数，而右边则是进行的操作以及返回的值。 如果箭头函数不需要参数或需要多个参数，就使用一个圆括号代表参数部分。 var f = () =&gt; 5; // 等同于 var f = function (){ return 5 }; var sum = (num1, num2) =&gt; num1 + num2; // 等同于 var sum = function(num1, num2) { return num1 + num2; }; 如果箭头函数的代码块部分多于一条语句，就要使用大括号将它们括起来，并且使用return语句返回。 var sum = (num1, num2) =&gt; { return num1 + num2; } ##解构 ES6允许按照一定模式，从数组和对象中提取值，对变量进行赋值，这被称为解构（Destructuring）。可见近似的理解为批量赋值。 数组形式 let [x,y] = [3,4,5]; // x=3, y = 4 [x, y = &apos;b&apos;] = [&apos;a&apos;] // x=&apos;a&apos;, y=&apos;b&apos;，默认值 对象的解构赋值 var { foo, bar } = { foo: “aaa”, bar: “bbb” }; // foo =”aaa” bar =”bbb” var { foo: baz } = { foo: “aaa”, bar: “bbb” }; //baz = “aaa”,变量名与属性名不一致 ##模板 你可以在你的语句中像下面这样内嵌JavaScript变量： var firstName = “Jack”; var message = Hello ${firstName}!; // “Hello Jack!” 模块系统在ES6标准中，JavaScript原生支持module了，将不同功能的代码分别写在不同文件中，各模块只需导出公共接口部分，然后通过模块的导入的方式可以在其他地方使用。 导出 // circle.js export function area(radius) { return Math.PI * radius * radius; } export function circumference(radius) { return 2 * Math.PI * radius; } 使用export命令定义了模块的对外接口以后，其他JS文件就可以通过import命令加载这个模块（文件）。 // main.js import { area, circumference } from &apos;./circle&apos;; console.log(&quot;圆面积：&quot; + area(4)); console.log(&quot;圆周长：&quot; + circumference(14)); 参考 ECMAScript 6简介 全面解析ECMAScript 6模块系统 ECMAScript六个值得看好的特性","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://yoursite.com/tags/JavaScript/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"jdk8-lambda表达式","slug":"022java/java8-lambda","date":"2016-01-23T00:00:00.000Z","updated":"2019-02-20T07:43:52.459Z","comments":true,"path":"category/022java/java8-lambda.html","link":"","permalink":"http://yoursite.com/category/022java/java8-lambda.html","excerpt":"","text":"#lambda入门 一个简单的例子没有使用lambda public class Test1{ public static void main(String args[]){ Runnable r = new Runnable(){ public void run(){ System.out.println(&quot;hello,lambda!&quot;); } }; r.run(); } } 使用lambda public class Test2 { public static void main(String args[]) { Runnable r = () -&gt; System.out.println(&quot;hello,lambda&quot;); r.run(); } } ##函数式接口 为了配合lambda，jdk8引入了一个新的定义叫做：函数式接口（Functional interfaces）函数式接口特征:（1）是一个接口（2）只有一个待实现的方法 jdk8开始，接口可以有default方法，所以，函数式接口也是可以有default方法的，但是，只能有一个未实现的方法。 参数可以写类型，也可以不写，jvm很智能的，它能自己推算出来。方法可以有返回，也可以无返回，如果有多个语句，还要返回值，需要加上return 参考 Java8 学习笔记(一) Java 8新特性：全新的Stream API","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[],"keywords":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}]},{"title":"业务敏捷性","slug":"006系统设计/业务敏捷性","date":"2015-12-29T00:00:00.000Z","updated":"2019-02-18T01:44:21.786Z","comments":true,"path":"category/006系统设计/业务敏捷性.html","link":"","permalink":"http://yoursite.com/category/006系统设计/业务敏捷性.html","excerpt":"","text":"敏捷三种不同类型的敏捷性,战略、业务组合和运营灵活性。战略敏捷性包括发现并把握改变游戏规则的机遇的能力。业务组合敏捷性是指快速高效地将资源(包括现金、人才和管理层注意力)从前景不佳的业务领域转移到更有吸引力的领域中的能力。 业务敏捷性（Business Agility）当今的商业环境在不断地改变并且变得越来越复杂。组织、个人、公众都面临着巨大的压力，这些压力迫使他们要对变化的环境作出快速的反应，同时还要求他们在运作方法上有创新精神。这就需要组织机构灵活并且频繁快速地在战略层、战术层、操作层做出决策。商业环境因素可以分成主要的四种类型：市场因素、顾客需求因素、技术因素、社会环境因素。这些因素的影响会随着时间变得更强。这会导致更大的压力和更激烈的竞争。除此以外，为了增加利润，组织、个人、组织内的部门会面临来自更少的预算和上层管理层要求提高绩效和利润的更大压力，在这种情况下，经理们必须以更快的速度来应对，即创新和敏捷。 《世界是平的》的作者托马斯•弗里德曼在他的作品中讲了两个很有趣的故事： ※ 作者本人给位于德克萨斯州的奥斯汀的Dell管理部门写了一封信，询问他自己的Dell笔记本的各个部件来自哪些国家。得到的答复是：美国因特尔公司设在菲律宾、哥斯达黎加、马来西亚或中国的工厂生产的处理器；韩国、日本、中国台湾或德国生产的内存；中国内地或中国台湾生产的显卡…… ※ 某天早上10点，Dell发现很多客户订购的笔记本电脑都要求配备40G的硬盘，如果这样的话，两个小时后，供给链将出现断货信号，并自动地发送给Dell销售部门、公司网站以及所有的订购电话接线员。如果你正好10点半向Dell发出订单，公司的销售代表会对你说：“您现在只需要在40G硬盘价格的基础上多支付10美元，就可以得到60G的硬盘的配置。”利用这种促销手段，在一两个小时内，Dell可以根据全球供应链的情况重新塑造顾客对产品的需求结构。 上述两个故事都在讲述一件事，那就是怎样让企业的业务变得更加灵活，或者说更加敏捷。不难想象，Dell公司已经具备了比较完善的供应链管理系统、财务系统、客户关系系统等IT基础设施，这也说明Dell的企业经营管理已经依赖于IT系统的支撑。今天，绝大多数企业都具备了类似特征。不否认存在一些企业的经营管理还依赖于传统的电话沟通、面对面的洽谈会，但这已经是少数，而且会越来越少。 如今一个企业是否能够让CEO的决策、企业战略的调整、市场方向的重定位等等一系列问题快速变更与执行，几乎都依赖于信息化建设的完善。然而，这种完善程度不仅意味着信息系统在企业内部的覆盖面，更依赖于这些系统之间的协作性与敏捷性。 让我们在此用简短的一句话来为业务敏捷性下个定义： 业务敏捷性是指企业对变更快速和有效地进行响应、并且利用变更来得到竞争优势的能力。 IT系统一直在不断演变和发展，寻求对业务敏捷性的更好支持在美国、欧洲三四十年的IT系统建设中，基本经历了三个应用阶段： 第一阶段信息发布，即传统的Information Management System，在这个阶段最主要的任务是把某一个业务下的信息数据管理起来，比如早期的财务系统、人力资源系统； 第二阶段是企业系统的内部整合，当企业内部绝大多数信息系统都已经建立之后，是否能够有效地进行协作和资源整合，成为主要解决的问题； 第三个阶段，是企业内部的信息系统需要与外部环境，包括供应商、分销商和客户进行整合。 当然不是所有的企业都经历的这些阶段，也不是所有的企业都已经进入第三阶段，这只是一个整体性的发展路线，或者说趋势。 业务敏捷性取决于企业信息的自由流动、服务和业务流程。而且这也要求IT系统必须能够满足业务的变更。 事实上，满足IT系统的敏捷性，必须逾越几个最基本的障碍： 能够统一描述出各种业务，或者说业务对象与业务模型。这些业务对象和业务模型需要很容易被组合或重组。因为企业业务并非随意创建，而是由基本规则在支撑。尽管我们不断宣称需要灵活、敏捷，但这样灵活性的变更发展也必须有一定的原则。这如同企业市场开拓，从一个区域向另一个区域拓展，而绝非游击队的模式： 打一枪换一个地方。 企业IT系统一般会由多平台（IBM、BEA、Microsoft、SAP、Oracle等）和多技术（J2EE、.NET、遗留技术等）构成。大企业的异构性会更复杂。某项业务可能涉及到企业内部系统、外部环境、供应商、分销商和客户等等。这就使得业务的变更牵涉到众多合作伙伴。所以必须有更好的互联技术来满足不同系统之间的信息交互，这种信息互联必须基于统一的标准和构架，而且能够很容易定位与获取。 SOASOA为支撑业务敏捷性提供了新的思路和方法,上个世纪九十年代所诞生的了BPR（业务过程再造），除了吸引了很多眼球之外，没有形成实实在在的应用，直到如今逐渐成熟起来的BPM Suites（业务过程管理套件），才让BPR再次走入了人们的视线。 Forrester研究公司的分析师们为此还设想出了一个全新的类别：“动态业务应用”。这是根据SOA的发展原则，并在发展前程中涵盖了一种多个字母组合的服务技术。 该研究公司整敦促架构师与开发人员即可开始着手制定未来5年内的业务策略和技术实现计划，从整体出发，考虑SOA与B3技术平台。这里所说的B3技术平台即是指BPM(业务流程管理)、BI(商业智能)和业务规则。 除此之外，Forrester 研究公司所定义的“动态业务应用”将不仅仅在灵活性方面有了突出考虑，同时做到以人为本，它还提倡使用Ajax和其他的丰富互联网应用(rich Internet application，RIA)技术将Web 2.0式的社会关系网络方案合并其中。 就目前行业发展看来，有很多的企业其实已经高瞻的看到了这方面的好处，他们也在通过结合SOA与B3平台(BPM、BI、业务规则)的方法走在了通向动态业务应用的道路上，尽管这一路可能会有不少的曲折和坎坷，Forrester研究公司分析师如是说。同时他还指出了一个具体的例子，美国Washington Mutual公司的在线租贷应用系统就是使用了BPM实现了业务规则，逻辑过程的自动化，以软件的形式评估申请人及其所对应的国家制度规定，并通过BI系统完成了对客户的动态化处理。另一个例子则是Best Buy公司，作为一个电子产品零售商，它成功地将业务规则与BPM结合，从产品需求性和地域性等角度考虑，提供了适当的本地营销计划。 BPM与SOA的关系BPM与SOA的本质是截然不同的:SOA是一种架构方法;BPM则是一组流程协调管理理念。但两者的结合完美的解决了IT技术与业务的鸿沟 .SOA提供架构方法,用IT技术把系统实现为独立的服务,BPM把它们管理起来。 参考文章胡长城:SOA与业务敏捷http://soft.zdnet.com.cn/software_zone/2007/0709/417200.shtmlhttp://news.mbalib.com/story/24901http://www.ciotimes.com/cloud/cjs/74678.html","categories":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}],"tags":[{"name":"工作流","slug":"工作流","permalink":"http://yoursite.com/tags/工作流/"}],"keywords":[{"name":"系统设计YY","slug":"系统设计YY","permalink":"http://yoursite.com/categories/系统设计YY/"}]},{"title":"Scala类的学习","slug":"020programming/scala/scala中类的学习","date":"2015-12-07T00:00:00.000Z","updated":"2019-02-18T01:46:41.991Z","comments":true,"path":"category/020programming/scala/scala中类的学习.html","link":"","permalink":"http://yoursite.com/category/020programming/scala/scala中类的学习.html","excerpt":"","text":"类的学习单例对象 object Timer { var count = 0 def currentCount(): Long = { count += 1 count } } 执行结果 scala&gt; Timer.currentCount() res0: Long = 1 伴生对象单例对象可以和类具有相同的名称，此时该对象也被称为“伴生对象”。通常将伴生对象作为工厂使用 class Bar(foo: String) object Bar { def apply(foo: String) = new Bar(foo) } 使用伴生对象的apply方法是Scala中构建对象的常用手法。例如，Array(1, 4, 9,16)返回一个数组，用的就是Array伴生对象的apply方法 apply是什么？ 当类或对象有一个主要用途的时候，可以使用apply object FooMaker { def apply() = println(&quot;hello world&quot;) def main(arg: Array[String]) { val newFoo = FooMaker(); } } #Trait 概念上类似Java的interface, 但是更强大 trait Philosophical { def philosophize() { println(“I consume memory, therefore I am!”) }}mix in有两种方式, extend和with extendclass Frog extends Philosophical { //mix in Philosophical trait override def toString = &quot;green&quot; } withwith, 需要显式的指明超类 class Animal trait HasLegs class Frog extends Animal with Philosophical with HasLegs { override def toString = &quot;green&quot; } Trait真正的作用在于, 模块封装, 可以把相对独立的功能封装在trait中, 并在需要的时候进行mix in, 其他参考网站 Hadoop学习笔记—10.Shuffle过程那点事儿","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/tags/Scala/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"Scala中的类型系统","slug":"020programming/scala/Scala中的类型系统","date":"2015-05-07T00:00:00.000Z","updated":"2019-02-18T01:46:51.687Z","comments":true,"path":"category/020programming/scala/Scala中的类型系统.html","link":"","permalink":"http://yoursite.com/category/020programming/scala/Scala中的类型系统.html","excerpt":"","text":"Scala中的类型系统学习Scala中定义类型的两种方式在Scala当中可以用以下两种方式定义类型: 定义类、特质或对象 直接用type关键字定义类型 在Scala当中，标注类型的时候可以直接用类和特质的名字来引用其类型，要引用对象的类型，需要用对象的type成员来引用其类型。 Scala中的路径依赖Scala中的自身类型以及自身类型的限定Scala中的依赖注入Scala中的链式风格调用—-Type机制的用法Scala中的复合数据类型Scala中的抽象类型Scala中的类型参数Scala中的类型约束Scala中的型变、逆变、协变Scala中的结构类型参考网站http://blog.csdn.net/u013063153/article/details/53066333","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/tags/Scala/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"Scala基本语法","slug":"020programming/scala/scala基本语法","date":"2015-02-07T00:00:00.000Z","updated":"2019-02-18T01:46:19.372Z","comments":true,"path":"category/020programming/scala/scala基本语法.html","link":"","permalink":"http://yoursite.com/category/020programming/scala/scala基本语法.html","excerpt":"","text":"Scala的命令行工具编写源码upper1.sc的文件内容 class Upper { def upper(strings: String*): Seq[String] = { strings.map((s:String) =&gt; s.toUpperCase()) } } val up = new Upper println(up.upper(&quot;Hello&quot;, &quot;World!&quot;)) 编译为 JVM 的字节码将脚本文件编译为 JVM 的字节码（一组 .class 文件）Scala 编译器叫作 scalac 执行scalac -Xscript Upper1 src/main/scala/progscala2/introscala/upper1.sc会发现当前文件夹下产生很多class文件 Scala生成的代码，文件夹目录与包结构吻合。 执行脚本scala Upper1ArrayBuffer(HELLO, WORLD!) scala src/main/scala/progscala2/introscala/upper1.sc 假如在命令行输入 scala 命令时不指定文件参数，REPL 将启动 假如输入 scala 命令时指定 Scala 源文件，scala 命令将会以脚本的形式编译并运行文件 sbt 中执行console任务也能进入 Scala REPL 环境） 如果有参数scala -cp . progscala2.introscala.Upper Hello World! 逆向工程java逆向工程javap -cp . Upper1 scala逆向工程scalap -cp . Upper1 object Upper1 extends scala.AnyRef { def this() = { /* compiled code */ } def main(args: scala.Array[scala.Predef.String]): scala.Unit = { /* compiled code */ } } 基本语法解释再次回到代码 class Upper { def upper(strings: String*): Seq[String] = { strings.map((s:String) =&gt; s.toUpperCase()) } } val up = new Upper println(up.upper(&quot;Hello&quot;, &quot;World!&quot;)) 使用def创建函数,def后面依次为，输入方法名称以及可选的参数列表。再输入可选的返回类型。有时候，Scala 能够推导出返回类型。 返回类型由冒号加类型表示。最后使用等于号（=）将方法签名和方法体分隔开。String 类型后面的 * ，代表了变长的 String 类型参数列表。 Scala 使用方括号（[…]）表示参数类型，而 Java 使用角括号（&lt;…&gt;）。 val关键字用于声明不变变量book。可变数据是错误之源，推荐使用不变值。 使用println方法时，无需调用scala.Console.println方法。 println 方法只是众多被自动加载的方法和类型中的一员，有一个叫作 Predef 的库对象对这些自动加载的方法和类型进行定义。 开发一个main函数 object Upper { def main(args: Array[String]) = { args.map(_.toUpperCase()).foreach(printf(&quot;%s &quot;,_)) println(&quot;&quot;) } } 在 Scala 中，main 方法必须为对象方法。（在 Java 中，main 方法必须是类静态方法。） 两处使用了 _，这两个 _ 分别位于不同的作用域中，彼此之间没有任何关联 为什么使用等号呢？而不像 Java 那样，使用花括号表示方法体呢？使用等号也强调了函数式编程的一个准则：值和函数是高度对齐的概念。假如方法体仅包含一个表达式，那么 Scala 允许你省略花括号。所以说，使用等号能够避免可能的解析歧义。 函数字面量map 方法的输入参数为函数字面量（function literal）。strings.map((s:String) =&gt; s.toUpperCase())而这些函数字面量便是“匿名”函数。在其他语言中，它们也被称为 Lambda、闭包（closure）、块（block）或过程（proc）函数有很多表达式，可以使用{}来格式化代码,匿名函数也可以， { i: Int =&gt; println(&quot;hello world&quot;) i * 2 } 此函数字面量的参数表中只包含了一个字符串参数 s。它的函数体位于箭头=&gt;之后。该函数体调用了 s 的 UpperCase() 方法。此次调用的返回值会自动被这个函数字面量返回。 在 Scala 中，函数或方法中把最后一条表达式的返回值作为自己的返回值。尽管 Scala 中存在 return 关键字，但只能在方法中使用，上面这样的匿名函数则不允许使用。事实上，方法中也很少用到这个关键字。 函数的简写(s:String) =&gt; s.toUpperCase()等价如下函数_.toUpperCase() 使用占位符 _ 来替代命名参数。也就是说：_ 起到了匿名参数的作用，在调用 toUpperCase 方法之前，_ 将被字符串替换。Scala 同时也为我们推断出了该变量的类型为 String 类型。 操作符a + b 是如下方法调用的简写： a.+(b)Scala 并不支持三元表达式,因为有一元操作。 遍历while (n &gt; 0) { rr = r * n n -= 1 } for (i &lt;- 1 to 3; j &lt;- 1 to 3 if i != j) print ((10 * i + j) + “ “) &lt;-右边的表达式的所有值,至于这个遍历具体如何执行，则取决于表达式的类型在Scala中，每个表达式都有一个类型当 for 推导式仅包含单一表达式时使用原 括号,当其包含多个表达式时使用大括号 Scala 的 for 推导式并不提供 break 和 continue 功能。Scala 提供的其他特性 使得这两个功能没有存在的必要，那么如果需要break时我们该怎么做呢？有如下几个选项： 使用Boolean型的控制变量。 使用嵌套函数——你可以从函数当中return。 使用Breaks对象中的break方法： 基本集合数组基本定义10个整数的数组，所有元素初始化为0val nums = new ArrayInt greetStrings为val, 但是内部的数组值是可变的val greetStrings = new ArrayString 长度为2的Array[String]——类型是推断出来的val s = Array(“Hello”, “World”) 循环val b = new Array[String](10) for (i &lt;- 0 until b.length) println(i + &quot;: &quot; + b(i)) for (i &lt;- (0 until b.length).reverse) println(i + &quot;: &quot; + b(i)) for (elem &lt;- b) println(elem)//打印值 for (elem &lt;- b if elem % 2 == 0) yield 2 +elem //改变数据 for使用了if 使用()而不是[]来访问元素，［］和java&lt;&gt;的类型相同，泛型类型。对Array类的操作方法列在ArrayOps相关条目下 Listval numbers = List(1, 2, 3, 4) 对于List最常用的操作符为::, 把新的elem放到list最前端 scala&gt; val numbers = List(1, 2, 3, 4) numbers: List[Int] = List(1, 2, 3, 4) scala&gt; var numbers2 = 7::numbers numbers2: List[Int] = List(7, 1, 2, 3, 4) :::, 两个list的合并 val oneTwo = List(1, 2) val threeFour = List(2,3, 4) val oneTwoThreeFour = oneTwo ::: threeFour scala&gt; val oneTwoThreeFour = oneTwo ::: threeFour oneTwoThreeFour: List[Int] = List(1, 2, 2, 3, 4) Setvar myset = Set(1, 1, 2) 元组 Tupleval hostPort = (“localhost”, 80) 元组不能通过名称获取字段，而是使用位置下标来读取对象；而且这个下标基于1，而不是基于0hostPort._1 与模式匹配 hostPort match { case (&quot;localhost&quot;, port) =&gt; ... case (host, port) =&gt; ... } 创建两个元素的元组时，可以使用特殊语法：-&gt; scala&gt; 1 -&gt; 2 res1: (Int, Int) = (1,2) Mapval treasureMap = MapInt, String val myMap = Map(“foo” -&gt; “bar”)myMap(“foo”) val numbers = Map(1 -&gt; “one”, 2 -&gt; “two”)numbers.get(2) 函数组合子（Functional Combinators）被用在标准的数据结构上 mapmap对列表中的每个元素应用一个函数，返回应用后的元素所组成的列表。foreachfilterzippartitionfind参考https://code.csdn.net/DOC_Scala/scala_class_twitter/file/%E9%9B%86%E5%90%88.md#anchor_0 其他如果想使用数值类型，记得看看RichInt、RichDouble等。同理，如果想使用字符串，记得看看SpringOps。 那些数学函数位于scala.math包中，而不是位于某个类中 参考网站 官方api csdn学习 scala学习指南","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/tags/Scala/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"Scala","slug":"020programming/scala/scala学习笔记","date":"2015-02-07T00:00:00.000Z","updated":"2019-02-18T01:46:10.801Z","comments":true,"path":"category/020programming/scala/scala学习笔记.html","link":"","permalink":"http://yoursite.com/category/020programming/scala/scala学习笔记.html","excerpt":"","text":"Scala的设计哲学Scala试图将以下三组对立的思想融合到一种语言中。 函数式编程和面向对象编程。 富有表达力的语法和静态类型。 高级的语言特性同时保持与Java 的高度集成。 Scala 完全支持面向对象编程（OOP）。Scala 引入特征（trait）改进了 Java 的对象模型。trait 能通过使用混合结构（mixin composition）简洁地实现新的类型。在 Scala 中，一切都是对象，即使是数值类型。 ## 接口 ##租借模式（loaner pattern） Scala 使用 trait 来替代接口 协变 和 逆变如果某类型可以强制转换为子孙类，我们称为协变（+T 或? extends T），如果某类型可以强制转换为祖先类，我们称为逆变（-T 或? super T）。如果某类型完全不能被强制转换，就称为不变（Invariance） 静态类型和表达力Scala做了以下几个简单的设计决策，以提高代码表达力。 把类型标注（type annotation）换到变量右边。 类型推断。 可扩展的语法。 用户自定义的隐式转换 scala的简化规则 无参数的方法可以用作后缀操作符（postfix operator） 只有一个参数的方法可以当作中缀操作符（infix operator）。 还有一些对特殊字符的专门规定，比如方法名的最后一个字符如果是“:”，则方法的调用方向反转 在定义匿名函数时（又称 lambda），Scala 提供了占位符语法。可以使用“_”关键字作为函数参数的占位符。如果使用多个占位符，每个相应位置的占位符对应于相应位置的参数。 其他Scala会自动地加载scala.Predef对象，使它的成员方法对所有程序可用。 implicit关键字在Scala里有两种不同用法。第一种用法是给方法声明一种特殊参数如果编译器在作用域里找到了合适的值就会自动传递给方法。 implicit 关键字的另一种用法是把一种类型转换为另一种。有两种场景会发生隐式转换，第一种场景是当你给一个函数传递参数的时候，如果Scala发现函数需要的参数类型（跟传给它的）不一样，Scala 会首先检查类型继承关系，如果没找到，就会去查找有没有合适的隐式转换方法。 隐式转换方法只是普通的方法，用implicit 关键字做了标注。 第二种场景是当调用某类型的某方法时，如果编译器发现该类型没有这个方法，Scala 会对该查找适用于该类型的隐式转换，直到找到一个转换后具有该方法的结果。 Scala 鼓励使用作为对象的函数（function as object），或称一等函数 领域特定语言时（DSL） 用可变对象的代码一般倾向于用命令式（imperative）的风格编码 参考网站 Hadoop学习笔记—10.Shuffle过程那点事儿","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/tags/Scala/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"Scala的高级特性","slug":"020programming/scala/scala中高级特性","date":"2015-02-07T00:00:00.000Z","updated":"2019-02-18T01:46:26.041Z","comments":true,"path":"category/020programming/scala/scala中高级特性.html","link":"","permalink":"http://yoursite.com/category/020programming/scala/scala中高级特性.html","excerpt":"","text":"惰性求值for 推导式（for comprehension）隐式类(Implicit Classes)=&gt; 隐式转换使用隐式能够减少代码，能够向已有类型中注入新的方法，也能够创建领域特定语言（DSL）。implicit 可分为：隐式参数隐式转换类型隐式调用函数 隐式参数当我们在定义方法时，可以把最后一个参数列表标记为implicit，表示该组参数是隐式参数。一个方法只会有一个隐式参数列表，置于方法的最后一个参数列表。如果方法有多个隐式参数，只需一个implicit修饰即可。 当调用包含隐式参数的方法是，如果当前上下文中有合适的隐式值，则编译器会自动为改组参数填充合适的值。如果没有编译器会抛出异常。 def calcTax(amount: Float)(implicit rate: Float): Float = amount * rateimplicit val currentTaxRate = 0.08Fval tax = calcTax(50000F) // 4000.0 隐式地转换类型使用隐含转换将变量转换成预期的类型是编译器最先使用 implicit 的地方。这个规则非常简单，当编译器看到类型X而却需要类型Y，它就在当前作用域查找是否定义了从类型X到类型Y的隐式定义例子： scala&gt; val i: Int = 3.5 //直接报错加上这句：scala&gt; implicit def double2Int(d: Double) = d.toInt再运行，没报错scala&gt; val i: Int = 3.5 //i=3 Scala支持这种隐式转换,定义转换函数后, 编译器就会自动调用上面的转换函数, 将Double转化为Int 隐式调用函数式调用函数可以转换调用方法的对象，比如但编译器看到X .method，而类型 X 没有定义 method（包括基类)方法，那么编译器就查找作用域内定义的从 X 到其它对象的类型转换，比如 Y，而类型Y定义了 method 方法，编译器就首先使用隐含类型转换把 X 转换成 Y，然后调用 Y 的 method。 class SwingType{ def wantLearned(sw : String) = println(&quot;兔子已经学会了&quot;+sw) } object swimming{ implicit def learningType(s : AminalType) = new SwingType } class AminalType object AminalType extends App{ val rabbit = new AminalType rabbit.wantLearned(&quot;breaststroke&quot;) //蛙泳 } 编译器在rabbit对象调用时发现对象上并没有wantLearning方法，此时编译器就会在作用域范围内查找能使其编译通过的隐式视图，找到learningType方法后，编译器通过隐式转换将对象转换成具有这个方法的对象，之后调用wantLearning方法 模式匹配（pattern matching）scala语言里的模式匹配类似java语言中switch语句 使用matchval times = 1 times match { case 1 =&gt; &quot;one&quot; case 2 =&gt; &quot;two&quot; case _ =&gt; &quot;some other number&quot; } 一个复杂的例子 for { x &lt;- Seq(1, 2, 2.7, &quot;one&quot;, &quot;two&quot;, &apos;four) // &lt;1&gt; } { val str = x match { // &lt;2&gt; case 1 =&gt; &quot;int 1&quot; // &lt;3&gt; case i: Int =&gt; &quot;other int: &quot;+i // &lt;4&gt; case d: Double =&gt; &quot;a double: &quot;+x // &lt;5&gt; case &quot;one&quot; =&gt; &quot;string one&quot; // &lt;6&gt; case s: String =&gt; &quot;other string: &quot;+s // &lt;7&gt; case unexpected =&gt; &quot;unexpected value: &quot; + unexpected // &lt;8&gt; } println(str) // &lt;9&gt; } 匹配其他任意输入，x 的值被赋给 unexpected 这个变量。由于未给出任何类型说明，unexpected 的类型被推断为 Any，起到了 default 语句的功能。 for { x &lt;- Seq(1, 2, 2.7, &quot;one&quot;, &quot;two&quot;, &apos;four) } { val str = x match { case _: Int | _: Double =&gt; &quot;a number: &quot;+x case &quot;one&quot; =&gt; &quot;string one&quot; case _: String =&gt; &quot;other string: &quot;+x case _ =&gt; &quot;unexpected value: &quot; + x } println(str) } 在try-catch-finally语法中通过模式匹配 try { remoteCalculatorService.add(1, 2) } catch { case e: ServerIsDownException =&gt; log.error(e, &quot;error.&quot;) } finally { remoteCalculatorService.close() } Case Class首先case class是用于让你更加简洁的使用pattern matching, 如果你想对一个class进行pattern matching, 最好在前面加上case。 case 关键字的另一个特征便是让编译器自动为我们生成许多方法，其中包括了类似于 Java 语言中 String、equals 和 hashCode 方法。默认是可以序列化的，也就是实现了Serializable。 编译器同时会生成一个伴生对象（companion object），伴生对象是一个与 case 类同名的单例对象 伴生对象中已经自动添加了不少方法，apply 方法便是其中之一。该方法接受的参数列表与构造函数接受的参数列表一致。 因为有伴生对象，所以初始化的时候可以不用new。case class User(name:String,age:Int)val user = User(&quot;ww&quot;,30) case class构造函数的参数是public级别的，我们可以直接访问user.name 参考例子 case class Person(name : String,age : Int) object ConstructorPattern{ def main(args:Array[String]) :Unit = { val p = new Person(&quot;nyz&quot;,27) def constructorPattern(p : Person) = p match { //构造器模式必须将Person类定义为case class,否则需要自己定义伴生对象并实现unapply方法。 case Person(name,age) =&gt; &quot;name =&quot; + name + &quot;,age =&quot; + age //case Person(_,age) =&gt; &quot;age =&quot; + age case _ =&gt; &quot;Other&quot; } println(constructorPattern(p)) } } http://www.cnblogs.com/zlslch/p/6115392.html 偏函数Scala中的Partial Function就是一个“残缺”的函数，就像一个严重偏科的学生，只对某些科目感兴趣，而对没有兴趣的内容弃若蔽履。Partial Function做不到以“偏”概全，因而需要将多个偏函数组合，最终才能达到全面覆盖的目的。所以这个Partial Function确实是一个“部分”的函数。 型变假设一个方法带有的参数类型为 List[AnyRef]，你可以传入 List[String] 吗？换句话说，List[String] 是否应该被看作是 List[AnyRef] 的一个子类型呢？如果是，这种转化称为协变（covariance）。 型变在scala中灵活的分为了不变，逆变和协变。如果某类型可以强制转换为子孙类，我们称为协变（+T 或? extends T），如果某类型可以强制转换为祖先类，我们称为逆变（-T 或? super T）。如果某类型完全不能被强制转换，就称为不变（Invariance）。 “+”表示协变，而“-”表示逆变C[+T]：如果A是B的子类，那么C[A]是C[B]的子类（协变）。C[-T]：如果A是B的子类，那么C[B]是C[A]的子类（逆变）。C[T]：无论A和B是什么关系，C[A]和C[B]没有从属关系。 type 成员的工作机制与参数化类型中的类型参数非常类似 https://my.oschina.net/xinxingegeya/blog/486671http://hongjiang.info/scala-covariance-and-contravariance/http://lib.csdn.net/article/scala/25849 正则表达式三重双引号来表示正则表达式字符串 参考网站 文档 http://hongjiang.info/message/http://zhuanlan.51cto.com/art/201703/534053.htm","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/tags/Scala/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"Ubuntu配置R开发环境","slug":"020programming/Ubuntu配置R开发环境","date":"2014-06-27T00:00:00.000Z","updated":"2018-12-10T15:45:40.696Z","comments":true,"path":"category/020programming/Ubuntu配置R开发环境.html","link":"","permalink":"http://yoursite.com/category/020programming/Ubuntu配置R开发环境.html","excerpt":"","text":"说明每个环境安装R估计都不一样，uname -a，显示的是3.13.0-24-generic使用cat /etc/issue，我的机器显示的为Ubuntu 14.04 LTS。下面的步骤展示安装RStudio Server的步骤 安装步骤wget http://cran.r-project.org/src/base/R-3/R-3.1.0.tar.gz tar -zxvf R-3.1.0.tar.gz cd R-3.1.0 报错：configure: error: No F77 compiler found这是因为Ubuntu现在默认没有fortran编译器，需要手工装一个，比如gfortran：sudo apt-get install gfortran 为了防止出现其他错误sudo apt-get install readline-common libxt-dev 执行./configure –enable-R-shlib –with-readline=no –with-x=no #安装 rStdiowget http://download2.rstudio.org/rstudio-server-pro-0.98.945-i386.debsudo gdebi rstudio-server-0.97.551-amd64.deb 如果出现没有安装python-dev我的因为以前安装过python,直接安装出现python-dev : 依赖: python (= 2.7.4-0ubuntu1) 但是 2.7.5-5ubuntu3 正要被安装用aptitude 工具进行降级处理。 执行 sudo aptitude install python-dev第一次选no，第二次yes。 然后：sudo aptitude install python-dev 再次：sudo gdebi rstudio-server-0.97.551-amd64.deb 出现rstudio-server start/running, process 13342 执行ps -aux|grep rstudio-server，发现已经启动。 访问http://127.0.0.1:8787/，使用你的linux的用户名/密码进行登陆。可以成功的看到如下界面 成功，尽情的享受吧！ 参考文章 http://timex1441.blog.163.com/blog/static/4726719720125410303262/http://www.ningoo.net/page/2 http://jianshu.io/p/2c739a25d20b http://bbs.csdn.net/topics/390462321 后来又发现了一片文章多人在线协作R开发RStudio Server http://blog.fens.me/r-rstudio-server/","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"R","slug":"R","permalink":"http://yoursite.com/tags/R/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"中国圣贤的使命","slug":"059杂七杂八/使命 ","date":"2014-06-21T00:00:00.000Z","updated":"2018-12-10T17:00:19.005Z","comments":true,"path":"category/059杂七杂八/使命 .html","link":"","permalink":"http://yoursite.com/category/059杂七杂八/使命 .html","excerpt":"","text":"什么是中国圣贤的使命？ 北宋大儒张横渠有言, 为天地立心；为生民立命。为往圣继绝学；为万世开太平。这句话道出了中国儒者的心声。 《左传·襄公二十四年》中写道：“太上有立德，其次有立功，其次有立言；虽久不废，此之谓不朽。”立德、立功、立言，这“三不朽”是儒家最高的人生理想。 立德：树立美好的品德，树立道德的典范，言为师，行为表。学高为师，德高为范。立功：做与国家与社会有价值的事情，并创造价值 。立言：不仅自己成功了，不仅自己取得想要的结果，还要把自己思想，观念，智慧，流传给后人。 为天地立心；为生民立命。为往圣继绝学；为万世开太平 。立德，立功，立言，这些都是中国圣贤的使命，在我们当今现代，比起“改变世界”的使命，也有过之无不及。 为什么要强调使命？ 我们看看德鲁克管理的定义：“管理就是界定企业的使命，并激励和组织人力资源去实现这个使命。界定使命是企业家的任务，而激励与组织人力资源是领导力的范畴，二者的结合就是管理。”我们看看里面包含的关键词“界定使命”, “激励”,“组织”，“实现使命”。 一直以来有个固定思维，说西方的管理，强调目标管理，目标=成功。而中国管理的则强调使命，但我们看看上面的管理定义，可以看出西方现代管理其实和中国古代哲学一样，也把使命放在了第一位。 中国圣贤使命的思考 中国古代先哲给我们留下了特别的宝贵精神遗产。但反观如今我们的价值观，却有点不知何从的地步。我们对很多东西持怀疑的态度。我们不信佛，我们不信儒家思想，谁要是像个书生，我们会投以鄙夷。我们也不信三民主义，我们也对马克思主义产生了怀疑。更不用说一些所谓的“专家”了。 但我们从古代先哲的使命观中，也可以找寻到一些治病的方法。“为往圣继绝学”，优秀的东西是不会过时的，智慧没有错，错的是用的人。同样英雄也不应问出处。“往圣”可以包含古今中外，道法儒佛，对其思想有批判的继承和发扬。我们同样也可轻易的判断那些“意见领袖”们，你们的言论符合“为天地立心；为生民立命。为往圣继绝学；为万世开太平”。否则，why信奉你们。 那么，你的使命是什么呢？","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}],"tags":[{"name":"德鲁克","slug":"德鲁克","permalink":"http://yoursite.com/tags/德鲁克/"}],"keywords":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}]},{"title":"CSS3的@font-face和字体介绍","slug":"020programming/html/CSS3的@font-face和字体介绍","date":"2014-02-06T00:00:00.000Z","updated":"2018-12-10T15:09:52.738Z","comments":true,"path":"category/020programming/html/CSS3的@font-face和字体介绍.html","link":"","permalink":"http://yoursite.com/category/020programming/html/CSS3的@font-face和字体介绍.html","excerpt":"本文将介绍@font-face用法，以及用字体来代替图标的具体方法。 @font-face学习@font-face是CSS3中的一个模块，他主要是把自己定义的Web字体嵌入到你的网页中,@font-face的出现允许用户从网上下载字体并直接在电脑上自动完成安装，因此开发者不用担心某个字体在用户的电脑中是否存在的问题。 @font-face的语法规则@font-face { font-family: &lt;YourWebFontName&gt;; src: &lt;source&gt; [&lt;format&gt;][,&lt;source&gt; [&lt;format&gt;]]*; [font-weight: &lt;weight&gt;]; [font-style: &lt;style&gt;]; } YourWebFontName:此值指的就是你自定义的字体名称，最好是使用你下载的默认字体，他将被引用到你的Web元素中的font-family。如font-family:&quot;YourWebFontName&quot;; source:此值指的是你自定义的字体的存放路径，可以是相对路径也可以是绝路径； format：此值指的是你自定义的字体的格式，主要用来帮助浏览器识别，其值主要有以下几种类型：truetype,opentype,truetype-aat,embedded-opentype,avg等； weight和style:这两个值大家一定很熟悉,weight定义字体是否为粗体，style主要定义字体样式，如斜体。","text":"本文将介绍@font-face用法，以及用字体来代替图标的具体方法。 @font-face学习@font-face是CSS3中的一个模块，他主要是把自己定义的Web字体嵌入到你的网页中,@font-face的出现允许用户从网上下载字体并直接在电脑上自动完成安装，因此开发者不用担心某个字体在用户的电脑中是否存在的问题。 @font-face的语法规则@font-face { font-family: &lt;YourWebFontName&gt;; src: &lt;source&gt; [&lt;format&gt;][,&lt;source&gt; [&lt;format&gt;]]*; [font-weight: &lt;weight&gt;]; [font-style: &lt;style&gt;]; } YourWebFontName:此值指的就是你自定义的字体名称，最好是使用你下载的默认字体，他将被引用到你的Web元素中的font-family。如font-family:&quot;YourWebFontName&quot;; source:此值指的是你自定义的字体的存放路径，可以是相对路径也可以是绝路径； format：此值指的是你自定义的字体的格式，主要用来帮助浏览器识别，其值主要有以下几种类型：truetype,opentype,truetype-aat,embedded-opentype,avg等； weight和style:这两个值大家一定很熟悉,weight定义字体是否为粗体，style主要定义字体样式，如斜体。 一个具体的使用例子html使用字体部分 &lt;h2 class=&quot;neuesDemo&quot;&gt;Neues Bauen Demo&lt;/h2&gt; 样式定义如下: h2.neuesDemo { font-family: &apos;NeuesBauenDemo&apos; } NeuesBauenDemo是哪里来的? 答案通过@font-face来定义自己的Web Font: 12345678910 @font-face &#123; font-family: &apos;NeuesBauenDemo&apos;; src: url(&apos;../fonts/neues_bauen_demo-webfont.eot&apos;); src: url(&apos;../fonts/neues_bauen_demo-webfont.eot?#iefix&apos;) format(&apos;embedded-opentype&apos;), url(&apos;../fonts/neues_bauen_demo-webfont.woff&apos;) format(&apos;woff&apos;), url(&apos;../fonts/neues_bauen_demo-webfont.ttf&apos;) format(&apos;truetype&apos;), url(&apos;../fonts/neues_bauen_demo-webfont.svg#NeuesBauenDemo&apos;) format(&apos;svg&apos;); font-weight: normal; font-style: normal;&#125; 为什么要用多个url? 主要是为了解决浏览器的兼容问题。后面在讨论，先了解一下woff,ttf,svg这些字体的格式。 字体的格式上面说了format：指的是你自定义的字体的格式，其值主要有以下几种类型：truetype,opentype,truetype-aat,embedded-opentype,avg等；现在简单的分别介绍。 TureTpe(.ttf)格式ttf字体是Windows和Mac的最常见的字体，是一种RAW格式，因此他不为网站优化,支持这种字体的浏览器有【IE9+,Firefox3.5+,Chrome4+,Safari3+,Opera10+,iOS Mobile Safari4.2+】； OpenType(.otf)格式.otf字体被认为是一种原始的字体格式，其内置在TureType的基础上，所以也提供了更多的功能,支持这种字体的浏览器有【Firefox3.5+,Chrome4.0+,Safari3.1+,Opera10.0+,iOS Mobile Safari4.2+】； Web Open Font Format(.woff)格式.woff字体是Web字体中最佳格式，他是一个开放的TrueType/OpenType的压缩版本，同时也支持元数据包的分离,支持这种字体的浏览器有【IE9+,Firefox3.5+,Chrome6+,Safari3.6+,Opera11.1+】； Embedded Open Type(.eot)格式.eot字体是IE专用字体，可以从TrueType创建此格式字体,支持这种字体的浏览器有【IE4+】； SVG(.svg)格式.svg字体是基于SVG字体渲染的一种格式,支持这种字体的浏览器有【Chrome4+,Safari3.1+,Opera10.0+,iOS Mobile Safari3.2+】。 多浏览器支持浏览器的支持情况： 从上面浏览器对字体支持情况可以得出,为了达到更多浏览版本的支持,至少需要.woff,.eot两种格式字体，甚至还需要.svg等字体达到更多种浏览版本的支持。为了使@font-face达到更多的浏览器支持，Paul Irish写了一个独特的@font-face语法叫Bulletproof @font-face: @font-face { font-family: &apos;YourWebFontName&apos;; src: url(&apos;YourWebFontName.eot?&apos;) format(&apos;eot&apos;);/*IE*/ src:url(&apos;YourWebFontName.woff&apos;) format(&apos;woff&apos;), url(&apos;YourWebFontName.ttf&apos;) format(&apos;truetype&apos;);/*非IE*/ } 为了更多浏览器支持，你也可以写成： @font-face { font-family: &apos;YourWebFontName&apos;; src: url(&apos;YourWebFontName.eot&apos;); /* IE9 Compat Modes */ src: url(&apos;YourWebFontName.eot?#iefix&apos;) format(&apos;embedded-opentype&apos;), /* IE6-IE8 */ url(&apos;YourWebFontName.woff&apos;) format(&apos;woff&apos;), /* Modern Browsers */ url(&apos;YourWebFontName.ttf&apos;) format(&apos;truetype&apos;), /* Safari, Android, iOS */ url(&apos;YourWebFontName.svg#YourWebFontName&apos;) format(&apos;svg&apos;); /* Legacy iOS */ } 这就是我们上面看到的多个url的来历。现在的问题是怎样获取字体，以及怎样生成几种不同的格式。 免费字体的获取和处理40个可通过@font-face使用的免费字体，可以挑选你的字体。例如我们到Google Web Fonts,下载我们需要的文件,例如文件名Duru_Sans.zip,解压缩会发现里面有一个DuruSans-Regular.ttf文件。现在我们需要想办法获得@font-face所需的.eot,.woff,.ttf,.svg所有字体格式。要获取这些字体格式，我们需要第三方工具或者软件来实现。可通过访问fontsquirrel来上传我们的DuruSans-Regular.ttf文件， 上载成功后，会出现一个下载按钮，可进行下载为一个zip文件,解压缩后包含我们所需的各种类型字体和用法的demo，见下图: 有没有更简单的？可以使用托管的Web字体服务。 使用托管的Web字体服务以Google Web Fonts服务为例，访问Google Web Fonts,选择一个你喜欢的字体（Add to collection），然后使用这个字体（Use），Google Web Fonts服务会自动生成一段类似&lt;link href=&#39;http://fonts.googleapis.com/css?family=Artifika&#39; rel=&#39;stylesheet&#39; type=&#39;text/css&#39;&gt;的代码,通过访问http://fonts.googleapis.com/css?family=Artifika用不同的浏览器的访问结果是不一样的，例如使用google浏览器， @font-face { font-family: &apos;Artifika&apos;; font-style: normal; font-weight: 400; src: local(&apos;Artifika Medium&apos;), local(&apos;Artifika-Medium&apos;), url(http://themes.googleusercontent.com/static/fonts/artifika/v4/r0NXNnpds-Akyno8nTG0zfesZW2xOQ-xsNqO47m55DA.woff) format(&apos;woff&apos;); } 使用Safari浏览器版本4,结果如下： @font-face { font-family: &apos;Artifika&apos;; font-style: normal; font-weight: 400; src: local(&apos;Artifika Medium&apos;), local(&apos;Artifika-Medium&apos;), url(http://themes.googleusercontent.com/static/fonts/artifika/v4/z0sjSaXjDxxgrMTOLI1lSfesZW2xOQ-xsNqO47m55DA.ttf) format(&apos;truetype&apos;); } 可见谷歌字体API具有跨浏览特性,具体的用法可访问谷歌字体API使用教程。 使用字体来代替图标使用字体来实现图标,尺寸和颜色可以用css来控制,而且字体文件很小,对性能优化有好处。先介绍一个图标字体Font Awesome,中文介绍可以访问http://www.bootcss.com/p/font-awesome/。Font Awesome是一套专为Twitter Bootstrap设计的图标字体，几乎囊括了网页中可能用到的所有图标，这些图标通过Web Font的方式来显示，可以被任意缩放、改变颜色，你可以像修改文字样式那样来修改图标样式。源码可访问https://github.com/FortAwesome/Font-Awesome/假设我要显示一个红色的铃铛 实现代码如下： &lt;!doctype html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;./font-awesome/css/font-awesome.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; &lt;a href=&quot;#&quot;&gt; &lt;i class=&quot;fa fa-bell&quot; style=&quot;color:red&quot; &gt;&lt;/i&gt; &lt;/a&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 假设font-awesome.css已经存在，只用了fa和fa-bell样式就已经搞定，看看他们的定义: .fa { display: inline-block; font-family: FontAwesome; font-style: normal; font-weight: normal; line-height: 1; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } .fa-bell:before { content: &quot;\\f0f3&quot;; } 这里用了一下css3的伪元素，fa用来进行占位，line-height: 1,行高定义为数字，实际高度随着font-size变化。fa-bell:beforet用于把icon覆盖到实际元素上面。\\f0f3为铃铛的Unicode。这个编码我们记不住，可以借助字体编辑软件FontLab，使用其打开fontawesome-webfont.ttf文件，进行搜索(ctrl+f5)，操作结果如下图： FontLab可以设计自己的字体文件，具体使用可参见CSS3 用字体来代替图标的显示完全指南。 参考文献 前端性能优化进阶之CSS无图片技术 CSS3 @font-face CSS3 用字体来代替图标的显示完全指南 Web开放字体格式 Web版式和@font-face简介 谷歌字体API使用教程 40个可通过@font-face使用的免费字体 深入了解css的行高Line Height属性","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"HTML&CSS","slug":"HTML-CSS","permalink":"http://yoursite.com/tags/HTML-CSS/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"ubuntu14下安装mysql5.6","slug":"020programming/db/mysql/ubuntu14下安装mysql","date":"2013-06-26T00:00:00.000Z","updated":"2018-12-10T15:17:19.628Z","comments":true,"path":"category/020programming/db/mysql/ubuntu14下安装mysql.html","link":"","permalink":"http://yoursite.com/category/020programming/db/mysql/ubuntu14下安装mysql.html","excerpt":"","text":"安装mysql 安装mysql依赖包 安装依赖包libaio1.so sudo apt-get install libaio1 其他依赖，若已安装可省略 sudo apt-get install cmake libncurses5-dev bison g++ 下载并安装 在官方网站下载mysql-apt-config_0.2.1-1ubuntu14.04_all.deb 执行 sudo dpkg -i mysql-apt-config_0.2.1-1ubuntu14.04_all.deb 在出现的界面选择MySQL 5.6，接着执行下面命令 sudo apt-get update 安装mysql-server sudo apt-get install mysql-server 检查是否安装成功 mysql -uroot -p 输入前面安装时设置的root密码，如果进入成功则安装完成 #mysql管理# 目录的创建 sudo mkdir -p /usr/local/mysql/data 组及用户创建 sudo groupadd mysql #添加组 sudo useradd -g mysql mysql -s /bin/false #创建用户mysql并加入到mysql组，不允许mysql用户直接登录系统 启动/停止 Mysql sudo /etc/init.d/mysql startsudo /etc/init.d/mysql stop 常用命令load data infile ‘/home/ww/mydata/big-test/weblog_entries.txt’ into table weblogs fields terminated by ‘\\t’ terminated by ‘\\r\\n’; ALTER TABLE weblogs MODIFY COLUMN md5 VARCHAR(50); 参考http://blog.csdn.net/linsanhua/article/details/17608269http://wkm.iteye.com/blog/2018192http://www.cnblogs.com/wuhou/archive/2008/09/28/1301071.html /usr/my.cnf cd /usr ; /usr/bin/mysqld_safe &amp;Windows操作系统最后的换行符是\\r\\n，unix是\\n","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"从古龙武侠看管理","slug":"059杂七杂八/武侠看管理","date":"2013-06-11T00:00:00.000Z","updated":"2018-12-10T17:00:35.009Z","comments":true,"path":"category/059杂七杂八/武侠看管理.html","link":"","permalink":"http://yoursite.com/category/059杂七杂八/武侠看管理.html","excerpt":"","text":"引言发现小时候看古龙的小说当时还真没看懂，只觉得主角怎么那么苦，不过现在在回忆一下，发现古龙竟然把一个一个的场景印在了我的脑海中，印象比较深刻的有，使用孔雀翎的场景，阿飞一步一步跟在上官惊虹的半步之间，傅红雪不停练习一个拔刀姿势， 胡铁花在酒馆里的离开寡妇的那一幕，李寻欢和上官惊虹的对峙，永远垂着头的师爷，老伯床底下的老奴。古龙其实把一些生活的哲学，融入到武侠中，今天约谈一下武侠中对管理的一些启发。 从武侠看人才好的杀手应该分成两种,一种是顶级的,一种是优秀的。什么是顶级的？这种人武功已经达到化境，这是一个什么的境界？也就是说对于他来说，已经没有招式可言,你可以想想张三丰、扫地僧、风清扬的样子。 路上的一片树叶、树枝、石头什么的，足以使对手毙命，老板们最喜欢的是这类人才，但是可遇不可求。 还有的就是优秀的人才，这种人应该不少,他们的武功也很高，且多有一颗可怀才不遇的心，交代的事情一般都能搞定。但是出任务前，老板会给他匹配相应的团队，可能要叮嘱他一下哪些危险的地方，建议他用什么招式等等,不然有可能回不来了。这种人还有个致命的缺陷，他一直以为自己是个顶级杀手。更要命的是老板也把他当成了第一种人。 从武侠看执行力上次说，武侠中化境，其实是比喻一个人的执行力很强，在管理上，其实也可以用一个武功修为来比喻，那就是”人刀合一”。我的观念是管理方式一定要和个人气质，习惯,能力要匹配。想想一个伙计，武功不高，却到处拿着”屠龙刀”,行走江湖会怎么样?但这种人很多，如果一个人说他用德鲁克管理方式或者他说用的敏捷的管理流程，你一定要怀疑他拿着的是不是”屠龙刀”。","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}],"tags":[{"name":"武侠","slug":"武侠","permalink":"http://yoursite.com/tags/武侠/"}],"keywords":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}]},{"title":"Ubuntu 14.04 LTS 安装Oracle 11","slug":"020programming/db/oracle/linux下的安装","date":"2013-05-01T00:00:00.000Z","updated":"2018-12-10T15:16:53.461Z","comments":true,"path":"category/020programming/db/oracle/linux下的安装.html","link":"","permalink":"http://yoursite.com/category/020programming/db/oracle/linux下的安装.html","excerpt":"","text":"安装必要的包以下安装针对我的平台， sudo apt-get install kshsudo apt-get install libstdc++5sudo apt-get install zlibc 出错sudo apt-get install elfutilssudo apt-get install expat 创建用户和组sudo groupadd oinstallsudo groupadd dbasudo useradd -g oinstall -G dba -d /opt/oracle -s /bin/bash oraclesudo passwd oracle //设置密码也为oracle #修改内核参数#切换到root用户，修改 /etc/sysctl.conf 文件，加上如下参数 fs.aio-max-nr = 1048576fs.file-max = 6815744kernel.shmall = 2097152kernel.shmmax = 536870912kernel.shmmni = 4096kernel.sem = 250 32000 100 128net.ipv4.ip_local_port_range = 9000 65500net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048586 为使上述配置生效，执行如下命令，如果没有出现错误，就成功 /sbin/sysctl -p修改用户限制vi /etc/security/limits.conf，在文件中加入 oracle soft nproc 2047oracle hard nproc 16384oracle soft nofile 1024oracle hard nofile 65536oracle soft stack 10240 修改用户验证选项root用户下：修改/etc/pam.d/login文件加上如下参数 修改用户配置文件修改/etc/profile文件加入如下参数if [ $USER = “oracle” ]; then if [ $SHELL = “/bin/ksh” ]; then ulimit -p 16384 ulimit -n 65536 else ulimit -u 16384 -n 65536 fifi 创建需要的文件夹（用于安装oracle）并设置其所有权 mkdir -p /opt/oraclemkdir -p /opt/oraInventorychown -R oracle:oinstall /opt/oraclechown -R oracle:oinstall /opt/oraInventory 为Oracle配置环境变量su oraclecd ~vi .bash_profile 在文件中加入以下内容 export ORACLE_BASE=/opt/oracle #这个后面可以随便填写export ORACLE_HOME=$ORACLE_BASE/product/11.2.0/dbhome_1 #数据库的sidexport ORACLE_SID=orclexport ORACLE_UNQNAME=orcl #默认字符集export NLS_LANG=.AL32UTF8export PATH=${PATH}:${ORACLE_HOME}/bin/; oracle本身并不支持ubuntu来安装，所以要进行欺骗oracle的安装程序（sudo执行）：在root用户下执行： ln -s /etc /etc/rc.dln -s /lib/i386-linux-gnu/libgcc_s.so.1 /lib/ln -s /usr/bin/awk /bin/awkln -s /usr/bin/basename /bin/basenameln -s /usr/bin/rpm /bin/rpmln -s /usr/lib/i386-linux-gnu/libpthread_nonshared.a /usr/lib/libpthread_nonshared.aln -s /usr/lib/i386-linux-gnu/libc_nonshared.a /usr/lib/libc_nonshared.aln -s /usr/lib/i386-linux-gnu/libstdc++.so.6 /lib/ln -s /usr/lib/i386-linux-gnu/libstdc++.so.6 /usr/lib/ln -s /usr/lib/i386-linux-gnu/libstdc++.so.5 /lib/ln -s /usr/lib/i386-linux-gnu/libstdc++.so.5 /usr/lib/ echo ‘Red Hat Linux release 5’ &gt; /etc/redhat-release 下载oracle文件到/opt/oracle下（以前建立的home)使用oracle用户登录，然后解压缩Oracle安装文件unzip linux_11gR2_database_1of2.zip解压完成后，会在其下建立文件database。 cd databasechmod 777 runInstaller 安装oracle无法使用命令/usr/bin/xdpyinfo自动检查显示器颜色 在root下执行 #xdpyinfo 记录下name of display：后的字符串，如 127.0.0.1：1.0（也可执行xdpyinfo | grep ‘name of display’） #xhost +返回信息为： access control disabled,clients can connect from any host 3.切换到Oracle用户 #su - oracle 4.在Oracle用户下执行命令，注意：一定要和root下的一致。export DISPLAY=:1.0 5.在Oracle下执行xdpyinfo命令，如果能正常显示，说明设置成功 乱码问题关闭该界面，先在终端输入：export LANG=zh_CN.gbk然后再执行：./runInstaller oracle 11g出现”未找到文件 E:\\development_tools\\database\\oracle\\install_d\\dbhome\\owb\\external\\oc4j_applications\\applications\\WFMLRSVCApp.ear” 2of2\\database\\stage\\Components里文件复制到cp -r Components/ /opt/oracle/database/stage/Component/ 重复安装清空/opt/oraInventory 遭遇Error in invoking target ‘agent nmhs’ of makefile vi $ORACLE_HOME/sysman/lib/ins_emagent.mk Search for the line$(MK_EMAGENT_NMECTL)Change it to:$(MK_EMAGENT_NMECTL) -lnnz11 http://www.postgres.org.cn/http://bbs.pgsqldb.com 打开一个新的终端，输入如下四个命令：sed -i ‘s/^(TNSLSNR_LINKLINE.\\$(TNSLSNR_OFILES)) (\\$(LINKTTLIBS))/\\1 -Wl,–no-as-needed \\2/g’ $ORACLE_HOME/network/lib/env_network.mksed -i ‘s/^(ORACLE_LINKLINE.\\$(ORACLE_LINKER)) (\\$(PL_FLAGS))/\\1 -Wl,–no-as-needed \\2/g’ $ORACLE_HOME/rdbms/lib/env_rdbms.mksed -i ‘s/^(\\$LD \\$LD_RUNTIME) (\\$LD_OPT)/\\1 -Wl,–no-as-needed \\2/g’ $ORACLE_HOME/bin/genorasdkshsed -i ‘s/^(\\s*)(\\$(OCRLIBS_DEFAULT))/\\1 -Wl,–no-as-needed \\2/g’ $ORACLE_HOME/srvm/lib/ins_srvm.mk然后在图形界面点击‘Retry’就能继续安装了。 最后应该就顺利完成了，按照安装程序提示最后执行两个脚本:sudo /opt/oraInventory/orainstRoot.shsudo /opt/oracle/product/11.2.0/dbhome_1/root.sh 没有sudo权限 linux oracle No protocol specified 在root用户下执行echo ‘oracle ALL=(ALL) ALL’ &gt;&gt; /etc/sudoers 使用netca来配置监听 使用dbca来创建数据库 http://www.linuxidc.com/Linux/2011-12/50092.htmhttp://www.xuebuyuan.com/1598278.html http://blog.csdn.net/idber/article/details/9039857http://www.2cto.com/database/201302/189368.htmlhttp://www.th7.cn/db/Oracle/201406/58542.shtml","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://yoursite.com/tags/Oracle/"},{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"PostgreSQL 9.2在CentOS 6.3下的安装和windos下的测试","slug":"020programming/db/postgres/setup","date":"2013-05-01T00:00:00.000Z","updated":"2018-12-10T15:17:09.834Z","comments":true,"path":"category/020programming/db/postgres/setup.html","link":"","permalink":"http://yoursite.com/category/020programming/db/postgres/setup.html","excerpt":"","text":"开始在服务器端，我们常常将PostgreSQL安装在linux端,而在本机测试时，本程序猿还是喜欢安装在window端。 Linux安装步骤安装编译源码所需的工具和库yum -y install wget gcc readline-devel zlib-devel make 下载源码压缩和解压进入源码压缩包下载目录 cd /usr/src 下载 wget http://ftp.postgresql.org/pub/source/v9.2.4/postgresql-9.2.4.tar.bz2 解压 tar jxvf ./postgresql-9.2.4.tar.bz2 执行源码编译配置脚本进入解压缩源码目录 cd ./postgresql-9.2.4 依次执行以下脚本,进行源码编译安装 ./configure make make install 新增用户和用户组,初始化数据库数据文件目录新增用户组 groupadd postgres 新增用户 useradd postgres -M -g postgres 新建数据库执行文件目录 mkdir -p /usr/local/pgsql 新建数据库数据文件目录 mkdir -p /data/pgsql/data 修改目录拥有者 chown -R postgres /usr/local/pgsql/. chown -R postgres /data/pgsql/data chown -R postgres /data/pgsql/data/. 编辑PATH搜索路径 vi /etc/profile在文件的最后 PATH=/usr/local/pgsql/bin:$PATH export PATH 生效PATH搜索路径 source /etc/profile 执行数据库初始化脚本变更登录用户 su postgres 执行数据库初始化脚本 /usr/local/pgsql/bin/initdb --encoding=utf8 -D /data/pgsql/data 退出变更登录,回到root权限 exit 复制PostgreSQL执行脚本 cp /usr/src/postgresql-9.2.4/contrib/start-scripts/linux /etc/init.d/postgresql 增加执行权限 chmod +x /etc/init.d/postgresql 编辑PostgreSQL执行脚本，指定数据库文件目录 vi /etc/init.d/postgresql 修改其中的PGDATA=&quot;/data/pgsql/data&quot; 编辑配置文件，配置可访问数据库的网络地址 vi /data/pgsql/data/postgresql.conf listen_addresses = ‘*’ 启动PostgreSQL服务启动PostgreSQL service postgresql start 重启数据库服务 service postgresql restart 设置开机自动启动服务chkconfig postgresql on 编辑配置文件，设置密码md5验证vi /data/pgsql/data/pg_hba.conf 修改器内容如下 # &quot;local&quot; is for Unix domain socket connections only local all all md5 # IPv4 local connections: #host all all 127.0.0.1/32 trust host all all all md5 trust为明文，这样是危险的，一般改成md5。 用户和数据库的创建创建用户 需要先启动数据库后，才能执行以下命令 createuser -W -h 127.0.0.1 testUser 创建用户testUser,-W表示需要输入密码。 创建数据库 createdb -U testUser mydb -U表示用户，mydb为数据库名。 防火墙设置有时客户端访问不了数据库，可能是防火墙的问题，,postgresql默认的访问端口为5432,操作如下: /sbin/iptables -I INPUT -p tcp --dport 5432 -j ACCEPT /etc/rc.d/init.d/iptables save #保存配置 /etc/rc.d/init.d/iptables restart #重启服务 其他相关命令如下：service iptables stop #关掉防火墙chkconfig iptables off #永久关闭防火墙 windows安装步骤下载http://www.enterprisedb.com/products-services-training/pgbindownload下载一个免安装版本，解压缩到D:\\pgsql。 设置环境变量设置环境变量,写一个批处理,setEnv.cmd,内容如下 set PGHOME=d:\\pgsql set PATH=%PGHOME%\\bin;%path% set PGHOST=localhost set PGLIB=%PGHOME%\\lib set PGDATA=%PGHOME%\\data start 初始化数据库initdb -D &quot;D:\\pgsql\\data-new&quot; --encoding=utf8 --locale=C 若不使用-U pgsqlUser,则数据库里自动添加当前windows用户为数据库帐号pgsqlUser可以通过net user pgsqlUser 123456 /add,但有的系统可能不支持。就不要搞复杂了。 启动数据库D:\\pgsql&gt;pg_ctl start 上面命令在setEnv.cmd执行后执行 如果没有，需要指定PGDATA,进行D:\\pgsql\\bin,执行 pg_ctl -D &quot;D:\\pgsql\\data-new&quot; -l logfile start 停止数据库pg_ctl -D &quot;D:\\pgsql\\data&quot; stop 创建数据库和用户和linux类似,需要先启动数据库 createuser -s -W -h 127.0.0.1 test s表示为超级用户，-W提示输入密码 删除，执行dropuser -h 127.0.0.1 test 创建数据库 createdb -h 127.0.0.1 -U test mydb 命令行访问数据库 psql -h 127.0.0.1 -U test -d mydb 在psql中进行密码修改 ALTER USER test PASSWORD &apos;goodluck&apos;; psql的其他基本用法 显示当前日期 select current_date;计算md5 加密 select md5(‘123456’);执行sql语句 PostgreSQL数据库备份与恢复备份备份整个库 pg_dump -h 127.0.0.1 -C -U test mydb &gt; mydb.bak 备份整个库表结构 pg_dump -h 127.0.0.1 -C -s -U test mydb &gt; mydb.sql 备份单个表结构 pg_dump -h 127.0.0.1 -C -t my_calendar_event -t outsourcing_project -s -U test mydb &gt; mydb.sql 上面是备份两个表的结构，-t指定。 恢复psql -h 127.0.0.1 -U test -d mydb &lt; testdb.bak 学习网站 http://www.postgres.org.cn/http://bbs.pgsqldb.com","categories":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://yoursite.com/tags/PostgreSQL/"}],"keywords":[{"name":"Programming","slug":"Programming","permalink":"http://yoursite.com/categories/Programming/"}]},{"title":"使用hexo搭建我的博客","slug":"059杂七杂八/Hexo建站过程","date":"2013-02-06T00:00:00.000Z","updated":"2019-02-18T01:42:39.664Z","comments":true,"path":"category/059杂七杂八/Hexo建站过程.html","link":"","permalink":"http://yoursite.com/category/059杂七杂八/Hexo建站过程.html","excerpt":"hexo是由Node.js驱动的一款快速、简单且功能强大的博客框架。在网上有很多关于安装的问题, 本文简单阐述一下本网站的建设过程，以及在安装的过程中遇到的问题和一些有用的技巧。","text":"hexo是由Node.js驱动的一款快速、简单且功能强大的博客框架。在网上有很多关于安装的问题, 本文简单阐述一下本网站的建设过程，以及在安装的过程中遇到的问题和一些有用的技巧。 安装主题假设Hexo已经安装完成（Hexo 2.4.5 或以上版本）假设安装目录为klwork.github.io,记为根目录。以前我用的默认主题，也不错，很干净，后来发现一个主题跟适合我这种懒人。Pacman，比较喜欢它的界面和文档的自动目录结构。 Pacman安装 下载Pacman git clone https://github.com/A-limon/pacman.git pacman 复制pacman文件夹到根目录\\themes下 修改你的博客根目录下的config.yml配置文件中的theme属性，将其设置为pacman。 增加导航栏打开根目录\\themes\\pacman的_config.yml文件,修改menu如下： menu: 首页: / 存档: /archives 项目: /project 关于: /about 例如我们要建立一个关于的导航，只需在在根目录\\source\\about目录中新建一个index.md文件，即可成功完成导航。 加入第三方javascript文件有时需要第三方的javascript,例如我需要一个返回顶部的按钮。官方的文档说放到根目录\\themes\\pacman\\scripts,会自动加载。我试了一下不成功。最后是修改根目录\\themes\\pacman\\layout\\_partial目录下的after_footer.ejs,写在jquery文件下,如下所示： &lt;script src=&quot;&lt;%- config.root %&gt;js/jquery-2.0.3.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;&lt;%- config.root %&gt;js/scrolltopcontrol.js&quot;&gt;&lt;/script&gt; ... 最后放到$(document).ready的函数体中，进行初始化 scrolltotop.offset(350,120); scrolltotop.init(); 添加RSShexo提供了RSS的生成插件，需要手动安装和设置。步骤如下： 安装RSS插件到本地：npm install hexo-generator-feed –save 开启RSS功能：编辑hexo/_config.yml，在文件尾部添加如下代码：plugins:- hexo-generator-feed 修改根目录\\themes\\pacman的_config.yml中 rss: /atom.xml,这样当执行hexo g后，在根目录下\\public下，发现atom.xml,这安装成功。 在站点添加链接,在themes\\pacman\\layout\\_partial修改header.ejs中适当位置加入链接,如下: &lt;% for (var i in theme.menu){ %&gt; &lt;li&gt;&lt;a href=&quot;&lt;%- theme.menu[i] %&gt;&quot;&gt;&lt;%= i %&gt;&lt;/a&gt;&lt;/li&gt; &lt;% } %&gt; &lt;li&gt; &lt;a href=&quot;/atom.xml&quot;&gt;RSS&lt;/a&gt; &lt;/li&gt; 则在头部菜单加入rss链接。 livereload一旦修改了网站中的内容，livereload会立马检测到并且通知浏览器刷新页面，不需要自己手动按 F5 刷新,太爽了，边写文章边预览，下面是步骤： 安装 LiveReload 的 node.js 版本： $ npm install -g node-livereload 安装 livereload 的浏览器插件，以谷歌浏览器为例。 完成后，到你的 Hexo 博客根目录下执行：livereload -e &quot;.md, .html, .png, .svg, .jpg, .gif, .css, .js, .json&quot; 启动hexo。 打开浏览器，进入你要访问的页面，并点击浏览器上方的LiveReload插件小图标。 修改Markdown源文件，浏览器是不是立即刷新，如果是，大功告成。 Q&amp;A怎样使用摘要？hexo的文章保存在source/_post目录下。在文档中插入&lt;!--more--&gt;就可以将文章分隔，&lt;!--more--&gt;以上的部分会已摘要的形式显示，当查看全文时more以下的部分才会显示出来。也可以在Markdown文件中定义description。 怎样定义特殊字符?有些字符，放在文件中，会出现编译错误，像，有一次我的文档出现了问题，找了老半天才查出原因。必须使用转义如下所示： \\{\\{文档中\\}\\} 怎样显示图片?同样放到source目中下。建议大家建立一个image文件夹，把文章中使用到的图片丢到这里来（当然可以按照你的习惯进行分类），之后在Markdown文件中按照这样的语法进行插入图片的操作，例如。 ![hello]({{BASE_PATH}}image/hello.jpg) 常用命令hexo g == hexo generate hexo d == hexo deploy hexo s == hexo server hexo n == hexo new hexo clean 上传到github如果一切都配置好了，发布上传很容易，执行hexo g &amp;&amp; hexo d就可以， 首先，配置好ssh key找到.ssh\\id_rsa.pub文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key： 将刚复制的内容粘贴到key那里。 使用ssh -T git@github.com进行测试，出现You&#39;ve successfully authenticated, but GitHub does not provide shell access表示成功。 其次，配置_config.yml中有关deploy的部分： 正确写法： deploy: type: git repository: git@github.com:klwork/klwork.github.io.git branch: master 参考文献 Hexo系列教程 Pacman主题介绍 LiveReload For Hex","categories":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"网站开发","slug":"网站开发","permalink":"http://yoursite.com/tags/网站开发/"}],"keywords":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"http://yoursite.com/categories/杂七杂八/"}]}]}